<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 What are Probablistic and Non-Probablistic Regression? | Notes for CS181: Machine Learning</title>
  <meta name="description" content="This is a set of note for CS181: Machine Learning (Spring 2023)" />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 What are Probablistic and Non-Probablistic Regression? | Notes for CS181: Machine Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a set of note for CS181: Machine Learning (Spring 2023)" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 What are Probablistic and Non-Probablistic Regression? | Notes for CS181: Machine Learning" />
  
  <meta name="twitter:description" content="This is a set of note for CS181: Machine Learning (Spring 2023)" />
  

<meta name="author" content="Weiwei Pan" />


<meta name="date" content="2023-05-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="what-is-regression.html"/>
<link rel="next" href="what-matters-in-ml-besides-prediction.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> What is CS181?</a>
<ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#why-is-ai-a-big-deal"><i class="fa fa-check"></i><b>2.1</b> Why Is AI a Big Deal?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="intro.html"><a href="intro.html#but-is-accuracy-enough"><i class="fa fa-check"></i><b>2.1.1</b> But Is Accuracy Enough?</a></li>
<li class="chapter" data-level="2.1.2" data-path="intro.html"><a href="intro.html#what-happens-when-machine-learning-models-are-catastrophically-wrong"><i class="fa fa-check"></i><b>2.1.2</b> What Happens When Machine Learning Models are Catastrophically Wrong?</a></li>
<li class="chapter" data-level="2.1.3" data-path="intro.html"><a href="intro.html#are-machine-models-right-for-the-right-reasons"><i class="fa fa-check"></i><b>2.1.3</b> Are Machine Models Right for the Right Reasons?</a></li>
<li class="chapter" data-level="2.1.4" data-path="intro.html"><a href="intro.html#what-is-the-role-of-the-human-decision-maker"><i class="fa fa-check"></i><b>2.1.4</b> What is the Role of the Human Decision Maker?</a></li>
<li class="chapter" data-level="2.1.5" data-path="intro.html"><a href="intro.html#what-are-the-broader-impacts-of-tech"><i class="fa fa-check"></i><b>2.1.5</b> What are the Broader Impacts of Tech?</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#machine-learning-is-much-more-than-accuracy"><i class="fa fa-check"></i><b>2.2</b> Machine Learning is Much More Than Accuracy</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#what-is-cs181"><i class="fa fa-check"></i><b>2.3</b> What is CS181?</a></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#what-we-are-offering-you"><i class="fa fa-check"></i><b>2.4</b> What We are Offering You</a></li>
<li class="chapter" data-level="2.5" data-path="intro.html"><a href="intro.html#what-we-are-asking-from-you"><i class="fa fa-check"></i><b>2.5</b> What We are Asking From You</a></li>
<li class="chapter" data-level="2.6" data-path="intro.html"><a href="intro.html#grading-evaluation"><i class="fa fa-check"></i><b>2.6</b> Grading &amp; Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="what-is-regression.html"><a href="what-is-regression.html"><i class="fa fa-check"></i><b>3</b> What is Regression?</a>
<ul>
<li class="chapter" data-level="3.1" data-path="what-is-regression.html"><a href="what-is-regression.html#what-is-machine-learning"><i class="fa fa-check"></i><b>3.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="3.2" data-path="what-is-regression.html"><a href="what-is-regression.html#what-is-regression-1"><i class="fa fa-check"></i><b>3.2</b> What is Regression?</a></li>
<li class="chapter" data-level="3.3" data-path="what-is-regression.html"><a href="what-is-regression.html#almost-everything-is-linear-regression"><i class="fa fa-check"></i><b>3.3</b> (Almost) Everything is Linear Regression</a></li>
<li class="chapter" data-level="3.4" data-path="what-is-regression.html"><a href="what-is-regression.html#what-is-model-evaluation"><i class="fa fa-check"></i><b>3.4</b> What is Model Evaluation?</a></li>
<li class="chapter" data-level="3.5" data-path="what-is-regression.html"><a href="what-is-regression.html#what-is-model-critique"><i class="fa fa-check"></i><b>3.5</b> What is Model Critique?</a></li>
<li class="chapter" data-level="3.6" data-path="what-is-regression.html"><a href="what-is-regression.html#limitations-and-connections"><i class="fa fa-check"></i><b>3.6</b> Limitations and Connections</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="what-are-probablistic-and-non-probablistic-regression.html"><a href="what-are-probablistic-and-non-probablistic-regression.html"><i class="fa fa-check"></i><b>4</b> What are Probablistic and Non-Probablistic Regression?</a>
<ul>
<li class="chapter" data-level="4.1" data-path="what-are-probablistic-and-non-probablistic-regression.html"><a href="what-are-probablistic-and-non-probablistic-regression.html#what-is-probabilistic-regression"><i class="fa fa-check"></i><b>4.1</b> What is Probabilistic Regression?</a></li>
<li class="chapter" data-level="4.2" data-path="what-are-probablistic-and-non-probablistic-regression.html"><a href="what-are-probablistic-and-non-probablistic-regression.html#almost-everything-is-linear-regression-1"><i class="fa fa-check"></i><b>4.2</b> (Almost) Everything is Linear Regression</a></li>
<li class="chapter" data-level="4.3" data-path="what-are-probablistic-and-non-probablistic-regression.html"><a href="what-are-probablistic-and-non-probablistic-regression.html#the-cube-a-model-comparison-paradigm"><i class="fa fa-check"></i><b>4.3</b> The Cube: A Model Comparison Paradigm</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html"><i class="fa fa-check"></i><b>5</b> What Matters in ML Besides Prediction?</a>
<ul>
<li class="chapter" data-level="5.1" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#what-is-machine-learning-revisited"><i class="fa fa-check"></i><b>5.1</b> What is Machine Learning? Revisited</a></li>
<li class="chapter" data-level="5.2" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#what-are-we-uncertain-about"><i class="fa fa-check"></i><b>5.2</b> What Are We Uncertain About?</a></li>
<li class="chapter" data-level="5.3" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#where-is-uncertainty-coming-from"><i class="fa fa-check"></i><b>5.3</b> Where is Uncertainty Coming From?</a></li>
<li class="chapter" data-level="5.4" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#how-do-we-compute-uncertainty"><i class="fa fa-check"></i><b>5.4</b> How Do We Compute Uncertainty?</a></li>
<li class="chapter" data-level="5.5" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#mathematizing-uncertainty-starting-with-bias-and-variance"><i class="fa fa-check"></i><b>5.5</b> Mathematizing Uncertainty: Starting with Bias and Variance</a></li>
<li class="chapter" data-level="5.6" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#the-bias-variance-trade-off-in-machine-learning"><i class="fa fa-check"></i><b>5.6</b> The Bias-Variance Trade-off in Machine Learning</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#examples-of-the-bias-variance-trade-off"><i class="fa fa-check"></i><b>5.6.1</b> Examples of the Bias-Variance Trade-off</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html"><i class="fa fa-check"></i><b>6</b> What is Logistic Regression?</a>
<ul>
<li class="chapter" data-level="6.1" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html#logistic-regression-and-soft-classification"><i class="fa fa-check"></i><b>6.1</b> Logistic Regression and Soft-Classification</a></li>
<li class="chapter" data-level="6.2" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html#logistic-regression-and-bernoulli-likelihood"><i class="fa fa-check"></i><b>6.2</b> Logistic Regression and Bernoulli Likelihood</a></li>
<li class="chapter" data-level="6.3" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html#how-to-perform-maximum-likelihood-inference-for-logistic-regression"><i class="fa fa-check"></i><b>6.3</b> How to Perform Maximum Likelihood Inference for Logistic Regression</a></li>
<li class="chapter" data-level="6.4" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html#how-not-to-evaluate-classifiers"><i class="fa fa-check"></i><b>6.4</b> How (Not) to Evaluate Classifiers</a></li>
<li class="chapter" data-level="6.5" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html#how-to-interpret-logistic-regression"><i class="fa fa-check"></i><b>6.5</b> How to Interpret Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="how-do-we-responsibly-use-conditional-models.html"><a href="how-do-we-responsibly-use-conditional-models.html"><i class="fa fa-check"></i><b>7</b> How Do We Responsibly Use Conditional Models?</a>
<ul>
<li class="chapter" data-level="7.1" data-path="how-do-we-responsibly-use-conditional-models.html"><a href="how-do-we-responsibly-use-conditional-models.html#everything-weve-done-so-far-in-probabilistic-ml"><i class="fa fa-check"></i><b>7.1</b> Everything We’ve Done So Far in Probabilistic ML</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="case-study-responsibly-using-logistic-regression.html"><a href="case-study-responsibly-using-logistic-regression.html"><i class="fa fa-check"></i><b>8</b> Case Study: Responsibly Using Logistic Regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="case-study-responsibly-using-logistic-regression.html"><a href="case-study-responsibly-using-logistic-regression.html#case-study-machine-learning-model-for-loan-approval"><i class="fa fa-check"></i><b>8.1</b> Case Study: Machine Learning Model for Loan Approval</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="case-study-responsibly-using-logistic-regression.html"><a href="case-study-responsibly-using-logistic-regression.html#the-big-vague-question"><i class="fa fa-check"></i><b>8.1.1</b> The Big Vague Question</a></li>
<li class="chapter" data-level="8.1.2" data-path="case-study-responsibly-using-logistic-regression.html"><a href="case-study-responsibly-using-logistic-regression.html#the-concrete-and-rigorous-process-of-post-inference-analysis-of-machine-learning-models"><i class="fa fa-check"></i><b>8.1.2</b> The Concrete and Rigorous Process of Post-Inference Analysis of Machine Learning Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html"><i class="fa fa-check"></i><b>9</b> The Math of Training and Interpreting Logistic Regression Models</a>
<ul>
<li class="chapter" data-level="9.1" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#the-math-of-convex-optimization"><i class="fa fa-check"></i><b>9.1</b> The Math of Convex Optimization</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#convexity-of-the-logistic-regression-negative-log-likelihood"><i class="fa fa-check"></i><b>9.1.1</b> Convexity of the Logistic Regression Negative Log-Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#important-mathy-details-of-gradient-descent"><i class="fa fa-check"></i><b>9.2</b> Important Mathy Details of Gradient Descent</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#does-it-converge"><i class="fa fa-check"></i><b>9.2.1</b> Does It Converge?</a></li>
<li class="chapter" data-level="9.2.2" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#how-quickly-can-we-get-there"><i class="fa fa-check"></i><b>9.2.2</b> How Quickly Can We Get There?</a></li>
<li class="chapter" data-level="9.2.3" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#does-it-scale"><i class="fa fa-check"></i><b>9.2.3</b> Does It Scale?</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#interpreting-a-logistic-regression-model-log-odds"><i class="fa fa-check"></i><b>9.3</b> Interpreting a Logistic Regression Model: Log-Odds</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html"><i class="fa fa-check"></i><b>10</b> What are Neural Networks?</a>
<ul>
<li class="chapter" data-level="10.1" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#neural-network-as-universal-function-approximators"><i class="fa fa-check"></i><b>10.1</b> Neural Network as Universal Function Approximators</a></li>
<li class="chapter" data-level="10.2" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#neural-networks-as-regression-on-learned-feature-map"><i class="fa fa-check"></i><b>10.2</b> Neural Networks as Regression on Learned Feature Map</a></li>
<li class="chapter" data-level="10.3" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#everything-is-a-neural-network"><i class="fa fa-check"></i><b>10.3</b> Everything is a Neural Network</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#architecture-zoo"><i class="fa fa-check"></i><b>10.3.1</b> Architecture Zoo</a></li>
<li class="chapter" data-level="10.3.2" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#chatgpt"><i class="fa fa-check"></i><b>10.3.2</b> ChatGPT</a></li>
<li class="chapter" data-level="10.3.3" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#stable-diffusion"><i class="fa fa-check"></i><b>10.3.3</b> Stable Diffusion</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#neural-network-optimization"><i class="fa fa-check"></i><b>10.4</b> Neural Network Optimization</a></li>
<li class="chapter" data-level="10.5" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#bias-variance-trade-off-for-neural-networks"><i class="fa fa-check"></i><b>10.5</b> Bias-Variance Trade-off for Neural Networks</a></li>
<li class="chapter" data-level="10.6" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#interpretation-of-neural-networks"><i class="fa fa-check"></i><b>10.6</b> Interpretation of Neural Networks</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-1-can-neural-network-models-make-use-of-human-concepts"><i class="fa fa-check"></i><b>10.6.1</b> Example 1: Can Neural Network Models Make Use of Human Concepts?</a></li>
<li class="chapter" data-level="10.6.2" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-2-can-neural-network-models-learn-to-explore-hypothetical-scenarios"><i class="fa fa-check"></i><b>10.6.2</b> Example 2: Can Neural Network Models Learn to Explore Hypothetical Scenarios?</a></li>
<li class="chapter" data-level="10.6.3" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-3-a-powerful-generalization-of-feature-importance-for-neural-network-models"><i class="fa fa-check"></i><b>10.6.3</b> Example 3: A Powerful Generalization of Feature Importance for Neural Network Models</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#the-difficulty-with-interpretable-machine-learning"><i class="fa fa-check"></i><b>10.7</b> The Difficulty with Interpretable Machine Learning</a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-4-not-all-explanations-are-created-equal"><i class="fa fa-check"></i><b>10.7.1</b> Example 4: Not All Explanations are Created Equal</a></li>
<li class="chapter" data-level="10.7.2" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-5-explanations-can-lie"><i class="fa fa-check"></i><b>10.7.2</b> Example 5: Explanations Can Lie</a></li>
<li class="chapter" data-level="10.7.3" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-6-the-perils-of-explanations-in-socio-technical-systems"><i class="fa fa-check"></i><b>10.7.3</b> Example 6: The Perils of Explanations in Socio-Technical Systems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html"><i class="fa fa-check"></i><b>11</b> The Math and Interpretation of Neural Network Models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#neural-networks-regression"><i class="fa fa-check"></i><b>11.1</b> Neural Networks Regression</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#why-its-hard-to-differentiate-a-neural-network"><i class="fa fa-check"></i><b>11.1.1</b> Why It’s Hard to Differentiate a Neural Network</a></li>
<li class="chapter" data-level="11.1.2" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#differentiating-neural-networks-backpropagation"><i class="fa fa-check"></i><b>11.1.2</b> Differentiating Neural Networks: Backpropagation</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#interpreting-neural-networks"><i class="fa fa-check"></i><b>11.2</b> Interpreting Neural Networks</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#example-1-can-neural-network-models-make-use-of-human-concepts-1"><i class="fa fa-check"></i><b>11.2.1</b> Example 1: Can Neural Network Models Make Use of Human Concepts?</a></li>
<li class="chapter" data-level="11.2.2" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#example-2-can-neural-network-models-learn-to-explore-hypothetical-scenarios-1"><i class="fa fa-check"></i><b>11.2.2</b> Example 2: Can Neural Network Models Learn to Explore Hypothetical Scenarios?</a></li>
<li class="chapter" data-level="11.2.3" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#example-3-a-powerful-generalization-of-feature-importance-for-neural-network-models-1"><i class="fa fa-check"></i><b>11.2.3</b> Example 3: A Powerful Generalization of Feature Importance for Neural Network Models</a></li>
<li class="chapter" data-level="11.2.4" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#example-4-the-perils-of-explanations"><i class="fa fa-check"></i><b>11.2.4</b> Example 4: The Perils of Explanations</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#neural-network-models-and-generalization"><i class="fa fa-check"></i><b>11.3</b> Neural Network Models and Generalization</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="the-math-behind-bayesian-regression.html"><a href="the-math-behind-bayesian-regression.html"><i class="fa fa-check"></i><b>12</b> The Math Behind Bayesian Regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="the-math-behind-bayesian-regression.html"><a href="the-math-behind-bayesian-regression.html#bayesian-linear-regression"><i class="fa fa-check"></i><b>12.1</b> Bayesian Linear Regression</a></li>
<li class="chapter" data-level="12.2" data-path="the-math-behind-bayesian-regression.html"><a href="the-math-behind-bayesian-regression.html#bayesian-linear-regression-over-arbitrary-bases"><i class="fa fa-check"></i><b>12.2</b> Bayesian Linear Regression over Arbitrary Bases</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="bayesian-modeling-framework.html"><a href="bayesian-modeling-framework.html"><i class="fa fa-check"></i><b>13</b> Bayesian Modeling Framework</a>
<ul>
<li class="chapter" data-level="13.1" data-path="bayesian-modeling-framework.html"><a href="bayesian-modeling-framework.html#components-of-machine-learning-reasoning"><i class="fa fa-check"></i><b>13.1</b> Components of Machine Learning Reasoning</a></li>
<li class="chapter" data-level="13.2" data-path="bayesian-modeling-framework.html"><a href="bayesian-modeling-framework.html#bayesian-modeling-paradigm"><i class="fa fa-check"></i><b>13.2</b> Bayesian Modeling Paradigm</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="bayesain-vs-frequentist-inference.html"><a href="bayesain-vs-frequentist-inference.html"><i class="fa fa-check"></i><b>14</b> Bayesain vs Frequentist Inference?</a>
<ul>
<li class="chapter" data-level="14.1" data-path="bayesain-vs-frequentist-inference.html"><a href="bayesain-vs-frequentist-inference.html#the-bayesian-modeling-process"><i class="fa fa-check"></i><b>14.1</b> The Bayesian Modeling Process</a></li>
<li class="chapter" data-level="14.2" data-path="bayesain-vs-frequentist-inference.html"><a href="bayesain-vs-frequentist-inference.html#bayesian-vs-frequentist-inference"><i class="fa fa-check"></i><b>14.2</b> Bayesian vs Frequentist Inference</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html"><i class="fa fa-check"></i><b>15</b> The Math of Posterior Inference</a>
<ul>
<li class="chapter" data-level="15.1" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html#the-bayesian-modeling-process-1"><i class="fa fa-check"></i><b>15.1</b> The Bayesian Modeling Process</a></li>
<li class="chapter" data-level="15.2" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html#point-estimates-from-the-posterior"><i class="fa fa-check"></i><b>15.2</b> Point Estimates from the Posterior</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html#comparison-of-posterior-point-estimates-and-mle"><i class="fa fa-check"></i><b>15.2.1</b> Comparison of Posterior Point Estimates and MLE</a></li>
<li class="chapter" data-level="15.2.2" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html#law-of-large-numbers-for-bayesian-inference"><i class="fa fa-check"></i><b>15.2.2</b> Law of Large Numbers for Bayesian Inference</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html#bayesian-logistic-regression"><i class="fa fa-check"></i><b>15.3</b> Bayesian Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="whats-hard-about-sampling.html"><a href="whats-hard-about-sampling.html"><i class="fa fa-check"></i><b>16</b> What’s Hard About Sampling?</a>
<ul>
<li class="chapter" data-level="16.1" data-path="whats-hard-about-sampling.html"><a href="whats-hard-about-sampling.html#bayesian-vs-frequentist-inference-1"><i class="fa fa-check"></i><b>16.1</b> Bayesian vs Frequentist Inference</a></li>
<li class="chapter" data-level="16.2" data-path="whats-hard-about-sampling.html"><a href="whats-hard-about-sampling.html#what-is-sampling-and-why-do-we-care"><i class="fa fa-check"></i><b>16.2</b> What is Sampling and Why do We Care?</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html"><i class="fa fa-check"></i><b>17</b> The Math of Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="17.1" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#pca-as-dimensionality-reduction-to-maximize-variance"><i class="fa fa-check"></i><b>17.1</b> PCA as Dimensionality Reduction to Maximize Variance</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#finding-a-single-pca-component"><i class="fa fa-check"></i><b>17.1.1</b> Finding a Single PCA Component</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#pca-as-dimensionality-reduction-to-minimize-reconstruction-loss"><i class="fa fa-check"></i><b>17.2</b> PCA as Dimensionality Reduction to Minimize Reconstruction Loss</a></li>
<li class="chapter" data-level="17.3" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#a-latent-variable-model-for-pca"><i class="fa fa-check"></i><b>17.3</b> A Latent Variable Model for PCA</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#one-principle-component"><i class="fa fa-check"></i><b>17.3.1</b> One Principle Component</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#autoencoders-and-nonlinear-pca"><i class="fa fa-check"></i><b>17.4</b> Autoencoders and Nonlinear PCA</a></li>
<li class="chapter" data-level="17.5" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#a-probabilistic-latent-variable-model-for-pca"><i class="fa fa-check"></i><b>17.5</b> A Probabilistic Latent Variable Model for PCA</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="the-math-of-expectation-maximization.html"><a href="the-math-of-expectation-maximization.html"><i class="fa fa-check"></i><b>18</b> The Math of Expectation Maximization</a></li>
<li class="chapter" data-level="19" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html"><i class="fa fa-check"></i><b>19</b> Motivation for Latent Variable Models</a>
<ul>
<li class="chapter" data-level="19.1" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#latent-variable-models"><i class="fa fa-check"></i><b>19.1</b> Latent Variable Models</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#example-gaussian-mixture-models-gmms"><i class="fa fa-check"></i><b>19.1.1</b> Example: Gaussian Mixture Models (GMMs)</a></li>
<li class="chapter" data-level="19.1.2" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#item-response-models"><i class="fa fa-check"></i><b>19.1.2</b> Item-Response Models</a></li>
<li class="chapter" data-level="19.1.3" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#example-factor-analysis-models"><i class="fa fa-check"></i><b>19.1.3</b> Example: Factor Analysis Models</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#maximum-likelihood-estimation-for-latent-variable-models-expectation-maximization"><i class="fa fa-check"></i><b>19.2</b> Maximum Likelihood Estimation for Latent Variable Models: Expectation Maximization</a></li>
<li class="chapter" data-level="19.3" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#the-expectation-maximization-algorithm"><i class="fa fa-check"></i><b>19.3</b> The Expectation Maximization Algorithm</a></li>
<li class="chapter" data-level="19.4" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#monotonicity-and-convergence-of-em"><i class="fa fa-check"></i><b>19.4</b> Monotonicity and Convergence of EM</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html"><i class="fa fa-check"></i><b>20</b> Review of Latent Variables, Compression and Clustering</a>
<ul>
<li class="chapter" data-level="20.0.1" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#example-gaussian-mixture-models-gmms-1"><i class="fa fa-check"></i><b>20.0.1</b> Example: Gaussian Mixture Models (GMMs)</a></li>
<li class="chapter" data-level="20.0.2" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#example-item-response-models"><i class="fa fa-check"></i><b>20.0.2</b> Example: Item-Response Models</a></li>
<li class="chapter" data-level="20.0.3" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#example-factor-analysis-models-1"><i class="fa fa-check"></i><b>20.0.3</b> Example: Factor Analysis Models</a></li>
<li class="chapter" data-level="20.1" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#pca-versus-probabilistic-pca-ppca"><i class="fa fa-check"></i><b>20.1</b> PCA Versus Probabilistic PCA (pPCA)</a>
<ul>
<li class="chapter" data-level="20.1.1" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#what-to-know-about-expectation-maximization"><i class="fa fa-check"></i><b>20.1.1</b> What to Know About Expectation Maximization</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#non-probabilistic-clustering-versus-probabilistic-clustering"><i class="fa fa-check"></i><b>20.2</b> Non-Probabilistic Clustering Versus Probabilistic Clustering</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="topic-models.html"><a href="topic-models.html"><i class="fa fa-check"></i><b>21</b> Topic Models</a>
<ul>
<li class="chapter" data-level="21.1" data-path="topic-models.html"><a href="topic-models.html#our-first-latent-variable-model"><i class="fa fa-check"></i><b>21.1</b> Our First Latent Variable Model</a></li>
<li class="chapter" data-level="21.2" data-path="topic-models.html"><a href="topic-models.html#reasoning-about-text-corpa-using-topic-modeling"><i class="fa fa-check"></i><b>21.2</b> Reasoning About Text Corpa Using Topic Modeling</a></li>
<li class="chapter" data-level="21.3" data-path="topic-models.html"><a href="topic-models.html#our-second-latent-variable-model-plsa"><i class="fa fa-check"></i><b>21.3</b> Our Second Latent Variable Model: pLSA</a></li>
<li class="chapter" data-level="21.4" data-path="topic-models.html"><a href="topic-models.html#our-third-latent-variable-model-lda"><i class="fa fa-check"></i><b>21.4</b> Our Third Latent Variable Model: LDA</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html"><i class="fa fa-check"></i><b>22</b> Math and Intuition of Hidden Markov Models</a>
<ul>
<li class="chapter" data-level="22.1" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html#markov-models"><i class="fa fa-check"></i><b>22.1</b> Markov Models</a>
<ul>
<li class="chapter" data-level="22.1.1" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html#transition-matrices-and-kernels"><i class="fa fa-check"></i><b>22.1.1</b> Transition Matrices and Kernels</a></li>
<li class="chapter" data-level="22.1.2" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html#applications-of-markov-models"><i class="fa fa-check"></i><b>22.1.2</b> Applications of Markov Models</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html#hidden-markov-models"><i class="fa fa-check"></i><b>22.2</b> Hidden Markov Models</a></li>
<li class="chapter" data-level="22.3" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html#learning-and-inference-for-hmms"><i class="fa fa-check"></i><b>22.3</b> Learning and Inference for HMMs</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html"><i class="fa fa-check"></i><b>23</b> The Intuition of Markov Decision Processes</a>
<ul>
<li class="chapter" data-level="23.1" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#review-modeling-sequential-data"><i class="fa fa-check"></i><b>23.1</b> Review: Modeling Sequential Data</a>
<ul>
<li class="chapter" data-level="23.1.1" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#why-model-sequential-data-dynamics"><i class="fa fa-check"></i><b>23.1.1</b> Why Model Sequential Data (Dynamics)?</a></li>
</ul></li>
<li class="chapter" data-level="23.2" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#modeling-sequential-data-and-sequential-actions"><i class="fa fa-check"></i><b>23.2</b> Modeling Sequential Data and Sequential Actions</a>
<ul>
<li class="chapter" data-level="23.2.1" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#describing-a-dynamic-world"><i class="fa fa-check"></i><b>23.2.1</b> Describing a Dynamic World</a></li>
<li class="chapter" data-level="23.2.2" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#acting-in-a-dynamic-world"><i class="fa fa-check"></i><b>23.2.2</b> Acting in a Dynamic World</a></li>
<li class="chapter" data-level="23.2.3" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#describing-worlds-as-mdps"><i class="fa fa-check"></i><b>23.2.3</b> Describing Worlds as MDP’s</a></li>
</ul></li>
<li class="chapter" data-level="23.3" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#modeling-sequential-decisions-planning"><i class="fa fa-check"></i><b>23.3</b> Modeling Sequential Decisions: Planning</a>
<ul>
<li class="chapter" data-level="23.3.1" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#modeling-action-choice"><i class="fa fa-check"></i><b>23.3.1</b> Modeling Action Choice</a></li>
<li class="chapter" data-level="23.3.2" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#modeling-cumulative-reward"><i class="fa fa-check"></i><b>23.3.2</b> Modeling Cumulative Reward</a></li>
<li class="chapter" data-level="23.3.3" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#planning-optimizing-action-choice-for-cumulative-reward"><i class="fa fa-check"></i><b>23.3.3</b> Planning: Optimizing Action Choice for Cumulative Reward</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes for CS181: Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="what-are-probablistic-and-non-probablistic-regression" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> What are Probablistic and Non-Probablistic Regression?<a href="what-are-probablistic-and-non-probablistic-regression.html#what-are-probablistic-and-non-probablistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><img src="https://i.imgur.com/xDR9VQd.png" /></p>
<div id="what-is-probabilistic-regression" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> What is Probabilistic Regression?<a href="what-are-probablistic-and-non-probablistic-regression.html#what-is-probabilistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As we saw in Lecture #2, in regression, we can choose amongst a very large number of loss functions (i.e. functions that quantify the fit of our model). One way to justify our choice is to reason explicitly about how residuals (prediction errors) arise.</p>
<p>If we include in our model specification a theory of how residuals arise as a random variable, then we have a <em>probabilistic regression model</em>.</p>
<p>A probabilistic regression problem is the task of predicting an output value <span class="math inline">\(y \in \mathbb{R}\)</span>, given an input <span class="math inline">\(\mathbf{x} \in \mathbb{R}^D\)</span>, where the data generating process includes a source of random noise <span class="math inline">\(\epsilon\)</span>.</p>
<p>We have seen one way to solve a probabilistic regression problem, where the observation noise is <em>additive</em>:
1. <em>choose</em> a training data set of <span class="math inline">\(N\)</span> number of observations, <span class="math inline">\(\mathcal{D} = \{(\mathbf{x}_1, y_1),\ldots,(\mathbf{x}_N, y_N)\}\)</span>.
2. <em>hypothesize</em> that the relationship between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(y\)</span> is captured by
<span class="math display">\[
  f_\mathbf{w}(\mathbf{x})
  \]</span>
where <span class="math inline">\(\mathbf{w} \in \mathbb{R}^M\)</span> are the parameters (i.e. unknown coefficients) of the function <span class="math inline">\(f\)</span>.
3. <em>choose</em> a RV <span class="math inline">\(\epsilon\)</span> that describes the additive observation noise:
<span class="math display">\[
y = f_\mathbf{w}(\mathbf{x}) + \epsilon,\; \epsilon \sim p(\epsilon | \theta)
\]</span>
This choice defines a <em>likelihood</em> <span class="math inline">\(p(y| \mathbf{w}, \mathbf{x})\)</span>, describing the likelihood of observing <span class="math inline">\((y, \mathbf{x})\)</span> given that the model is <span class="math inline">\(f_\mathbf{w}\)</span>.
5. <em>choose</em> a math notion for “how well <span class="math inline">\(f_\mathbf{w}\)</span> fits the noisy data”, i.e. a <em>loss function</em>, <span class="math inline">\(\mathcal{L}(\mathbf{w})\)</span>.
6. <em>choose</em> a way to solve for the parameters <span class="math inline">\(\mathbf{w}\)</span> that minimizes the loss function:
<span class="math display">\[
  \mathbf{w}^* = \mathrm{argmin}_\mathbf{w}\; \mathcal{L}(\mathbf{w})
  \]</span></p>
<p>This framework applies to probabilistic regression with any functional form for <span class="math inline">\(f\)</span> (linear, polynomial, neural network etc).</p>
<p><em>Note:</em> while <span class="math inline">\(\mathbf{w}\)</span> is learnt, the parameters <span class="math inline">\(\theta\)</span> of the noise distribution <span class="math inline">\(p(\epsilon | \theta)\)</span> is typically set by the ML engineer prior to learning <span class="math inline">\(\mathbf{w}\)</span>. Parameters like <span class="math inline">\(\theta\)</span> that must be chosen prior to learning are called <strong><em>hyperparameters</em></strong>.</p>
<p>Technically, <span class="math inline">\(y\)</span> is conditioned on <span class="math inline">\(\mathbf{w}\)</span>, <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\theta\)</span> in the likelihood, <span class="math inline">\(p(y| \mathbf{w}, \mathbf{x}, \theta)\)</span>, but typically we drop the <span class="math inline">\(\theta\)</span> and write <span class="math inline">\(p(y| \mathbf{w}, \mathbf{x})\)</span> if we are not intending to infer it from the data.</p>
</div>
<div id="almost-everything-is-linear-regression-1" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> (Almost) Everything is Linear Regression<a href="what-are-probablistic-and-non-probablistic-regression.html#almost-everything-is-linear-regression-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Overwhelmingly frequently in machine learning, we assume <em>additive zero-mean homoskedastic Gaussian noise</em>, that is
<span class="math display">\[
y = f_\mathbf{w}(\mathbf{x}) + \epsilon,\; \epsilon \sim \mathcal{N}(0, \sigma^2)
\]</span>
which implies a Gaussian likelihood:
<span class="math display">\[
y | \mathbf{w}, \mathbf{x} \sim \mathcal{N}(f_\mathbf{w}(\mathbf{x}), \sigma^2).
\]</span>
We <em>choose</em> the negative joint log-likelihood of the data as the metric for how well our model fits the data
<span class="math display">\[
\mathcal{L}(\mathbf{w}) = -\sum_{n=1}^N \log p(y_n | \mathbf{w}, \mathbf{x}_n).
\]</span>
Choosing this loss follows the Maximum Likelihood Principle, encoding our belief that models that renders the observed data more likely are better models.</p>
<p>Now, we learn parameters of <span class="math inline">\(f\)</span> to minimize <span class="math inline">\(\mathcal{L}\)</span> (equivalently, maximizing the joint log-likelihood of the data):
<span class="math display">\[
\mathbf{w}^{\text{MLE}} = \mathrm{argmin}_{\mathbf{w}} \mathcal{L}(\mathbf{w})
\]</span>
The solution <span class="math inline">\(\mathbf{w}^{\text{MLE}}\)</span> is called the <em>maximum likelihood estimate</em> of <span class="math inline">\(\mathbf{w}\)</span>.</p>
<p>We showed that under this set of assumption and choices, probabilistic regression is equivalent to non-probabilistic regression:
<span class="math display">\[
\mathbf{w}^{\text{MLE}} = \mathbf{w}^{\text{OLS}} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}
\]</span>
Maximizing log-likelihood is the same as minimizing the MSE.</p>
<p><strong>Question 1:</strong> If the solution to probablistic regression is equivalent to non-probabilistic regression, does this mean that probabilistic regression models are equivalent to non-probabilistic models?</p>
<p><strong>Question 2:</strong> We saw that MSE:
1. cannot be used alone to evaluate the “goodness” of a model
2. can be helpful to detect overfitting
3. may be unhelpful in detecting underfitting</p>
<p>What about the log-likelihood?</p>
<p><strong>Question 3:</strong> How does one evaluate a probabilistic model?</p>
<p><strong>Question 4:</strong> How do we choose hyperparameters of our probabilistic model?</p>
<ol style="list-style-type: decimal">
<li>Validation</li>
<li>Prior or Domain Knowledge</li>
<li>Analytically or Algorithmically Optimizing an Objective Function:
<span class="math display">\[
\sigma^{\text{MLE}} = \mathrm{argmax}_\sigma\; \ell(\mathbf{w}^{\text{MLE}}, \sigma) = \mathrm{argmax}_\sigma\; \log\left[\prod_{n=1}^N p(y_n | \mathbf{x}_n, \mathbf{w}^{\text{MLE}}, \sigma)\right]
\]</span></li>
</ol>
</div>
<div id="the-cube-a-model-comparison-paradigm" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> The Cube: A Model Comparison Paradigm<a href="what-are-probablistic-and-non-probablistic-regression.html#the-cube-a-model-comparison-paradigm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="i.-axis-probabilistic-vs-non-probabilistic" class="section level4 hasAnchor" number="4.3.0.1">
<h4><span class="header-section-number">4.3.0.1</span> I. Axis: Probabilistic vs Non-probabilistic<a href="what-are-probablistic-and-non-probablistic-regression.html#i.-axis-probabilistic-vs-non-probabilistic" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Definition</th>
<th>Pro</th>
<th>Con</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Probabilistic</td>
<td>Specifying a distribution for the data (and potentially the model), explicitly defining the source and type of noise</td>
<td>When model is “wrong”, we have more assumptions we can interrogate and change in order to improve the model</td>
<td>We need to make more choices (e.g. assuming a distribution for the noise); more choices means more chances to be wrong</td>
</tr>
<tr class="even">
<td>Non-Probabilistic</td>
<td>Specify a functional form for the data; does not explicitly define sources and types of data randomness</td>
<td>We make less assumptions about the data (which may be wrong); not all useful objective functions (e.g. human personal preference) can be easily given a probabilistic interpretation</td>
<td>When the “model” is wrong, we have less assumptions to attribute the problem and hence fewer obvious ways to fix the model</td>
</tr>
</tbody>
</table>
<p><strong>Probabilistic:</strong> regression by specifying likelihood</p>
<p><strong>Non-probabilistic:</strong> regression by minimizing MSE, KNN, kernel regression (for now), regression trees (for now)</p>
<p><strong>Question:</strong> When is it better to use probabilistic regression, when is it better to use non-probabilistic regression?</p>
</div>
<div id="ii.-axis-parametric-vs-non-parametric" class="section level4 hasAnchor" number="4.3.0.2">
<h4><span class="header-section-number">4.3.0.2</span> II. Axis: Parametric vs Non-Parametric<a href="what-are-probablistic-and-non-probablistic-regression.html#ii.-axis-parametric-vs-non-parametric" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Definition</th>
<th>Pro</th>
<th>Con</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Parametric</td>
<td>Assumes a fixed, finite functional form <span class="math inline">\(f(\mathbf{x})\)</span> for regressor</td>
<td>Once the parameters of the functional form is learnt, only the parameters needs to be saved</td>
<td>Need to guess the right functional form for the regressor</td>
</tr>
<tr class="even">
<td>Non-Parametric</td>
<td>Functional form is implicit and can adapt to observed data</td>
<td>No need to specify an explicit functional form</td>
<td>Typically, the training data needs to be saved for every prediction</td>
</tr>
</tbody>
</table>
<p><strong>Parametric:</strong> linear, polynomial, arbitrary basis regression (probabilistic and non-probabilistic); regression trees</p>
<p><strong>Non-Parametric:</strong> KNN, kernel regression</p>
<p><strong>Question:</strong> When is it better to use parametric regression, when is it better to use non-parametric regression?</p>
</div>
<div id="iii.-axis-ease-of-interpretation" class="section level4 hasAnchor" number="4.3.0.3">
<h4><span class="header-section-number">4.3.0.3</span> III. Axis: Ease of Interpretation<a href="what-are-probablistic-and-non-probablistic-regression.html#iii.-axis-ease-of-interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Example</th>
<th>Potential Pro</th>
<th>Potential Con</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Simple Models</td>
<td>Models that can be easily inspected in its entirety, models whose decision process can be easily described</td>
<td>Through interpretation these models can provide scientific insight, be validated by humans, encourage user trust</td>
<td>Simple models may be, though are not necessarily, lacking in capacity to capture complex trends in data</td>
</tr>
<tr class="even">
<td>Complex Models</td>
<td>Models with too many parameters, complex non-linear transformation of data, or whose decision process is lengthy</td>
<td>Complex models often have the capacity to capture very interesting trends in the data</td>
<td>Complex models may be, though are not necessarily, more difficult to interpret, explain, validate and trust</td>
</tr>
</tbody>
</table>
<p><strong>Model Interpretation:</strong> interpretability isn’t a binary label, every model can be “interpreted” in some way:
- <em>linear regression:</em> looking at regression weights
- <em>arbitrary basis regression (including neural networks):</em> looking at regression weights and the features <span class="math inline">\(\phi(\mathbf{x})\)</span>
- <em>probabilistic regression:</em> looking at regression weights and the noise distribution
- <em>regression tree:</em> printing out the tree as a sequence of branching decisions
- <em>KNN:</em> looking at the <span class="math inline">\(k\)</span> nearest neighbors
- <em>kernel regression:</em> interpreting the kernel <span class="math inline">\(k\)</span> as a measure of similarity between two inputs <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{x}&#39;\)</span></p>
<p><strong>Question:</strong> When do we need interpretable models, when does interpretability not matter? Who needs interpretable models? What does interpretable mean?</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="what-is-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="what-matters-in-ml-besides-prediction.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/03-What-are-Prob-Models.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
