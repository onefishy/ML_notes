<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 21 Topic Models | Notes for CS181: Machine Learning</title>
  <meta name="description" content="This is a set of note for CS181: Machine Learning (Spring 2023)" />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 21 Topic Models | Notes for CS181: Machine Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a set of note for CS181: Machine Learning (Spring 2023)" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 21 Topic Models | Notes for CS181: Machine Learning" />
  
  <meta name="twitter:description" content="This is a set of note for CS181: Machine Learning (Spring 2023)" />
  

<meta name="author" content="Weiwei Pan" />


<meta name="date" content="2023-05-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="review-of-latent-variables-compression-and-clustering.html"/>
<link rel="next" href="math-and-intuition-of-hidden-markov-models.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> What is CS181?</a>
<ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#why-is-ai-a-big-deal"><i class="fa fa-check"></i><b>2.1</b> Why Is AI a Big Deal?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="intro.html"><a href="intro.html#but-is-accuracy-enough"><i class="fa fa-check"></i><b>2.1.1</b> But Is Accuracy Enough?</a></li>
<li class="chapter" data-level="2.1.2" data-path="intro.html"><a href="intro.html#what-happens-when-machine-learning-models-are-catastrophically-wrong"><i class="fa fa-check"></i><b>2.1.2</b> What Happens When Machine Learning Models are Catastrophically Wrong?</a></li>
<li class="chapter" data-level="2.1.3" data-path="intro.html"><a href="intro.html#are-machine-models-right-for-the-right-reasons"><i class="fa fa-check"></i><b>2.1.3</b> Are Machine Models Right for the Right Reasons?</a></li>
<li class="chapter" data-level="2.1.4" data-path="intro.html"><a href="intro.html#what-is-the-role-of-the-human-decision-maker"><i class="fa fa-check"></i><b>2.1.4</b> What is the Role of the Human Decision Maker?</a></li>
<li class="chapter" data-level="2.1.5" data-path="intro.html"><a href="intro.html#what-are-the-broader-impacts-of-tech"><i class="fa fa-check"></i><b>2.1.5</b> What are the Broader Impacts of Tech?</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#machine-learning-is-much-more-than-accuracy"><i class="fa fa-check"></i><b>2.2</b> Machine Learning is Much More Than Accuracy</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#what-is-cs181"><i class="fa fa-check"></i><b>2.3</b> What is CS181?</a></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#what-we-are-offering-you"><i class="fa fa-check"></i><b>2.4</b> What We are Offering You</a></li>
<li class="chapter" data-level="2.5" data-path="intro.html"><a href="intro.html#what-we-are-asking-from-you"><i class="fa fa-check"></i><b>2.5</b> What We are Asking From You</a></li>
<li class="chapter" data-level="2.6" data-path="intro.html"><a href="intro.html#grading-evaluation"><i class="fa fa-check"></i><b>2.6</b> Grading &amp; Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="what-is-regression.html"><a href="what-is-regression.html"><i class="fa fa-check"></i><b>3</b> What is Regression?</a>
<ul>
<li class="chapter" data-level="3.1" data-path="what-is-regression.html"><a href="what-is-regression.html#what-is-machine-learning"><i class="fa fa-check"></i><b>3.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="3.2" data-path="what-is-regression.html"><a href="what-is-regression.html#what-is-regression-1"><i class="fa fa-check"></i><b>3.2</b> What is Regression?</a></li>
<li class="chapter" data-level="3.3" data-path="what-is-regression.html"><a href="what-is-regression.html#almost-everything-is-linear-regression"><i class="fa fa-check"></i><b>3.3</b> (Almost) Everything is Linear Regression</a></li>
<li class="chapter" data-level="3.4" data-path="what-is-regression.html"><a href="what-is-regression.html#what-is-model-evaluation"><i class="fa fa-check"></i><b>3.4</b> What is Model Evaluation?</a></li>
<li class="chapter" data-level="3.5" data-path="what-is-regression.html"><a href="what-is-regression.html#what-is-model-critique"><i class="fa fa-check"></i><b>3.5</b> What is Model Critique?</a></li>
<li class="chapter" data-level="3.6" data-path="what-is-regression.html"><a href="what-is-regression.html#limitations-and-connections"><i class="fa fa-check"></i><b>3.6</b> Limitations and Connections</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="what-are-probablistic-and-non-probablistic-regression.html"><a href="what-are-probablistic-and-non-probablistic-regression.html"><i class="fa fa-check"></i><b>4</b> What are Probablistic and Non-Probablistic Regression?</a>
<ul>
<li class="chapter" data-level="4.1" data-path="what-are-probablistic-and-non-probablistic-regression.html"><a href="what-are-probablistic-and-non-probablistic-regression.html#what-is-probabilistic-regression"><i class="fa fa-check"></i><b>4.1</b> What is Probabilistic Regression?</a></li>
<li class="chapter" data-level="4.2" data-path="what-are-probablistic-and-non-probablistic-regression.html"><a href="what-are-probablistic-and-non-probablistic-regression.html#almost-everything-is-linear-regression-1"><i class="fa fa-check"></i><b>4.2</b> (Almost) Everything is Linear Regression</a></li>
<li class="chapter" data-level="4.3" data-path="what-are-probablistic-and-non-probablistic-regression.html"><a href="what-are-probablistic-and-non-probablistic-regression.html#the-cube-a-model-comparison-paradigm"><i class="fa fa-check"></i><b>4.3</b> The Cube: A Model Comparison Paradigm</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html"><i class="fa fa-check"></i><b>5</b> What Matters in ML Besides Prediction?</a>
<ul>
<li class="chapter" data-level="5.1" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#what-is-machine-learning-revisited"><i class="fa fa-check"></i><b>5.1</b> What is Machine Learning? Revisited</a></li>
<li class="chapter" data-level="5.2" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#what-are-we-uncertain-about"><i class="fa fa-check"></i><b>5.2</b> What Are We Uncertain About?</a></li>
<li class="chapter" data-level="5.3" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#where-is-uncertainty-coming-from"><i class="fa fa-check"></i><b>5.3</b> Where is Uncertainty Coming From?</a></li>
<li class="chapter" data-level="5.4" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#how-do-we-compute-uncertainty"><i class="fa fa-check"></i><b>5.4</b> How Do We Compute Uncertainty?</a></li>
<li class="chapter" data-level="5.5" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#mathematizing-uncertainty-starting-with-bias-and-variance"><i class="fa fa-check"></i><b>5.5</b> Mathematizing Uncertainty: Starting with Bias and Variance</a></li>
<li class="chapter" data-level="5.6" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#the-bias-variance-trade-off-in-machine-learning"><i class="fa fa-check"></i><b>5.6</b> The Bias-Variance Trade-off in Machine Learning</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#examples-of-the-bias-variance-trade-off"><i class="fa fa-check"></i><b>5.6.1</b> Examples of the Bias-Variance Trade-off</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html"><i class="fa fa-check"></i><b>6</b> What is Logistic Regression?</a>
<ul>
<li class="chapter" data-level="6.1" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html#logistic-regression-and-soft-classification"><i class="fa fa-check"></i><b>6.1</b> Logistic Regression and Soft-Classification</a></li>
<li class="chapter" data-level="6.2" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html#logistic-regression-and-bernoulli-likelihood"><i class="fa fa-check"></i><b>6.2</b> Logistic Regression and Bernoulli Likelihood</a></li>
<li class="chapter" data-level="6.3" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html#how-to-perform-maximum-likelihood-inference-for-logistic-regression"><i class="fa fa-check"></i><b>6.3</b> How to Perform Maximum Likelihood Inference for Logistic Regression</a></li>
<li class="chapter" data-level="6.4" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html#how-not-to-evaluate-classifiers"><i class="fa fa-check"></i><b>6.4</b> How (Not) to Evaluate Classifiers</a></li>
<li class="chapter" data-level="6.5" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html#how-to-interpret-logistic-regression"><i class="fa fa-check"></i><b>6.5</b> How to Interpret Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="how-do-we-responsibly-use-conditional-models.html"><a href="how-do-we-responsibly-use-conditional-models.html"><i class="fa fa-check"></i><b>7</b> How Do We Responsibly Use Conditional Models?</a>
<ul>
<li class="chapter" data-level="7.1" data-path="how-do-we-responsibly-use-conditional-models.html"><a href="how-do-we-responsibly-use-conditional-models.html#everything-weve-done-so-far-in-probabilistic-ml"><i class="fa fa-check"></i><b>7.1</b> Everything We’ve Done So Far in Probabilistic ML</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="case-study-responsibly-using-logistic-regression.html"><a href="case-study-responsibly-using-logistic-regression.html"><i class="fa fa-check"></i><b>8</b> Case Study: Responsibly Using Logistic Regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="case-study-responsibly-using-logistic-regression.html"><a href="case-study-responsibly-using-logistic-regression.html#case-study-machine-learning-model-for-loan-approval"><i class="fa fa-check"></i><b>8.1</b> Case Study: Machine Learning Model for Loan Approval</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="case-study-responsibly-using-logistic-regression.html"><a href="case-study-responsibly-using-logistic-regression.html#the-big-vague-question"><i class="fa fa-check"></i><b>8.1.1</b> The Big Vague Question</a></li>
<li class="chapter" data-level="8.1.2" data-path="case-study-responsibly-using-logistic-regression.html"><a href="case-study-responsibly-using-logistic-regression.html#the-concrete-and-rigorous-process-of-post-inference-analysis-of-machine-learning-models"><i class="fa fa-check"></i><b>8.1.2</b> The Concrete and Rigorous Process of Post-Inference Analysis of Machine Learning Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html"><i class="fa fa-check"></i><b>9</b> The Math of Training and Interpreting Logistic Regression Models</a>
<ul>
<li class="chapter" data-level="9.1" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#the-math-of-convex-optimization"><i class="fa fa-check"></i><b>9.1</b> The Math of Convex Optimization</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#convexity-of-the-logistic-regression-negative-log-likelihood"><i class="fa fa-check"></i><b>9.1.1</b> Convexity of the Logistic Regression Negative Log-Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#important-mathy-details-of-gradient-descent"><i class="fa fa-check"></i><b>9.2</b> Important Mathy Details of Gradient Descent</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#does-it-converge"><i class="fa fa-check"></i><b>9.2.1</b> Does It Converge?</a></li>
<li class="chapter" data-level="9.2.2" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#how-quickly-can-we-get-there"><i class="fa fa-check"></i><b>9.2.2</b> How Quickly Can We Get There?</a></li>
<li class="chapter" data-level="9.2.3" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#does-it-scale"><i class="fa fa-check"></i><b>9.2.3</b> Does It Scale?</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#interpreting-a-logistic-regression-model-log-odds"><i class="fa fa-check"></i><b>9.3</b> Interpreting a Logistic Regression Model: Log-Odds</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html"><i class="fa fa-check"></i><b>10</b> What are Neural Networks?</a>
<ul>
<li class="chapter" data-level="10.1" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#neural-network-as-universal-function-approximators"><i class="fa fa-check"></i><b>10.1</b> Neural Network as Universal Function Approximators</a></li>
<li class="chapter" data-level="10.2" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#neural-networks-as-regression-on-learned-feature-map"><i class="fa fa-check"></i><b>10.2</b> Neural Networks as Regression on Learned Feature Map</a></li>
<li class="chapter" data-level="10.3" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#everything-is-a-neural-network"><i class="fa fa-check"></i><b>10.3</b> Everything is a Neural Network</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#architecture-zoo"><i class="fa fa-check"></i><b>10.3.1</b> Architecture Zoo</a></li>
<li class="chapter" data-level="10.3.2" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#chatgpt"><i class="fa fa-check"></i><b>10.3.2</b> ChatGPT</a></li>
<li class="chapter" data-level="10.3.3" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#stable-diffusion"><i class="fa fa-check"></i><b>10.3.3</b> Stable Diffusion</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#neural-network-optimization"><i class="fa fa-check"></i><b>10.4</b> Neural Network Optimization</a></li>
<li class="chapter" data-level="10.5" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#bias-variance-trade-off-for-neural-networks"><i class="fa fa-check"></i><b>10.5</b> Bias-Variance Trade-off for Neural Networks</a></li>
<li class="chapter" data-level="10.6" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#interpretation-of-neural-networks"><i class="fa fa-check"></i><b>10.6</b> Interpretation of Neural Networks</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-1-can-neural-network-models-make-use-of-human-concepts"><i class="fa fa-check"></i><b>10.6.1</b> Example 1: Can Neural Network Models Make Use of Human Concepts?</a></li>
<li class="chapter" data-level="10.6.2" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-2-can-neural-network-models-learn-to-explore-hypothetical-scenarios"><i class="fa fa-check"></i><b>10.6.2</b> Example 2: Can Neural Network Models Learn to Explore Hypothetical Scenarios?</a></li>
<li class="chapter" data-level="10.6.3" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-3-a-powerful-generalization-of-feature-importance-for-neural-network-models"><i class="fa fa-check"></i><b>10.6.3</b> Example 3: A Powerful Generalization of Feature Importance for Neural Network Models</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#the-difficulty-with-interpretable-machine-learning"><i class="fa fa-check"></i><b>10.7</b> The Difficulty with Interpretable Machine Learning</a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-4-not-all-explanations-are-created-equal"><i class="fa fa-check"></i><b>10.7.1</b> Example 4: Not All Explanations are Created Equal</a></li>
<li class="chapter" data-level="10.7.2" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-5-explanations-can-lie"><i class="fa fa-check"></i><b>10.7.2</b> Example 5: Explanations Can Lie</a></li>
<li class="chapter" data-level="10.7.3" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-6-the-perils-of-explanations-in-socio-technical-systems"><i class="fa fa-check"></i><b>10.7.3</b> Example 6: The Perils of Explanations in Socio-Technical Systems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html"><i class="fa fa-check"></i><b>11</b> The Math and Interpretation of Neural Network Models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#neural-networks-regression"><i class="fa fa-check"></i><b>11.1</b> Neural Networks Regression</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#why-its-hard-to-differentiate-a-neural-network"><i class="fa fa-check"></i><b>11.1.1</b> Why It’s Hard to Differentiate a Neural Network</a></li>
<li class="chapter" data-level="11.1.2" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#differentiating-neural-networks-backpropagation"><i class="fa fa-check"></i><b>11.1.2</b> Differentiating Neural Networks: Backpropagation</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#interpreting-neural-networks"><i class="fa fa-check"></i><b>11.2</b> Interpreting Neural Networks</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#example-1-can-neural-network-models-make-use-of-human-concepts-1"><i class="fa fa-check"></i><b>11.2.1</b> Example 1: Can Neural Network Models Make Use of Human Concepts?</a></li>
<li class="chapter" data-level="11.2.2" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#example-2-can-neural-network-models-learn-to-explore-hypothetical-scenarios-1"><i class="fa fa-check"></i><b>11.2.2</b> Example 2: Can Neural Network Models Learn to Explore Hypothetical Scenarios?</a></li>
<li class="chapter" data-level="11.2.3" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#example-3-a-powerful-generalization-of-feature-importance-for-neural-network-models-1"><i class="fa fa-check"></i><b>11.2.3</b> Example 3: A Powerful Generalization of Feature Importance for Neural Network Models</a></li>
<li class="chapter" data-level="11.2.4" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#example-4-the-perils-of-explanations"><i class="fa fa-check"></i><b>11.2.4</b> Example 4: The Perils of Explanations</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#neural-network-models-and-generalization"><i class="fa fa-check"></i><b>11.3</b> Neural Network Models and Generalization</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="the-math-behind-bayesian-regression.html"><a href="the-math-behind-bayesian-regression.html"><i class="fa fa-check"></i><b>12</b> The Math Behind Bayesian Regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="the-math-behind-bayesian-regression.html"><a href="the-math-behind-bayesian-regression.html#bayesian-linear-regression"><i class="fa fa-check"></i><b>12.1</b> Bayesian Linear Regression</a></li>
<li class="chapter" data-level="12.2" data-path="the-math-behind-bayesian-regression.html"><a href="the-math-behind-bayesian-regression.html#bayesian-linear-regression-over-arbitrary-bases"><i class="fa fa-check"></i><b>12.2</b> Bayesian Linear Regression over Arbitrary Bases</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="bayesian-modeling-framework.html"><a href="bayesian-modeling-framework.html"><i class="fa fa-check"></i><b>13</b> Bayesian Modeling Framework</a>
<ul>
<li class="chapter" data-level="13.1" data-path="bayesian-modeling-framework.html"><a href="bayesian-modeling-framework.html#components-of-machine-learning-reasoning"><i class="fa fa-check"></i><b>13.1</b> Components of Machine Learning Reasoning</a></li>
<li class="chapter" data-level="13.2" data-path="bayesian-modeling-framework.html"><a href="bayesian-modeling-framework.html#bayesian-modeling-paradigm"><i class="fa fa-check"></i><b>13.2</b> Bayesian Modeling Paradigm</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="bayesain-vs-frequentist-inference.html"><a href="bayesain-vs-frequentist-inference.html"><i class="fa fa-check"></i><b>14</b> Bayesain vs Frequentist Inference?</a>
<ul>
<li class="chapter" data-level="14.1" data-path="bayesain-vs-frequentist-inference.html"><a href="bayesain-vs-frequentist-inference.html#the-bayesian-modeling-process"><i class="fa fa-check"></i><b>14.1</b> The Bayesian Modeling Process</a></li>
<li class="chapter" data-level="14.2" data-path="bayesain-vs-frequentist-inference.html"><a href="bayesain-vs-frequentist-inference.html#bayesian-vs-frequentist-inference"><i class="fa fa-check"></i><b>14.2</b> Bayesian vs Frequentist Inference</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html"><i class="fa fa-check"></i><b>15</b> The Math of Posterior Inference</a>
<ul>
<li class="chapter" data-level="15.1" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html#the-bayesian-modeling-process-1"><i class="fa fa-check"></i><b>15.1</b> The Bayesian Modeling Process</a></li>
<li class="chapter" data-level="15.2" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html#point-estimates-from-the-posterior"><i class="fa fa-check"></i><b>15.2</b> Point Estimates from the Posterior</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html#comparison-of-posterior-point-estimates-and-mle"><i class="fa fa-check"></i><b>15.2.1</b> Comparison of Posterior Point Estimates and MLE</a></li>
<li class="chapter" data-level="15.2.2" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html#law-of-large-numbers-for-bayesian-inference"><i class="fa fa-check"></i><b>15.2.2</b> Law of Large Numbers for Bayesian Inference</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html#bayesian-logistic-regression"><i class="fa fa-check"></i><b>15.3</b> Bayesian Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="whats-hard-about-sampling.html"><a href="whats-hard-about-sampling.html"><i class="fa fa-check"></i><b>16</b> What’s Hard About Sampling?</a>
<ul>
<li class="chapter" data-level="16.1" data-path="whats-hard-about-sampling.html"><a href="whats-hard-about-sampling.html#bayesian-vs-frequentist-inference-1"><i class="fa fa-check"></i><b>16.1</b> Bayesian vs Frequentist Inference</a></li>
<li class="chapter" data-level="16.2" data-path="whats-hard-about-sampling.html"><a href="whats-hard-about-sampling.html#what-is-sampling-and-why-do-we-care"><i class="fa fa-check"></i><b>16.2</b> What is Sampling and Why do We Care?</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html"><i class="fa fa-check"></i><b>17</b> The Math of Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="17.1" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#pca-as-dimensionality-reduction-to-maximize-variance"><i class="fa fa-check"></i><b>17.1</b> PCA as Dimensionality Reduction to Maximize Variance</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#finding-a-single-pca-component"><i class="fa fa-check"></i><b>17.1.1</b> Finding a Single PCA Component</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#pca-as-dimensionality-reduction-to-minimize-reconstruction-loss"><i class="fa fa-check"></i><b>17.2</b> PCA as Dimensionality Reduction to Minimize Reconstruction Loss</a></li>
<li class="chapter" data-level="17.3" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#a-latent-variable-model-for-pca"><i class="fa fa-check"></i><b>17.3</b> A Latent Variable Model for PCA</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#one-principle-component"><i class="fa fa-check"></i><b>17.3.1</b> One Principle Component</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#autoencoders-and-nonlinear-pca"><i class="fa fa-check"></i><b>17.4</b> Autoencoders and Nonlinear PCA</a></li>
<li class="chapter" data-level="17.5" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#a-probabilistic-latent-variable-model-for-pca"><i class="fa fa-check"></i><b>17.5</b> A Probabilistic Latent Variable Model for PCA</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="the-math-of-expectation-maximization.html"><a href="the-math-of-expectation-maximization.html"><i class="fa fa-check"></i><b>18</b> The Math of Expectation Maximization</a></li>
<li class="chapter" data-level="19" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html"><i class="fa fa-check"></i><b>19</b> Motivation for Latent Variable Models</a>
<ul>
<li class="chapter" data-level="19.1" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#latent-variable-models"><i class="fa fa-check"></i><b>19.1</b> Latent Variable Models</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#example-gaussian-mixture-models-gmms"><i class="fa fa-check"></i><b>19.1.1</b> Example: Gaussian Mixture Models (GMMs)</a></li>
<li class="chapter" data-level="19.1.2" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#item-response-models"><i class="fa fa-check"></i><b>19.1.2</b> Item-Response Models</a></li>
<li class="chapter" data-level="19.1.3" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#example-factor-analysis-models"><i class="fa fa-check"></i><b>19.1.3</b> Example: Factor Analysis Models</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#maximum-likelihood-estimation-for-latent-variable-models-expectation-maximization"><i class="fa fa-check"></i><b>19.2</b> Maximum Likelihood Estimation for Latent Variable Models: Expectation Maximization</a></li>
<li class="chapter" data-level="19.3" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#the-expectation-maximization-algorithm"><i class="fa fa-check"></i><b>19.3</b> The Expectation Maximization Algorithm</a></li>
<li class="chapter" data-level="19.4" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#monotonicity-and-convergence-of-em"><i class="fa fa-check"></i><b>19.4</b> Monotonicity and Convergence of EM</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html"><i class="fa fa-check"></i><b>20</b> Review of Latent Variables, Compression and Clustering</a>
<ul>
<li class="chapter" data-level="20.0.1" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#example-gaussian-mixture-models-gmms-1"><i class="fa fa-check"></i><b>20.0.1</b> Example: Gaussian Mixture Models (GMMs)</a></li>
<li class="chapter" data-level="20.0.2" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#example-item-response-models"><i class="fa fa-check"></i><b>20.0.2</b> Example: Item-Response Models</a></li>
<li class="chapter" data-level="20.0.3" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#example-factor-analysis-models-1"><i class="fa fa-check"></i><b>20.0.3</b> Example: Factor Analysis Models</a></li>
<li class="chapter" data-level="20.1" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#pca-versus-probabilistic-pca-ppca"><i class="fa fa-check"></i><b>20.1</b> PCA Versus Probabilistic PCA (pPCA)</a>
<ul>
<li class="chapter" data-level="20.1.1" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#what-to-know-about-expectation-maximization"><i class="fa fa-check"></i><b>20.1.1</b> What to Know About Expectation Maximization</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#non-probabilistic-clustering-versus-probabilistic-clustering"><i class="fa fa-check"></i><b>20.2</b> Non-Probabilistic Clustering Versus Probabilistic Clustering</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="topic-models.html"><a href="topic-models.html"><i class="fa fa-check"></i><b>21</b> Topic Models</a>
<ul>
<li class="chapter" data-level="21.1" data-path="topic-models.html"><a href="topic-models.html#our-first-latent-variable-model"><i class="fa fa-check"></i><b>21.1</b> Our First Latent Variable Model</a></li>
<li class="chapter" data-level="21.2" data-path="topic-models.html"><a href="topic-models.html#reasoning-about-text-corpa-using-topic-modeling"><i class="fa fa-check"></i><b>21.2</b> Reasoning About Text Corpa Using Topic Modeling</a></li>
<li class="chapter" data-level="21.3" data-path="topic-models.html"><a href="topic-models.html#our-second-latent-variable-model-plsa"><i class="fa fa-check"></i><b>21.3</b> Our Second Latent Variable Model: pLSA</a></li>
<li class="chapter" data-level="21.4" data-path="topic-models.html"><a href="topic-models.html#our-third-latent-variable-model-lda"><i class="fa fa-check"></i><b>21.4</b> Our Third Latent Variable Model: LDA</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html"><i class="fa fa-check"></i><b>22</b> Math and Intuition of Hidden Markov Models</a>
<ul>
<li class="chapter" data-level="22.1" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html#markov-models"><i class="fa fa-check"></i><b>22.1</b> Markov Models</a>
<ul>
<li class="chapter" data-level="22.1.1" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html#transition-matrices-and-kernels"><i class="fa fa-check"></i><b>22.1.1</b> Transition Matrices and Kernels</a></li>
<li class="chapter" data-level="22.1.2" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html#applications-of-markov-models"><i class="fa fa-check"></i><b>22.1.2</b> Applications of Markov Models</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html#hidden-markov-models"><i class="fa fa-check"></i><b>22.2</b> Hidden Markov Models</a></li>
<li class="chapter" data-level="22.3" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html#learning-and-inference-for-hmms"><i class="fa fa-check"></i><b>22.3</b> Learning and Inference for HMMs</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html"><i class="fa fa-check"></i><b>23</b> The Intuition of Markov Decision Processes</a>
<ul>
<li class="chapter" data-level="23.1" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#review-modeling-sequential-data"><i class="fa fa-check"></i><b>23.1</b> Review: Modeling Sequential Data</a>
<ul>
<li class="chapter" data-level="23.1.1" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#why-model-sequential-data-dynamics"><i class="fa fa-check"></i><b>23.1.1</b> Why Model Sequential Data (Dynamics)?</a></li>
</ul></li>
<li class="chapter" data-level="23.2" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#modeling-sequential-data-and-sequential-actions"><i class="fa fa-check"></i><b>23.2</b> Modeling Sequential Data and Sequential Actions</a>
<ul>
<li class="chapter" data-level="23.2.1" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#describing-a-dynamic-world"><i class="fa fa-check"></i><b>23.2.1</b> Describing a Dynamic World</a></li>
<li class="chapter" data-level="23.2.2" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#acting-in-a-dynamic-world"><i class="fa fa-check"></i><b>23.2.2</b> Acting in a Dynamic World</a></li>
<li class="chapter" data-level="23.2.3" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#describing-worlds-as-mdps"><i class="fa fa-check"></i><b>23.2.3</b> Describing Worlds as MDP’s</a></li>
</ul></li>
<li class="chapter" data-level="23.3" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#modeling-sequential-decisions-planning"><i class="fa fa-check"></i><b>23.3</b> Modeling Sequential Decisions: Planning</a>
<ul>
<li class="chapter" data-level="23.3.1" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#modeling-action-choice"><i class="fa fa-check"></i><b>23.3.1</b> Modeling Action Choice</a></li>
<li class="chapter" data-level="23.3.2" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#modeling-cumulative-reward"><i class="fa fa-check"></i><b>23.3.2</b> Modeling Cumulative Reward</a></li>
<li class="chapter" data-level="23.3.3" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#planning-optimizing-action-choice-for-cumulative-reward"><i class="fa fa-check"></i><b>23.3.3</b> Planning: Optimizing Action Choice for Cumulative Reward</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes for CS181: Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="topic-models" class="section level1 hasAnchor" number="21">
<h1><span class="header-section-number">Chapter 21</span> Topic Models<a href="topic-models.html#topic-models" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><img src="https://i.imgur.com/xDR9VQd.png" /></p>
<div id="our-first-latent-variable-model" class="section level2 hasAnchor" number="21.1">
<h2><span class="header-section-number">21.1</span> Our First Latent Variable Model<a href="topic-models.html#our-first-latent-variable-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far all of our latent variable models are different instantiations of the same simple <em>graphical model</em>:
<img src="https://i.imgur.com/8oPsqd3.jpg" />
In <strong><em>probabilistic PCA (pPCA)</em></strong>, we assumed that <span class="math inline">\(\mathbf{z}_n\)</span> is a continuous variable that is lower dimensional than <span class="math inline">\(\mathbf{y}_n\)</span>, and that <span class="math inline">\(\mathbf{y}_n\)</span> is a (Gaussian) noisy observation of a linear transformation of <span class="math inline">\(\mathbf{z}_n\)</span>.
- we want to learn <span class="math inline">\(p(\mathbf{z}_n|\mathbf{y}_n)\)</span>: because <span class="math inline">\(\mathbf{z}_n\)</span> represents the low-dimensional “compression” of <span class="math inline">\(\mathbf{y}_n\)</span>
- we want to learn the parameters <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\phi\)</span>: learning the parameters of the model helps us generate new data</p>
<p>In <strong><em>Gaussian Mixture Models (GMMs)</em></strong>, we assumed that <span class="math inline">\(\mathbf{z}_n\)</span> is a categorical (index) variable, and that <span class="math inline">\(\mathbf{y}_n\)</span> an observation from a Gaussian whose mean and covariance are indexed by <span class="math inline">\(\mathbf{z}_n\)</span>.
- we want to learn <span class="math inline">\(p(\mathbf{z}_n|\mathbf{y}_n)\)</span>: because <span class="math inline">\(\mathbf{z}_n\)</span> represents our guess of which to which cluster does <span class="math inline">\(\mathbf{y}_n\)</span> belong
- we want to learn the mixture means <span class="math inline">\(\mu\)</span>: because <span class="math inline">\(\mu_k\)</span> is a natural example of a “typical” point in the <span class="math inline">\(k\)</span>-th cluster
- we want to learn the mixture covariances <span class="math inline">\(\Sigma\)</span>: because <span class="math inline">\(\Sigma_k\)</span> gives us the correlation between samples in the <span class="math inline">\(k\)</span>-th cluster
- we want to learn the mixture coefficients <span class="math inline">\(\pi\)</span>: because <span class="math inline">\(\pi\)</span> gives us an estimate of the relative proportions of the clusters</p>
<p>In <strong><em>Item-Response Analysis</em></strong>, we assumed that <span class="math inline">\(\mathbf{z}_n\)</span> is a continuous variable and that <span class="math inline">\(\mathbf{y}_n\)</span> is a binary observation whose probability of positive outcome is a function <span class="math inline">\(g\)</span> of <span class="math inline">\(\mathbf{z}_n\)</span>.
- we want to learn <span class="math inline">\(p(\mathbf{z}_n|\mathbf{y}_n)\)</span>: because <span class="math inline">\(\mathbf{z}_n\)</span> represents our guess of the latent characteristic (e.g. stress level) we are trying to measure in our human subject
- we want to learn the parameters of <span class="math inline">\(g\)</span>: because we can use <span class="math inline">\(g\)</span> to generate more data (e.g. we can ask, if a subject has this <span class="math inline">\(\mathbf{z}_n\)</span> value, what would their response <span class="math inline">\(\mathbf{y}_n\)</span> be on our test?).
- we want to learn the mean of prior <span class="math inline">\(p(\mathbf{z}_n)\)</span>: because the mean represents the population average (e.g. average stress level amongst students)
- we want to learn the covariance of prior <span class="math inline">\(p(\mathbf{z}_n)\)</span>: because the covariace represents the variations amongst our population (e.g. how stress levels varies in the population)</p>
</div>
<div id="reasoning-about-text-corpa-using-topic-modeling" class="section level2 hasAnchor" number="21.2">
<h2><span class="header-section-number">21.2</span> Reasoning About Text Corpa Using Topic Modeling<a href="topic-models.html#reasoning-about-text-corpa-using-topic-modeling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When I am given a set of numeric data (even a set of images represented by pixel values), I can easily separate out the data into “clusters” and then interpret each “cluster mean” as a typical example for that cluster. But what if the data is text-based? How can we cluster text documents and how can we interpret what each cluster is about?</p>
<p>Clustering text documents usually involves the following components:
1. turning a text document into a vector of numbers <span class="math inline">\(\mathbf{d}\)</span>
- <strong>Bag of Words:</strong> we take all the unique words in our vocabulary (say of size <span class="math inline">\(D\)</span>), then create a maping between words and indices. For each document, we will simply count how many of each word appears in the document, and organize these counts into a vector <span class="math inline">\(D\)</span> dimensional vector.</p>
<p><img src="https://i.imgur.com/hjTbNem.png" /></p>
<ol start="2" style="list-style-type: decimal">
<li>we define the objects of interest during modeling:
<ul>
<li><strong>Topics:</strong> we want to discover a set of <span class="math inline">\(K\)</span> themes, called <em>topics</em>, from the documents. We represent each topic as a distribution over words in the vocabulary. That is, each topic is a <span class="math inline">\(D\)</span> dimensional vector.</li>
<li><strong>Topics per Document:</strong> we want to know what each document is talking about, in terms of the topics. We represent this information as a distribution over the <span class="math inline">\(K\)</span> topics, i.e. this is a <span class="math inline">\(K\)</span> dimensional vector.</li>
</ul></li>
</ol>
<p>A typical model for <strong><em>Topic Modeling</em></strong> will involve:</p>
<ol style="list-style-type: decimal">
<li>a way of generating the observed data (e.g. bag of words representation of documents) as some combination of <span class="math inline">\(K\)</span> topics and a mixture of topics for each document.</li>
<li>a way of learning the <span class="math inline">\(K\)</span> topics (as a distribution over <span class="math inline">\(D\)</span> vocabulary) and a way of learning the topic mixture for each document (as a distribution <span class="math inline">\(K\)</span> topics).</li>
</ol>
<p>Different topic models will accomplish the above two tasks differently.</p>
</div>
<div id="our-second-latent-variable-model-plsa" class="section level2 hasAnchor" number="21.3">
<h2><span class="header-section-number">21.3</span> Our Second Latent Variable Model: pLSA<a href="topic-models.html#our-second-latent-variable-model-plsa" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>It turns out that we can recast Topic Modeling as a <em>latent variable model</em>! This latent variable model will finally be more complex than our first model!</p>
<p>Correspondingly, EM steps for this latent variable model will be more involved to derive (don’t worry, we’re not doing it in this class!).</p>
<p>Our main jobs are:
1. to be able to look at these graphical models and then be able to articulate the how does this model explain the structure in the observed data?.
2. interpret the parameters and latent variables <span class="math inline">\(\mathbf{z}\)</span> that we learn from EM.</p>
<p><strong>Probabilistic Latent Semantic Analysis (pLSA)</strong>
<img src="https://i.imgur.com/mpl4c0Q.png" /></p>
<p><strong>The structure behind the observed data:</strong>
- start with <span class="math inline">\(J\)</span> number of documents in my corpus; for each document, there is a mixture of topics; for each topic, there is a distribution over words
- for the <span class="math inline">\(j\)</span>-th document, the <span class="math inline">\(I\)</span> number of words is generated as follows:
- sample a single topic <span class="math inline">\(\mathbf{z}_{ij}\)</span> from the mixture of topics for the <span class="math inline">\(j\)</span>-th document
- sample a single word from the distribution over words representing this topic</p>
<p><strong>Inference (EM):</strong>
- the <em>observed data log-likelihood</em> is (omitting indices):
<span class="math display">\[p(d, w) = \sum_{z} p(w|z)p(z|d)p(d)\]</span>
or alternatively
<span class="math display">\[p(w|d) = \sum_{z} p(w|z)p(z|d)\]</span>
- we want to learn the mixture of topics for each document, i.e. <span class="math inline">\(p(z|d)\)</span>
- we want to learn the distribution over words defined by each topic, i.e. <span class="math inline">\(p(w|z)\)</span></p>
<p>Doing EM to infer this information is equivalent to a form of Nonnegative Matrix Factorization (NMF).
<img src="https://i.imgur.com/dAhJS57.png" /></p>
</div>
<div id="our-third-latent-variable-model-lda" class="section level2 hasAnchor" number="21.4">
<h2><span class="header-section-number">21.4</span> Our Third Latent Variable Model: LDA<a href="topic-models.html#our-third-latent-variable-model-lda" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the pre-class prep materials, we introduced another topic model, <strong><em>Latent Dirichlet Allocation</em></strong>. So why do we need yet another topic model (especially one that is so complicated)???</p>
<p>This is because when we learn the mixture of topics for each document, <span class="math inline">\(p(z|d)\)</span>, and the distribution over words defined by each topic, <span class="math inline">\(p(w|z)\)</span> by MLE, we can often overfit! For example, even if your corpus is large, for many topics, the number of “examples” may still be small and thus our model may overfit.</p>
<p>Rather than regularizing our MLE, we can put priors on both the mixture of topics per document and on the distribution over words defined by each topic. Through these priors, we can express lots of interesting beliefs we have about how the topics should look and how each document should look.</p>
<p><strong>Latent Dirichlet Allocation</strong>
<img src="https://i.imgur.com/dCeUL84.png" /></p>
<p>Again, in the above:
- <span class="math inline">\(w\)</span> represents single words
- <span class="math inline">\(z\)</span> represents single topics</p>
<p>Now, instead of supposing that each document has a fixed but unknown mixture of topics, <span class="math inline">\(p(z|d)\)</span>, we suppose that each document was randomly assigned a mixture of topics (this is our <em>prior</em> on <span class="math inline">\(p(z|d)\)</span>):
- <span class="math inline">\(\theta\)</span> represents the per document topic mixture</p>
<p>Since <span class="math inline">\(\theta\)</span> is a mixture (the mixture components must sum to 1), it make sense to model <span class="math inline">\(\theta\)</span> as a Dirichlet random variable with hyperparameters <span class="math inline">\(\alpha\)</span>.</p>
<p>Instead of supposing that each topic has a fixed but unknown distribution over words, <span class="math inline">\(p(w|z)\)</span>, we suppose that each topic was randomly assigned a distribution over words (this is our <em>prior</em> on <span class="math inline">\(p(w|z)\)</span>):
- <span class="math inline">\(\beta\)</span> represents the per topic word distribution</p>
<p>Since <span class="math inline">\(\theta\)</span> is a distribution over words (the distribution must sum to 1), it make sense to model <span class="math inline">\(\theta\)</span> as a Dirichlet random variable with hyperparameters <span class="math inline">\(\eta\)</span>.</p>
<p><strong>The Generative Story for LDA</strong></p>
<p>For the <span class="math inline">\(k\)</span>-th topic:
- we sample a distribution over words <span class="math inline">\(\beta_k\)</span></p>
<p>For the <span class="math inline">\(j\)</span>-th document:
- we sample a distribution over topics <span class="math inline">\(\alpha_j\)</span>
- For the <span class="math inline">\(i\)</span>-th word in the <span class="math inline">\(j\)</span>-document:
- we sample a single topic <span class="math inline">\(z_{ij}\)</span> from the topic mixture
- we sample a single word from the word distribution of the topic <span class="math inline">\(z_{ij}\)</span></p>
<p><strong>Inference (Variational EM):</strong>
- we learn the hyper-parameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\eta\)</span> by maximizing the ELBO
- unfortunately, in the E-step, the posterior over the latent variables cannot be analytically derived. In this case, we approximate the posterior using <strong><em>variational inference</em></strong>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="review-of-latent-variables-compression-and-clustering.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="math-and-intuition-of-hidden-markov-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/19-Topic-Models.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
