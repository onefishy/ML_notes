[["index.html", "Notes for CS181: Machine Learning Chapter 1 Introduction", " Notes for CS181: Machine Learning Weiwei Pan 2023-05-02 Chapter 1 Introduction This is a set of notes for the Spring 2023 iteration of CS181: Machine Learning. CS 181 provides a broad and rigorous introduction to machine learning, probabilistic reasoning and decision-making in uncertain environments. We discuss the motivations behind common machine learning algorithms, and the properties that determine whether or not they will work well for a particular task. We derive the mathematical underpinnings for many common methods, as well as apply machine learning to challenges with real data. In doing so, students will gain a strong conceptual understanding of machine learning methods that can empower them to pursue future theoretical and practical directions. "],["intro.html", "Chapter 2 What is CS181? 2.1 Why Is AI a Big Deal? 2.2 Machine Learning is Much More Than Accuracy 2.3 What is CS181? 2.4 What We are Offering You 2.5 What We are Asking From You 2.6 Grading &amp; Evaluation", " Chapter 2 What is CS181? 2.1 Why Is AI a Big Deal? Artifcial intellegence or machine learning models are becoming increasinglly ubiquitous in modern life. These models have already made meaningful impact on many of the most pressing problems we face today. Most impactfully, AI has enabled me to generate endless alternate reality depictions of my animal co-conspirators: 2.1.1 But Is Accuracy Enough? When machine learning models are applied to safety-critical, risk-adverse domains such as health care, reliable measurements of a model’s justifications for a prediction as well as its predictive uncertainty may be as crucial as correctness of its predictions. Predictive uncertainty helps us quantify risk in down-stream tasks: We also care about the source of uncertainty: Reducible uncertainty due to the lack of data is often called epistemic uncertainty, whereas irreducible uncertainty due to the inherent noisiness of the prediciton is called aleatoric uncertainty. Knowing where our uncertainty comes from helps us make decisions about how to mitigate risk. 2.1.2 What Happens When Machine Learning Models are Catastrophically Wrong? Unfortunately, many of the machine learning models that achieve the most impressive performances do not provide any indications to users when they are operating “out of their depth”. In fact, when ML models break, they often do so silently and their failures may go unnoticed for a long time. Image from: Why ReLU Networks Yield High-Confidence Predictions Far Away From the Training Data and How to Mitigate the Problem 2.1.2.1 Example: Detecting When the Model is Operating in Unfamiliar Territories In Efficient Out-of-Distribution Detection in Digital Pathology Using Multi-Head Convolutional Neural Networks (Linmans et al, Medical Imaging with Deep Learning 2020), the authors train an uncertainty-aware neural network model to detect breast cancer metastasis. The epistemic uncertainty is used during test time to detect new types of breast tissue images that were not included in the training data. Classification of these novel types of images can be deferred to a human. Unfamiliar types were caught at test time with AUC of .98. 2.1.3 Are Machine Models Right for the Right Reasons? In order for human decision makers to interact with machine learning models meaningfully – for example, in order to verify their correctness – we need these models to be interpretable. But explaning complex models is difficult. 2.1.3.1 Example: Explainable ML Diagnostic Model In An explainable deep-learning algorithm for the detection of acute intracranial haemorrhage from small datasets (Lee et al, Nature Biomedical Engineering, 2019), the authors build a neural network model to detect acute intracranial haemorrhage (ICH) and classifies five ICH subtypes. Model classifications are explained by highlighting the pixels that contributed the most to the decision. The highligthed regions tends to overlapped with ‘bleeding points’ annotated by neuroradiologists on the images. 2.1.4 What is the Role of the Human Decision Maker? We tend to think of modeling as a purely mathematical or engineering feat, but in many cases the model has a complex interaction with a human decision maker. We not only need to worry about the performance of the model, we also need to worry about the performance the combined system of Human + AI. Image from: Does Higher Prediction Accuracy Guarantee Better Human-AI Collaboration? Of course, disasterous combinations of AI and humans have long been fodder for cinematic imagination. 2.1.4.1 Example: The Promises of Human + AI Systems In Consistent Estimators for Learning to Defer to an Expert (Mozannar et al, International Conference on Machine Learning, 2020), the authors trains a model that decides when (and how) to classify an input and when to defer the decision to an human expert. The joint Human + AI system can be superior to both components. 2.1.4.2 Example: The Perils of Human + AI Systems In How machine-learning recommendations influence clinician treatment selections: the example of antidepressant selection (Jacobs et al, Translational Psychiatry, 2021), the authors found that clinicians interacting with incorrect recommendations paired with simple explanations experienced significant reduction in treatment selection accuracy. Incorrect ML recommendations may adversely impact clinician treatment selections and that explanations are insufficient for addressing overreliance on imperfect ML algorithms. 2.1.5 What are the Broader Impacts of Tech? The days where we’d looked at Sillicon Valley unicorns and the promise of their tech through the rose-colored glasses of unbridled optimism are long gone. These days tech are still grabbing headlines, but often not for the right reasons. Increasing in prominence are calls for more diverse (in terms of domain expertise, discipline and lived-experiences) lenses to be applied to AI technology. 2.2 Machine Learning is Much More Than Accuracy Machine learning is much more than engineering for abstract performance metrics. Increasingly the field is grappling with the role digital technology is playing in our entire socio-technical system. Machine learning and data science are interdisciplinary fields that require people with diverse skill-sets/backgrounds to work closely and cooperatively. There is also increasing calls for machine learning/data science researchers to engage meaningfully with social, economic, political, cultural and ethical impacts of their work when embedded in complex human institutions. 2.3 What is CS181? Build statistical (Bayesian and non-Bayesian) models for: continuous, ordinal and categorical data Study algorithms for model fitting and inference Study paradigms for model evaluation and critique Understand ways models can fail or produce unintended negative impact in real-life settings Goal: students become familiar with standard statistical models and modern techniques of inference. At the end of the course you should be able to productively engage with current machine learning developments and apply a number of models to solve real-life problems. You should also be able to anticipate model failure modes and perform nuanced analyses of the broader social impact of your model. Focus: - Why: theory should serve a concrete purpose. - How: emphasize computational aspects of inference. - But Should I?: anticipate failure modes and negative social impacts. 2.4 What We are Offering You We have structured CS181 to accomodate a number of different learning goals and learning styles. We have videos in traditional lecture format, textbooks, notes and additional resources for those of you who learn best in traditional classrooms. We have plenty of interactive, synchronous course components where you can ask questions, practice what you learn and extend your knowledge. There is content for both students looking for a higher level overview of ML as well as links to resources for students looking for more depth on particular topics. There are access points to different types of research areas in ML (the “Beyond Sections” sections). 2.5 What We are Asking From You We’ve put in a substantial amount of work to structure CS181 to support your learning. We are asking you also put in work in order to make this a successful learning experience: Talk to course humans: Come to instructor and TF office hours Come to Sections to review and reinforce learning Come to Beyond Sections to contextualize and broaden your knowledge Ask questions: Ask questions to understand. There is no such thing as an obvious fact or a trivial question. Don’t let shyness of intimidation prevent you from asking for help to understand something. Ask questions to dig deeper. Every single concept in this course serves a purpose and has a justification. Don’t settle for knowing facts, there’s always a questions you can ask about something you already know that will show you something new and something deep. Focus on creating connections, relationships between and syntheses of different concepts. Don’t worry about memorizing lines of math. Challenge yourself! Machine learning is currently challenging our understanding of sentience/intelligence, ethics, law, labor, social/cultural dynamics. Machine learning should be challenging you! Make space for everyone’s perspective – there is no “solving machine learning”, “winning at machine learning” and no such thing as the “best solution” or “right answer”. Make space for your own perspective – your background, whatever it is, is an important perspective for machine learning. Look beyond the class – the goal of the class isn’t to get you to be fluent in math, stats or pytorch; the goal is to help you see how ML figures into your life (e.g. research, academic/professional interests/plans, empowered citizenship) 2.6 Grading &amp; Evaluation Generally speaking, the grading in this course is formative not punitive. We are looking to see that you’ve grasped basic skills and fundamental concepts, we are not looking to deduct points for various mistakes. Our evaluation philosophy is standard-based. That is, just as there is a basic level of fluency and familiarity you neeed with operation of motor vehicles to obtain a driver’s license, there is a basic level of fluency and familiarity with theory/implementation/impact analysis you are expected to gain in order to responsibly operate machine learning models in the real world. Course staff is here to support your learning in order to meet these standards and engage with ML meaningfully in your own way. In particular, we are not interested in: 1. competition 2. ranking of students along arbitrary axes 3. making anything hard for “hardness” sake "],["what-is-regression.html", "Chapter 3 What is Regression? 3.1 What is Machine Learning? 3.2 What is Regression? 3.3 (Almost) Everything is Linear Regression 3.4 What is Model Evaluation? 3.5 What is Model Critique? 3.6 Limitations and Connections", " Chapter 3 What is Regression? 3.1 What is Machine Learning? Machine learning is typically the task of learning a function, \\(f\\), given a set of training data, \\(\\mathcal{D}\\). This learnt function \\(f\\) can be used to make predictions on new data in regression and classification, this function can be used to explain how the observed data is structured in unsupervised learning. In this class, this function \\(f\\) can be linear (linear, polynomial, basis regression models), represented by a neural network (deep learning models), or defined without an explicit formula in terms of the input (non-parametric models)! 3.2 What is Regression? Thus far in the course, a regression problem is the task of predicting an output value \\(y \\in \\mathbb{R}\\), given an input \\(\\mathbf{x} \\in \\mathbb{R}^D\\). We have seen one way to solve a regression problem: 1. choose a training data set of \\(N\\) number of observations, \\(\\mathcal{D} = \\{(\\mathbf{x}_1, y_1),\\ldots,(\\mathbf{x}_N, y_N)\\}\\). 2. hypothesize that the relationship between \\(\\mathbf{x}\\) and \\(y\\) is captured by \\[ y = f_\\mathbf{w}(\\mathbf{x}) \\] where \\(\\mathbf{w} \\in \\mathbb{R}^M\\) are the parameters (i.e. unknown coefficients) of the function \\(f\\). 3. choose a math notion for “how well \\(f_\\mathbf{w}\\) fits the data”, i.e. a loss function, \\(\\mathcal{L}(\\mathbf{w})\\). 4. choose a way to solve for the parameters \\(\\mathbf{w}\\) that minimizes the loss function: \\[ \\mathbf{w}^* = \\mathrm{argmin}_\\mathbf{w}\\; \\mathcal{L}(\\mathbf{w}) \\] This framework applies to linear, polynomial and basis regression as well as fancy neural network regression. Note: every step in the above solution is a design choice, which means that you can chose wrongly for your given real-life problem. In your model evaluation step, you must revisit and critique each design choice. 3.3 (Almost) Everything is Linear Regression In linear regression, we assume that our function \\(f_\\mathbf{w}\\) as the form \\[ y = f_\\mathbf{w}(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} \\] If we choose Mean Square Error as our loss function, then \\[ \\mathcal{L}(\\mathbf{w}) = \\frac{1}{N}\\sum_{n=1}^N(y_n - f_\\mathbf{w}(\\mathbf{x}))^2. \\] If we choose to analytically minimize \\(\\mathcal{L}\\) over possible values of \\(\\mathbf{w}\\) (using calculus), then we get \\[ \\mathbf{w}^* = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y} \\] where \\(\\mathbf{X} \\in \\mathbb{R}^{N\\times D}\\) is the matrix of all your training input and \\(\\mathbf{y} \\in \\mathbb{R}^N\\) is the vector of all your training target values. For each new input \\(\\mathbf{x}\\), our model prediction would be \\[ \\hat{y} = (\\mathbf{w}^*)^\\top\\mathbf{x} = \\left[(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}\\right]^\\top\\mathbf{x}. \\] Once we have solved the linear regression problem, it turns out we have solved the regression problem for a large number of non-linear regression problems! Polynomial Regression: Polynomial regression of degree \\(K\\) is just linear regression on top of input data that’s been augmented with polynomial features: \\[ y = \\mathbf{w}^\\top \\phi(\\mathbf{x}) \\] where \\(\\phi: \\mathbb{R}^D \\to \\mathbb{R}^{D * K + 1}\\) is defined by \\[ \\phi((1\\; x_1\\; \\ldots\\; x_D)^\\top) = (1\\; x_1\\; x^2_1\\; \\ldots\\; x^K_1\\; x_2\\; x^2_2\\; \\ldots\\; x^K_2\\; \\ldots \\; x_D\\; x^2_D\\; \\ldots\\; x^K_D)^\\top. \\] Thus, the solution to polynomial regression is \\[ \\mathbf{w}^* = (\\phi(\\mathbf{X})^\\top\\phi(\\mathbf{X}))^{-1}\\phi(\\mathbf{X})^\\top\\mathbf{y} \\] For each new input \\(\\mathbf{x}\\), our model prediction would be \\[ \\hat{y} = (\\mathbf{w}^*)^\\top\\mathbf{x} = \\left[(\\mathbf{\\Phi}^\\top\\mathbf{\\Phi})^{-1}\\mathbf{\\Phi}^\\top\\mathbf{y}\\right]^\\top\\mathbf{x}, \\] where we write \\(\\mathbf{\\Phi}\\) for \\(\\phi(\\mathbf{X})\\). Regression on Arbitrary Bases: We can choose any non-linear feature map \\(\\phi: \\mathbb{R}^{D} \\to \\mathbb{R}^{D&#39;}\\) and capture non-linear trends in the data by performing linear regression on \\(\\phi(\\mathbf{x})\\). Neural Network Regression: Later, we will view neural network regression as: 1. first learning, rather than choosing, a feature map \\(\\phi\\) 2. then performing linear regression on \\(\\phi(\\mathbf{x})\\). 3.4 What is Model Evaluation? So, we’ve solved (linear) regression, i.e. we found \\(\\mathbf{w}^*\\) that minimizes the Mean Square Error on \\(\\mathcal{D}\\). Are we done? No! The machine learning has only just started! We still need to evaluate our model: what is the Mean Square Error of our model on the test data? Why do we need to check this? &gt; What if train MSE is 0.9 and test MSE is 10? can we visually inspect our model on training and test data (i.e. plot the model against the data)? Why visually inspect if we’ve computed MSE? we need to interpret the model. Why interpret when we have the MSE? &gt; What if we saw that selling price was related to square footage and number of rooms by \\(y = -5 * \\mathrm{Sqft} + \\mathrm{Rooms}\\)? does the model help us solve the problem? That is, who is making the final decision on the problem? How are they going to be using the model? What information do they need from the model to make good decisions? &gt; What we found the literal model that generated the data, but the MSE will always be around 10 mg (why MSE not be zero)? Say our task is to prescribe a new seizure medication for epileptic patients, where the safe dosage range is between 20 mg and 30 mg. Is this a helpful model? what are the limitations of our models – what are the assumptions we made when building/testing our model? How will our model behave when those assumptions are violated? When our model fails or succeeds, what will be the negative or positive real-life consequences and how will those consequences be distributed amongst the population? &gt; If our model causes physicians in a clinical trial to make poor decisions or draw incorrect conclusions, which patient groups will be the most impacted? How will they be impacted? 3.5 What is Model Critique? If the model fails any of the above four evaluation steps, we need to revisit and potentially re-do each design decision: Is our training data the “right” data set? Is the functional form (linear or polynomial of degree 2) we assumed correct? Does our loss function capture what we really want to happen in real-life? Did we optimize well? Did we provide the right information about the model to the human decision maker? 3.6 Limitations and Connections Our current approach to regression is limited in a number of ways: 1. We need to guess the “right” functional form of the data trend. (Non-parametric regression and Neural Network Regression will help.) 2. Our choice to always minimize MSE seem arbitrary. (Probabilistic Regression will help.) 3. We needed to do lots of fancy calculus to minimize our loss function – what if we can’t solve for the stationary points?! (Gradient Descent will help) 3. Minimizing the loss function seems to involve differentiation – what if it’s too hard to write out the gradient of my loss function?! (Automatic differentiation will help) 3. We can’t explain why observed \\(y\\)’s don’t agree with the predictions of a perfect model \\(f_\\mathbf{w}\\). (Probabilistic Regression will help.) 4. We have important domain knowledge (i.e. higher square footage should not negatively impact sales price) that we are not incorporating into our model. (Bayesian Regression will help) 5. We have no uncertainty, just predictions! (Confidence and predictive intervals will help, so will Bayesian models) 6. Is printing out the model parameters the best way to interpret the model? (Different XAI techniques will help) &gt; Say we find that selling price is related to square footage and number of rooms by \\[ y = 5\\; * \\mathrm{SqFt} + 0.1\\; * \\mathrm{Rooms} \\] What would you say is the most important factor in determining selling price? If you realized that \\(\\mathrm{SqFt} = 15 * \\mathrm{Rooms}\\), would this change your interpretation of the model? 7. Our discussion of model evaluation and critique have been in the abstract – we have not considered any real-life evaluations of or constraints on our models. (Working with domain experts will help) "],["what-are-probablistic-and-non-probablistic-regression.html", "Chapter 4 What are Probablistic and Non-Probablistic Regression? 4.1 What is Probabilistic Regression? 4.2 (Almost) Everything is Linear Regression 4.3 The Cube: A Model Comparison Paradigm", " Chapter 4 What are Probablistic and Non-Probablistic Regression? 4.1 What is Probabilistic Regression? As we saw in Lecture #2, in regression, we can choose amongst a very large number of loss functions (i.e. functions that quantify the fit of our model). One way to justify our choice is to reason explicitly about how residuals (prediction errors) arise. If we include in our model specification a theory of how residuals arise as a random variable, then we have a probabilistic regression model. A probabilistic regression problem is the task of predicting an output value \\(y \\in \\mathbb{R}\\), given an input \\(\\mathbf{x} \\in \\mathbb{R}^D\\), where the data generating process includes a source of random noise \\(\\epsilon\\). We have seen one way to solve a probabilistic regression problem, where the observation noise is additive: 1. choose a training data set of \\(N\\) number of observations, \\(\\mathcal{D} = \\{(\\mathbf{x}_1, y_1),\\ldots,(\\mathbf{x}_N, y_N)\\}\\). 2. hypothesize that the relationship between \\(\\mathbf{x}\\) and \\(y\\) is captured by \\[ f_\\mathbf{w}(\\mathbf{x}) \\] where \\(\\mathbf{w} \\in \\mathbb{R}^M\\) are the parameters (i.e. unknown coefficients) of the function \\(f\\). 3. choose a RV \\(\\epsilon\\) that describes the additive observation noise: \\[ y = f_\\mathbf{w}(\\mathbf{x}) + \\epsilon,\\; \\epsilon \\sim p(\\epsilon | \\theta) \\] This choice defines a likelihood \\(p(y| \\mathbf{w}, \\mathbf{x})\\), describing the likelihood of observing \\((y, \\mathbf{x})\\) given that the model is \\(f_\\mathbf{w}\\). 5. choose a math notion for “how well \\(f_\\mathbf{w}\\) fits the noisy data”, i.e. a loss function, \\(\\mathcal{L}(\\mathbf{w})\\). 6. choose a way to solve for the parameters \\(\\mathbf{w}\\) that minimizes the loss function: \\[ \\mathbf{w}^* = \\mathrm{argmin}_\\mathbf{w}\\; \\mathcal{L}(\\mathbf{w}) \\] This framework applies to probabilistic regression with any functional form for \\(f\\) (linear, polynomial, neural network etc). Note: while \\(\\mathbf{w}\\) is learnt, the parameters \\(\\theta\\) of the noise distribution \\(p(\\epsilon | \\theta)\\) is typically set by the ML engineer prior to learning \\(\\mathbf{w}\\). Parameters like \\(\\theta\\) that must be chosen prior to learning are called hyperparameters. Technically, \\(y\\) is conditioned on \\(\\mathbf{w}\\), \\(\\mathbf{x}\\) and \\(\\theta\\) in the likelihood, \\(p(y| \\mathbf{w}, \\mathbf{x}, \\theta)\\), but typically we drop the \\(\\theta\\) and write \\(p(y| \\mathbf{w}, \\mathbf{x})\\) if we are not intending to infer it from the data. 4.2 (Almost) Everything is Linear Regression Overwhelmingly frequently in machine learning, we assume additive zero-mean homoskedastic Gaussian noise, that is \\[ y = f_\\mathbf{w}(\\mathbf{x}) + \\epsilon,\\; \\epsilon \\sim \\mathcal{N}(0, \\sigma^2) \\] which implies a Gaussian likelihood: \\[ y | \\mathbf{w}, \\mathbf{x} \\sim \\mathcal{N}(f_\\mathbf{w}(\\mathbf{x}), \\sigma^2). \\] We choose the negative joint log-likelihood of the data as the metric for how well our model fits the data \\[ \\mathcal{L}(\\mathbf{w}) = -\\sum_{n=1}^N \\log p(y_n | \\mathbf{w}, \\mathbf{x}_n). \\] Choosing this loss follows the Maximum Likelihood Principle, encoding our belief that models that renders the observed data more likely are better models. Now, we learn parameters of \\(f\\) to minimize \\(\\mathcal{L}\\) (equivalently, maximizing the joint log-likelihood of the data): \\[ \\mathbf{w}^{\\text{MLE}} = \\mathrm{argmin}_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}) \\] The solution \\(\\mathbf{w}^{\\text{MLE}}\\) is called the maximum likelihood estimate of \\(\\mathbf{w}\\). We showed that under this set of assumption and choices, probabilistic regression is equivalent to non-probabilistic regression: \\[ \\mathbf{w}^{\\text{MLE}} = \\mathbf{w}^{\\text{OLS}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y} \\] Maximizing log-likelihood is the same as minimizing the MSE. Question 1: If the solution to probablistic regression is equivalent to non-probabilistic regression, does this mean that probabilistic regression models are equivalent to non-probabilistic models? Question 2: We saw that MSE: 1. cannot be used alone to evaluate the “goodness” of a model 2. can be helpful to detect overfitting 3. may be unhelpful in detecting underfitting What about the log-likelihood? Question 3: How does one evaluate a probabilistic model? Question 4: How do we choose hyperparameters of our probabilistic model? Validation Prior or Domain Knowledge Analytically or Algorithmically Optimizing an Objective Function: \\[ \\sigma^{\\text{MLE}} = \\mathrm{argmax}_\\sigma\\; \\ell(\\mathbf{w}^{\\text{MLE}}, \\sigma) = \\mathrm{argmax}_\\sigma\\; \\log\\left[\\prod_{n=1}^N p(y_n | \\mathbf{x}_n, \\mathbf{w}^{\\text{MLE}}, \\sigma)\\right] \\] 4.3 The Cube: A Model Comparison Paradigm 4.3.0.1 I. Axis: Probabilistic vs Non-probabilistic Model Definition Pro Con Probabilistic Specifying a distribution for the data (and potentially the model), explicitly defining the source and type of noise When model is “wrong”, we have more assumptions we can interrogate and change in order to improve the model We need to make more choices (e.g. assuming a distribution for the noise); more choices means more chances to be wrong Non-Probabilistic Specify a functional form for the data; does not explicitly define sources and types of data randomness We make less assumptions about the data (which may be wrong); not all useful objective functions (e.g. human personal preference) can be easily given a probabilistic interpretation When the “model” is wrong, we have less assumptions to attribute the problem and hence fewer obvious ways to fix the model Probabilistic: regression by specifying likelihood Non-probabilistic: regression by minimizing MSE, KNN, kernel regression (for now), regression trees (for now) Question: When is it better to use probabilistic regression, when is it better to use non-probabilistic regression? 4.3.0.2 II. Axis: Parametric vs Non-Parametric Model Definition Pro Con Parametric Assumes a fixed, finite functional form \\(f(\\mathbf{x})\\) for regressor Once the parameters of the functional form is learnt, only the parameters needs to be saved Need to guess the right functional form for the regressor Non-Parametric Functional form is implicit and can adapt to observed data No need to specify an explicit functional form Typically, the training data needs to be saved for every prediction Parametric: linear, polynomial, arbitrary basis regression (probabilistic and non-probabilistic); regression trees Non-Parametric: KNN, kernel regression Question: When is it better to use parametric regression, when is it better to use non-parametric regression? 4.3.0.3 III. Axis: Ease of Interpretation Model Example Potential Pro Potential Con Simple Models Models that can be easily inspected in its entirety, models whose decision process can be easily described Through interpretation these models can provide scientific insight, be validated by humans, encourage user trust Simple models may be, though are not necessarily, lacking in capacity to capture complex trends in data Complex Models Models with too many parameters, complex non-linear transformation of data, or whose decision process is lengthy Complex models often have the capacity to capture very interesting trends in the data Complex models may be, though are not necessarily, more difficult to interpret, explain, validate and trust Model Interpretation: interpretability isn’t a binary label, every model can be “interpreted” in some way: - linear regression: looking at regression weights - arbitrary basis regression (including neural networks): looking at regression weights and the features \\(\\phi(\\mathbf{x})\\) - probabilistic regression: looking at regression weights and the noise distribution - regression tree: printing out the tree as a sequence of branching decisions - KNN: looking at the \\(k\\) nearest neighbors - kernel regression: interpreting the kernel \\(k\\) as a measure of similarity between two inputs \\(\\mathbf{x}\\) and \\(\\mathbf{x}&#39;\\) Question: When do we need interpretable models, when does interpretability not matter? Who needs interpretable models? What does interpretable mean? "],["what-matters-in-ml-besides-prediction.html", "Chapter 5 What Matters in ML Besides Prediction? 5.1 What is Machine Learning? Revisited 5.2 What Are We Uncertain About? 5.3 Where is Uncertainty Coming From? 5.4 How Do We Compute Uncertainty? 5.5 Mathematizing Uncertainty: Starting with Bias and Variance 5.6 The Bias-Variance Trade-off in Machine Learning", " Chapter 5 What Matters in ML Besides Prediction? 5.1 What is Machine Learning? Revisited In Lecture #2 we defined machine learning as learning the parameters of function (or of a distribution, if we are being probabilistic) that best fits with observed data. This definition needs refinement! In reality, finding parameters is just sub-goal of a much more complex goal: What does “doing Machine Learning” look like? The short answer: it looks like making and justifying a sequence of choices, while making our assumptions and biases as explicit as possible: Choosing a training data set \\(\\mathcal{D}\\). Question: What assumptions do we make when we make this choice? Choosing a model for the trend in the data \\(f\\) or for the distribution of the data (trend and noise), i.e. the likelihood, \\(p(y \\vert f, \\theta)\\) Question: What assumptions do we make when we make this choice? Choosing a loss function – i.e. a way to measure the fit of \\(f\\) (and potentially \\(\\theta\\)) Question: What assumptions do we make when we make this choice? Choosing a way to optimize the loss function Question: What assumptions do we make when we make this choice? Choosing a way to evaluate the model we learned – we may choose to evaluate the model using a different metric than the loss function! Question: What assumptions do we make when we make this choice? What does a Machine Learning product look like? The short answer: it looks like a technical artifact as well as a recommended policy to guide the appropriate, ethical and responsible usage of it (and potentially much more!). For example, see Model Cards for Model Reporting 5.2 What Are We Uncertain About? If we really work with the idea that everything in machine learning is a choice, including the training data, this means that we could have chosen a different training data set. For example: if we collected our training data from patients in one hospital, we can ask what would have happened if we collected data from a different hospital? Generally speaking, data sets collected at different times, from different locations or from different populations will be slightly (or significantly) different. Thus, the functions \\(f\\) we learn on these datasets will differ and these different functions will produce different predictions for the same test point! So, we should be uncertain about: 1. (Math) the function \\(f\\) we learned (e.g. the parameters \\(\\mathbf{w}\\) for \\(f\\) or the function \\(f\\) itself when our model is non-parametric) 2. (Application) our interpretation of \\(f\\) 3. (Math) our prediction \\(\\hat{y}\\) for a new point \\(\\mathbf{x}\\) 4. (Application) our recommendation for how to make decisions based on our model Question: Why do we care about uncertainty? 5.3 Where is Uncertainty Coming From? Generally speaking: 1. uncertainty in \\(f\\) comes from us not having enough data to uniquely determine a function \\(f\\), this could be because of a combination of the below - \\(f\\) is a complex model (e.g. lots of parameters) compared to the number of training data points (the model is under determined) - the data is very noisy and there are very few observations (so that the trend isn’t clear) 2. uncertainty in our prediction \\(\\hat{y}\\) comes from a combination of the above: - uncertainty in \\(f\\) – if we aren’t sure about \\(f\\) we can’t be sure about \\(\\hat{y}\\) - noise in data – even if we are 100% certain that we have the right \\(f\\), we can still be uncertain about the prediction \\(\\hat{y}\\) due to observation noise Question: Why do we care about what’s causing uncertainty? 5.4 How Do We Compute Uncertainty? If we make some strong assumptions about \\(f\\), as well as the distribution of the data, we can analytically compute the uncertainty in \\(f\\) as well as the uncertainty in \\(\\hat{y}\\). Realistically, we often empirically estimate the uncertainty in \\(f\\) and \\(\\hat{y}\\) through simulating drawing new training data sets by resampling our existing data – this is called bootstrapping. For the different bootstrap training data sets, we learn different functions \\(f\\) and make different predictions \\(\\hat{y}\\). The empirical variance of learnt parameters \\(\\mathbf{w}\\) of \\(f\\) gives us an estimate of the confidence interval of our estimate of \\(\\mathbf{w}\\). The empirical variance of our prediction \\(\\hat{y}\\) gives us an estimate of the predictive interval. 5.5 Mathematizing Uncertainty: Starting with Bias and Variance So far we’ve been describing uncertainty purely in intuitive terms. In order to quantify and analyze uncertainty, we need mathematical formalism! One way to formalize our uncertainty over our prediction is to reason about why our prediction might be wrong. We do so by defining and decomposing the generalization error of our model. \\[\\begin{aligned} \\underbrace{\\mathbb{E}_{(\\mathbf{x}, y), \\mathcal{D}}\\left[ (y- f_\\mathbf{w}(\\mathbf{x}))^2\\right]}_{\\text{Generalization Error}} =&amp; \\mathbb{E}_{\\mathbf{x}}\\underbrace{\\mathrm{Var}[y|\\mathbf{x}]}_{\\text{Observation Noise}}\\\\ &amp; + \\mathbb{E}_{\\mathbf{x}}[(\\underbrace{\\mathbb{E}_{y|\\mathbf{x}}[y|\\mathbf{x}]}_{\\text{Average true $y$}\\\\ \\text{over noisy observations}} - \\underbrace{\\mathbb{E}_\\mathcal{D}[f_\\mathbf{w}(\\mathbf{x})]}_{\\text{Average prediction}\\\\\\text{over all possible training sets}})^2]\\\\ &amp;+\\mathbb{E}_{\\mathbf{x}} \\underbrace{\\mathrm{Var}[f_\\mathbf{w}(\\mathbf{x})]}_{\\text{Variance of Model}}\\\\ &amp;= \\text{Observation Noise} + \\text{Model Bias} + \\text{Model Variance} \\end{aligned}\\] From the math, we see that we have three reasons to be uncertain about our model predictions: 1. the observation noise – even if we are 100% certain that we have the right model, our prediction can still be wrong due to noise 2. model bias – we could have been wildly wrong in our guess of the form of the model (e.g. assuming linear function when modeling quadratic data) 3. model variance – the number of data points is insufficient to uniquely determine the model 5.6 The Bias-Variance Trade-off in Machine Learning One reason we want to work with the formalism of the generalization error is that by decomposing the generalization error, we see how we can reduce our uncertainty in our predictions. Immediately, we see that there is nothing we can do to reduce generalization error arising from observation noise – this error is irreducible. We can, however, choose our model so we have some control over model bias and model variance – these errors are reducible. Unfortunately, generally speaking, when we reduce model bias by making our models more complex, the complexity increases model variance (and vice versa): 5.6.1 Examples of the Bias-Variance Trade-off Many modification we perform on machine models are frequently just ways to manage the Bias-Variance Trade-off. Regularization: adding a penalty term to the MSE loss introduces bias (reduces the ability of the model to fit the data), in order to reduce variance (by biasing the optimization towards simpler models) Ensembling: creating a large set of very different complex models (low bias but high variance), and then reducing the variance by average the model predictions Boosting: iteratively making a simple base model (high bias but low variance) more complex and thereby reducing the bias without significantly increasing the variance "],["what-is-logistic-regression.html", "Chapter 6 What is Logistic Regression? 6.1 Logistic Regression and Soft-Classification 6.2 Logistic Regression and Bernoulli Likelihood 6.3 How to Perform Maximum Likelihood Inference for Logistic Regression 6.4 How (Not) to Evaluate Classifiers 6.5 How to Interpret Logistic Regression", " Chapter 6 What is Logistic Regression? 6.1 Logistic Regression and Soft-Classification One way to motivate and develop logistic regression is by casting it as “soft classification”. That is, instead of find a decision boundary that separates the input domain into two distinct classes, in logistic regression we assign a classification probability of a class to an input based on its distance to the boundary. The math of translating (signed) distance (any real number) into a probability (a number between 0 and 1) requires us to choose a function \\(\\sigma: \\mathbb{R} \\to (0, 1)\\), we typically choose \\(\\sigma\\) to be the sigmoid function, but many other choices are available. This gives us a model for the probability of giving a point \\(\\mathbf{x}\\) the label \\(y=1\\): \\[ p(y = 1 | \\mathbf{x}) = \\sigma(f_{\\mathbf{w}}(\\mathbf{x})) \\] 6.2 Logistic Regression and Bernoulli Likelihood Another way to motivate logistic regression is by: 1. first, model the binary outcome \\(y\\) as a Bernoulli RV, \\(\\mathrm{Ber}(\\theta)\\), where \\(\\theta\\) is the probability that \\(y=1\\). Note: assuming a Bernoulli distribution is assuming a noise distribution. 2. second, incorporate covariates, \\(\\mathbf{x}\\), into our model so that we might have a way to explain our prediction, giving us a likelihood: \\[ y \\vert \\mathbf{x} \\sim \\mathrm{Ber}(\\sigma(f_{\\mathbf{w}}(\\mathbf{x}))) \\] or alternatively, \\[ p(y = 1 | \\mathbf{x}) = \\sigma(f_{\\mathbf{w}} (\\mathbf{x})) \\] 6.3 How to Perform Maximum Likelihood Inference for Logistic Regression Again, we can choose to find \\(\\mathbf{w}\\) by maximizing the joint log-likelihood of the data \\[\\begin{aligned} \\ell(\\mathbf{w}) &amp;= \\log\\left[ \\prod_{n=1}^N \\sigma(f_{\\mathbf{w}} (\\mathbf{x}_n))^{y_n} (1 - \\sigma(f_{\\mathbf{w}} (\\mathbf{x}_n)))^{1-y_n}\\right]\\\\ &amp;= \\sum_{n=1}^N \\left[y_n \\log\\sigma(f_{\\mathbf{w}} (\\mathbf{x}_n)) + (1- y_n)\\log(1 - \\sigma(f_{\\mathbf{w}} (\\mathbf{x}_n)))\\right] \\end{aligned}\\] The Problem: While it’s still possible to write out the gradient of \\(\\ell(\\mathbf{w})\\) (this is already much harder than for basis regression), we can no longer analytically solve for the zero’s of the gradient. The “Solution”: Even if we can’t get the exact stationary points from the gradient. The gradient still contains useful information – i.e. the negative gradient at a point \\(\\mathbf{w}\\) is the direction of the fastest instantaneous increase in \\(\\ell(\\mathbf{w})\\). By following the gradient “directions”, we can “climb down” the graph of \\(\\ell(\\mathbf{w})\\). 6.4 How (Not) to Evaluate Classifiers Rule 1: Never just look at accuracy. Rule 2: Look at all possible trade-offs that a classifier makes (for whom is the classifier correct and for whom it is not). 6.5 How to Interpret Logistic Regression For logistic regression with linear boundaries, there are very intuitive ways to interpre the model: But are these “easy” interpretations reliable? "],["how-do-we-responsibly-use-conditional-models.html", "Chapter 7 How Do We Responsibly Use Conditional Models? 7.1 Everything We’ve Done So Far in Probabilistic ML", " Chapter 7 How Do We Responsibly Use Conditional Models? 7.1 Everything We’ve Done So Far in Probabilistic ML For a set of observations \\(y\\), we can collect additional data \\(\\mathbf{x}\\) and model the conditional distribution \\(p(y|\\mathbf{x})\\). This is exactly regression and classification models do, they use covariates \\(\\mathbf{x}\\) to predict the outcome \\(y\\). (Conditional Models) Why do we need additional data \\(\\mathbf{x}\\)? Why isn’t it sufficient to just model \\(y\\) by itself (as a binomial, Guassian or a mixture of Gaussians)? to predict to explain to model finer grain variations in y (i.e. model data heterogeneity) (Classification) For binary data (like whether or not an individual has kidney cancer), we can model \\(p(y|\\mathbf{x})\\) as a Bernoulli \\[ y \\sim \\mathrm{Ber}\\left(\\mathrm{sigm}\\left(f_\\mathbf{w}\\left(\\mathbf{x} \\right)\\right)\\right). \\] (Maximizing the Likelihood) We want to maximize the log Bernoulli likelihood of \\(y\\) (conditioned on \\(\\mathbf{x}\\)) over model parameters \\(\\mathbf{w}\\), but the zeros of the gradient of the log-likelihood cannot be analytically solved for! \\[ \\nabla_{\\mathbf{w}} \\ell(\\mathbf{w}) = -\\sum_{n=1}^N \\left(y_n - \\frac{1}{1 + e^{-\\mathbf{w}^\\top\\phi(\\mathbf{x}_{n})}} \\right) \\phi(\\mathbf{x}_{n}) =\\mathbf{0} \\] But just because we can’t analytically find the zeros of the gradient, it doesn’t mean that the gradient is useless! The gradient of a loss function points to the direction of the greatest rate of increase, hence the negative gradient points to the direction of the fastest way of decreasing the loss function. When we iteratively decrease the loss function by changing \\(\\mathbf{w}\\) in the direction of the negative gradient, this is called gradient descent. (Properties of Gradient Descent) Gradient descent sounds like we can optimize any function without work, is it as good as it sounds? What do I need to know in order to use gradient descent? Is gradient descent guaranteed to converge? Is gradient descent efficient? (Model Evaluation) How do I know my model is “correct”? Choose a meaningful numeric predictive metric (e.g. MSE, log-likelihood, accuracy, AUC etc) – which metric measures the model behavior that I actually care about? Evaluate your model along other important real-life axes: Probabilistic vs Non-probabilistic Parametric vs Non-parametric Interpretability (The Challenges of Conditional Models) Conditional models have so much potential to generate scientific understanding of our data but they are also the most frequently misused! Suppose that you fit a logistic regression model to predict whether a loan application should be approved. Suppose that you have three covariates: x_1 representing gender: 0 for male, 1 for female x_2 for the income x_3 for the loan amount Suppose that the parameters you found are: \\[ p(y=1 | x_1, x_2, x_3) = \\mathrm{sigm}(-1 + 3 x_1 + 1.5 x_2 + 1.75 x_3). \\] What can I conclude about the loan decisions data that this model was fitted on? Can I deploy this model for usage? Bias in your conclusions is exponentially exacerbated by bias in the data collection process. What information are you asking for? If you ask the wrong question, the covariates you collect are at best unpredictive, and at worse they can mask the signal in the data. In many data collection processes, one finds that the questionaire might be using “sex” and “gender” interchangeably, or “race” and “ethnicity” interchageably. But the former (depending on field and context) frequently refers to biological invariants (like chromosomes and genetic lineage), whereas the latter frequently refers to lived experiences (related to identity, culture etc). Caveat: although we collect data on race and sex as if delineation in these categories are more “immutable” or “factual”, many have/do argue that these concepts are also social constructs (see discussion on false binaries and false aggregtaion)! When you ask for “sex” are you asking about a person’s chromosomal information, becaues you’re studying chromosomal related diseases, or are you asking about their experiences with the US medical system and how their health outcome is affected by their gendery identity OR the identity that people tend to assume of them? For example: women, especially women of color, tend to be under-diagnosed and mis-diagnosed due to gendered treatments of patients. To complicate matters, the terms “gender” and “sex” can have very specific legal definitions – that may not align with defintions in medical or sociological contexts! For example, the language of the Equal Credit Opportunity Act is written in terms of “sex”, however, this term is not defined in the ECOA. The Consumer Financial Protection Bureau interprets (in their operating philosophy) this term to include both sexual orientation and gender identity. Did you already bias the answer? If you gave confusing, misleading or incorrect options for answers (think about when you were frustrated with the concept quizzes!), then the covariates you collect are at best unpredictive, and at worse they can mask the signal in the data. In many data collection processes, one finds that the respondent is forced into a category that is unrepresentative of their experience, like having “Asian” as a broad racial category that includes South Asian, East Asian, South East Asian, like providing binary choices for gender or sex. If you are looking for disparaties in health outcomes, you might miss disparaties amongst important smaller subgroups within broad group of “Asian”, since health outcomes are often affected by income and Asian-Americans have the largest in-group income disparity. Should you be asking for this information? The complexity of asking a seemingly simple question can seem overwhelming, often we resort to over-collecting (collecting every piece of information that we can think of) or under-collecting (not collecting any sensitive attribute). Why you might not want to But respondents (e.g. patients, applicants for benefits) can be suspicious that their data can be used precisely to discriminate against them on the basis of protected attributes. Note that data collection have often historically been used to oppress, not to up-lift, vunerable communities. So collecting this data without clear purpose can compromise user trust and constitute a privacy threat if this data is not well protected. Why you should However, in order to make sure our model is not disproportionately negatively impacting vunerable subgroups and that we are in compliancenb with laws like the EOCA, we need to check for disparate impact on protected groups. But in order to do that, we need to have access to sensitive attributes. Is correlation causation? It appears that the sex of the applicant the logistic regression model has the greatest influence on the prediction of the model, does this mean that the human decision makers were biased? (Hint: not necssarily, but it might be a hypothesis to explore to retroactively detect potential regulatory violation in this dataset.) If the interpretations of the coefficients of the logistic regression model does not imply human bias in the data, then can we use this model for real life loan decisions? (Hint: the answer is NO. It would be an violation!) "],["case-study-responsibly-using-logistic-regression.html", "Chapter 8 Case Study: Responsibly Using Logistic Regression 8.1 Case Study: Machine Learning Model for Loan Approval", " Chapter 8 Case Study: Responsibly Using Logistic Regression Motto: Being a machine learning researcher/practitioner is like being a doctor not like constructing a house according to blueprint. That is, you are using theory to diagnose problems and suggest treatment, you are executing actions based on a blueprint. Motto for Resolving Choice Paralysis: Non-action is the action of upholding the status quo! 8.1 Case Study: Machine Learning Model for Loan Approval Suppose that you have been asked by the International Bank of Z to use see if it’s possible to use ML to automate their loan approval process in order to reduce the cost of retaining a large staff of expert loan officers. They give you a training dataset consisting of 10,000 loan applications considered between 2002 and 2010 at the International Bank of Z. In this training set you have the following information: &gt;x_1 representing gender: 0 for male, 1 for female &gt;x_2 represents income &gt;x_3 represents personal debt &gt;x_4 represents the loan amount &gt;x_5 represents credit score &gt; y represents whether or not the loan was approved You do a 70/15/15 split of the historical data you have into training, validation and test. You fit a logistic regression model to predict whether a loan application should be approved. Suppose that you found the following parameters by maximizing the joint log-likelihood of your training data using your own implementation of gradient descent: \\[ p(y=1 | \\mathbf{x}) = \\sigma(-1 + 3 x_1 + 1.5 x_2 - 1.75 x_3 + 0.6 x_4 - 0.01 x_5). \\] For a new loan application, you approve if \\(p(y=1 | \\mathbf{x}) \\geq 0.5\\) and deny if \\(p(y=1 | \\mathbf{x}) &lt; 0.5\\). You computed the following metrics: Metric Train Validation Accuracy 49% 51% AUC 0.51 0.52 Log-likelihood -307.760577547 -407.547760577 8.1.1 The Big Vague Question Is this a good model? Do I present it to my clients tomorrow as a solution to their problem? If so, how will I convince them that this ML model is going to solve their business problem? 8.1.2 The Concrete and Rigorous Process of Post-Inference Analysis of Machine Learning Models (Inference) Did I do inference correctly? Did I write down the correct objective function and did I implement it correctly? &gt; You chose to maximize the log-likelihood since that’s pretty standard but you know that MLE has potential drawbacks. You check your numpy implementation of the objective function and it looks correct. &gt; &gt; You move on and will come back to check more carefully if you don’t find other issues. Did I optimize my objective function correctly? &gt; You checked that your gradient descent was implemented correctly and that your loss function was decreasing during gradient descent. &gt; &gt; You move on and will come back to check more carefully if you don’t find other issues. (Bias-Variance Trade-off) Did I choose the wrong model class? Am I underfitting (i.e. my model’s prediction on new data will have high error because it has high bias)? &gt; You check for underfitting by reducing the potential bias in your model. You try a polynomial feature map \\(\\phi\\) of degree 3 and get: &gt; \\[\\begin{aligned} p(y=1 | \\mathbf{x}) = \\sigma(&amp;-50 + 4 x_1 + 17 x_2 + 0.0001 x_3 + 0.2 x_4 + 9 x_5\\\\ &amp;- 0.0012 x^2_1 + 0.15 x^2_2 + 0.2226 x^2_3 - 0.26 x^2_4 + x^2_5\\\\ &amp;+ 1.2 x^3_1 - 1.0001 x_2 - 0.34 x^3_3 + 0.5 x^3_4 + x^3_5)\\\\ \\end{aligned}\\] Now your metrics look like: | Metric | Train | Validation | | ——– | ——– | ——– | | Accuracy | 98% | 85% | | AUC | 0.8 | 0.7 | | Log-likelihood | -1.2349e-9 | -7.547760577 | Am I overfitting (i.e. my model’s prediction on new data will have high error because it has high variance)? &gt; It looks like adding your polynomial feature map caused overfitting! Your generalization error will be high due to high model variance. For now, you try to reduce the variance of your model by increasing the potential bias of your model – by (1) using only degree 2 polynomials and by (2) adding \\(\\ell_1\\) regularization: &gt; \\[\\begin{aligned} p(y=1 | \\mathbf{x}) = \\sigma(&amp;-0.5 + 2.1 x_1 + 0.0001 x_3 + 0.2 x_4 + 0.15 x^2_2 + 0.063x^2_5) \\end{aligned}\\] Now your metrics look like: | Metric | Train | Validation | | ——– | ——– | ——– | | Accuracy | 89% | 86% | | AUC | 0.79 | 0.81 | | Log-likelihood | -4.9812e-3 | -5.21774e-3 | Since your train validation metrics look similar and similarly high-ish, you believe you’ve manage your bias-variance trade-off well. You move on and will come back to check more carefully if you need to. (Choose Meaningful Evaluation Metrics) Of the metrics that I know how to compute, which metrics should I use to evaluate my model? Do my metrics agree? Is it ok if they disagree? &gt; In this case, accuracy is higher than AUC – is this a deal breaker?! &gt; Not necessarily. Recall from that AUC is defined as the area under the ROC curve. Roughly, the AUC is measuring how well your model does over all possible classification thresholds. In contrast, the accuracy is measuring how well your model does for the threshold 0.5. &gt; As long as you are making good decisions using the threshold 0.5, you may not need to worry about the AUC. &gt; &gt; You move on and will come back to check more carefully if you need to. &gt; Which of my chosen metrics should I prioritize? &gt; If your client is just interested in making accurate decisions using your model, maybe prioritizing accuracy at a threshold of 0.5 is fine for this down-stream task. &gt; &gt; You move on and will come back to check more carefully if you need to. (Task-Sensitive Evaluation) But really, how should I choose to evaluate my model? What are my real-life goals? Do any of my metrics correspond to any real-life performance goals? &gt; In this case, your client most likely wants to avoid approving bad loans. This means that they care want the model to predict \\(y=0\\) when the loan would have been rejected by a human expert. Does a high accuracy mean that we’ve achieved this goal? &gt; &gt; You check the balance of the two classes in your loan data and see that about 75% of the loans were approved – there is class imbalance. &gt; &gt; You then check the training confusion matrix and see: &gt; &gt;| | Predicted 0| Predicted 1 | &gt;| ——– | ——– | ——– | &gt;| y=0 | 1488 | 262 | &gt;| y=1 | 578 | 4672 | &gt; &gt; Luckily, despite the class imbalance, your model does equally well on both classes! Have I interpreted my model? Does the interpretation make sense in the context of the problem? Have I made use of all available application domain knowledge? So far, your model seem to be doing well on your numeric metrics, does it mean that it’s right for the right reasons? Let’s interpre the model and see how it makes decisions: \\[\\begin{aligned} p(y=1 | \\mathbf{x}) = \\sigma(&amp;-0.5 + 2.1 x_1 + 0.0001 x_3 + 0.2 x_4 + 0.15 x^2_2 + 0.063x^2_5) \\end{aligned}\\] You note several things right away: (1) it seems like “gender”, \\(x_1\\), has the biggest impact on probability of approval (2) the loan amount and debt positively impact the probability of approval! Do these model properties make sense? You worry that the coefficients cannot be reliably interpreted because the covariates are measured at different scales. So you check and are relieved to see that the data has been standardized and all take value between -1 and 1. This gives you some confidence that your interpretation of coefficients are not obviously wrong It seems counter-intuitive in your model that the more loans you take out and the more debt you have, the more likely your loan will be approved. Maybe there’s a bug in your optimization or coding?! Not necessarily! If you consult a domain expert, they will tell you that often times loan amount is correlated with income, as is debt amount (this is why people look at income to debt ratio). That is, co-linearity amongst your covariates might explain the counter-intuitive coefficients. You compute the Pearson corrlation between \\(x_3\\) and \\(x_2\\), then \\(x_4\\) and \\(x_2\\); you find that both loan amount and debt are slightly correlated with income in the data. You move on and will come back to check more carefully if you need to. What are your uncertainties and where are they? You bootstrap your training data 100 times and estimate confidence intervals on the coefficients of your logisic regression and get the following means and 95% confidence intervals: \\[\\begin{aligned} &amp;w_0 \\text{(bias term)}: 0.5 \\pm 1.2\\\\ &amp;w_1 \\text{(for $x_1$)}: 0.05 \\pm 1.9,\\\\ &amp;w_2\\text{(for $x^2_2$)}: 0.1 \\pm 2.12, \\\\ &amp;w_3\\text{(for $x_3$)}: 0.108 \\pm 1.3, \\\\ &amp;w_4\\text{(for $x_4$)}: 0.08 \\pm 0.9, \\\\ &amp;w_5\\text{(for $x^2_5$)}: 0.25 \\pm 1.34 \\end{aligned}\\] As the confidence intervals of the coefficients overlap a lot, it’s not reasonable for us to be certain about our model interpretation – e.g. that “gender” has the greatest impact on loan approval probability. (Model Critique) Revisit every design decision – did I make the wrong decision? Did I choose the right data set? Are there forseeable issues with my data set? &gt; You worry that outliers, mistakes, missingness and other odd features of the data might be biasing your model. You should check the quality of your data by looking at data summaries (mean, median, variance, range and percentage missing for each covariate) and visualizations (scatter plots of your data, choosing a couple of covariates at a time). &gt; &gt; You also sanity-check that the data summaries match your expectation of what are reasonable values. You notice that the data dictionary says \\(x_1\\) represents gender (i.e. related to lived-experiences) but the values encode “male” and “female” (i.e. related to biological invariants). You wonder if the bank mislabled this covariate (i.e. \\(x_1\\) should represent sex rather than gender). &gt; &gt; You also noticed that the “gender” covariate is coded as a binary, this would cause non-binary applicants to be misclassified (misclassification potentially introduces noise). &gt; Did I choose the right model type and right model class? &gt; You struggled a bit interpreting logistic regression with polynomial features. Would KNN or decision tree be easier to interpret (Hint: the model decision process might be easier to explain but validating your interpretation would be harder, as there are fewer exposed assumptions)? &gt; Did I choose the right loss function and the right optimization procedure? &gt; You wonder if you should have maximized likelihood if what you really wanted to check was accuracy. What happens if you try to optimize for accuracy directly (Hint: you’ll have problems differentiating)? (Model Deployment) Should I use this model? If so how? Did your model pass all your checks above? &gt; Running through the above, you’ve already flagged a potential issue: while accurate, your model (as it stands) cannot be reliably interpreted. &gt; &gt; Is this a problem? &gt; &gt; You realize that your client International Bank of Z is based in the EU, and thus under The General Data Protection Regulation, the bank may be legally required to provide an explanation for each loan decision as well as a recommendation for recourse in case the loan is rejected. In this case, it is critical that we can provide a reliable interpretation of the model’s decision making process. &gt; &gt; You also flagged that the “gender” covariate might actually be containing sex information rather than gender, the binary encoding of this covariate also causes potential misclassification of non-binary applicants. &gt; &gt; Is this a problem? &gt; &gt; Incorrect encoding and mislabeling introduces noise into the data, potentially making the tasks of modeling and model interpretation much harder! &gt; Is your dataset legally obtained for the purposes of building predictive models? &gt; You realize that you need to check with the bank whether or not the loan applicants had given legal consent for their data to be used by a 3rd-party body for analysis! &gt; &gt; Just because the bank has the right to analyze historical loan application data it does not necessarily mean that the bank is authorized to released the data to 3rd-parties! &gt; How will your model be used in decision making once deployed? Is it legal to use your model as intended? &gt; As described in your project spec, the bank wants to rely on your model to make loan decisions, replacing human loan officers for most loan applications. &gt; &gt; As it stands, it may not be legal for your model to make loan decisions or to even inform loan decisions in the US! Laws like the Equal Credit Opportunity Act prohibits basing loan decisions, for example, on sex. Given your interpretation, sex has the biggest impact on the classification probability of your logistic regression! &gt; &gt; To complicate matters, the terms “gender” and “sex” can have very specific legal definitions – that may not align with defintions in medical or sociological contexts! For example, the language of the Equal Credit Opportunity Act is written in terms of “sex”, however, this term is not defined in the ECOA. The Consumer Financial Protection Bureau interprets (in their operating philosophy) this term to include both sexual orientation and gender identity. &gt; Is your model serving the needs of your users? Is your model harming the public good (e.g. who is your affected community and how are they being impacted)? Is your model causing disparate impact? &gt; In this case, the users of our model are bank employees, but the most significant affected community is that of loan applicants. It can very well be the case that the bank might be perfectly happy with your model but loan applicants will not be – for example, if your model systemmatically denies loan applications from a specific subpopulation. &gt; Given that you only have sex information but no other demographic covariates, you can check if your model is equally accurate for male applicants as it is for female applicants: \\[\\begin{aligned} &amp;\\texttt{male}: 89\\% \\text{ test accuracy}\\; &amp;\\texttt{female}: 85\\% \\text{ test accuracy} \\end{aligned}\\] While the test accuracy for male applicants is slightly higher, the two groups are comparable. Does this mean that your model will not cause disparate impact? No! Just because that your model is accurate in predicting a human loan officer’s decision, it does not make your model fair! In fact, you check the observed loan approval rates in your data set and find that over 80% of loans from female applicants were rejected while over 60% of male applicants were approved. This is very much disparate impact! If your model captured this decision process perfectly, you would be automating and perpetuating this disparate impact! "],["the-math-of-training-and-interpreting-logistic-regression-models.html", "Chapter 9 The Math of Training and Interpreting Logistic Regression Models 9.1 The Math of Convex Optimization 9.2 Important Mathy Details of Gradient Descent 9.3 Interpreting a Logistic Regression Model: Log-Odds", " Chapter 9 The Math of Training and Interpreting Logistic Regression Models 9.1 The Math of Convex Optimization A convex set \\(S\\subset \\mathbb{R}^D\\) is a set that contains the line segment between any two points in \\(S\\). Formally, if \\(x, y \\in S\\) then \\(S\\) contains all convex combinations of \\(x\\) and \\(y\\): \\[ tx + (1-t) y \\in S,\\quad t\\in [0, 1]. \\] This definition is illustrated intuitively in the below: A function \\(f\\) is a convex function if domain of \\(f\\) is a convex set, and the line segment between the points \\((x, f(x))\\) and \\((y, f(y))\\) lie above the graph of \\(f\\). Formally, for any \\(x, y\\in \\mathrm{dom}(f)\\), we have \\[ \\underbrace{f(tx + (1-t)y)}_{\\text{height of graph of $f$}\\\\ \\text{at a point between $x$ and $y$}} \\quad \\leq \\underbrace{tf(x) + (1-t)f(y)}_{\\text{height of point on line segment}\\\\ \\text{between $(x, f(x))$ and $(y, f(y))$}},\\quad t\\in [0, 1] \\] This definition is intuitively illustrated below: How do we check that a function \\(f\\) is convex? If \\(f\\) is differentiable then \\(f\\) is convex if the graph of \\(f\\) lies above every tangent plane. Theorem: If \\(f\\) is differentiable then \\(f\\) is convex if and only if for every \\(x \\in \\mathrm{dom}(f)\\), we have \\[ \\underbrace{f(y)}_{\\text{height of graph of $f$ over $y$}} \\geq \\underbrace{f(x) + \\nabla f(x)^\\top (y - x)}_{\\text{height of plane tangent to $f$ at $x$, evaluated over $y$}},\\quad \\forall y\\in \\mathrm{dom}(f) \\] This theorem is illustrated below. The theorem can be made intuitive, but takes a bit of unwrapping. Don’t worry about the exact statement, it suffices to know that there is a condition we can check to see if a function is convex. Luckily, checking a twice-differentiable function is convex is (relatively) easier! If \\(f\\) is twice-differentiable then \\(f\\) is convex if the “second derivative is positive”. Theorem: If \\(f\\) is twice-differentiable then \\(f\\) is convex if and only if the Hessian \\(\\nabla^2 f(x)\\) is positive semi-definite for every \\(x\\in \\mathrm{dom}(f)\\). Verifying that a complex function \\(f\\) is complex can be difficult (even if \\(f\\) is twice-differentiable). More commonly, we show that a complex function is convex because it is made from simple convex functions using a set of allowed operations. Properties of Convex Functions. How to build complex convex functions from simple convex functions: if \\(w_1, w_2 \\geq 0\\) and \\(f_1, f_2\\) are convex, then \\(h = w_1 f_1 + w_2 f_2\\) is convex if \\(f\\) and \\(g\\) are convex, and \\(g\\) is univariate and non-decreasing then \\(h = g \\circ f\\) is convex Log-sum-exp functions are convex: \\(f(x) = \\log \\sum_{k=1}^K e^{x}\\) Note: there are many other convexity preserving operations on functions. A convex optimization problem is an optimization of the following form: \\[\\begin{aligned} \\mathrm{min}\\; &amp;f(x) &amp; (\\text{convex objective function})\\\\ \\text{subject to}\\; &amp; h_i(x) \\leq 0, i=1, \\ldots, i &amp; (\\text{convex inequality constraints}) \\\\ &amp; a_j^\\top x - b_j = 0, j=1, \\ldots, J &amp; (\\text{affine equality constraints}) \\\\ \\end{aligned}\\] The set of points that satisfy the constraints is called the feasible set. You can prove that the a convex optimization problem optimizes a convex objective function over a convex feasible set. But why should we care about convex optimization problems? Theorem: Let \\(f\\) be a convex function defined over a convex feasible set \\(\\Omega\\). Then if \\(f\\) has a local minimum at \\(x\\in \\Omega\\) – \\(f(y) \\geq f(x)\\) for \\(y\\) in a small neighbourhood of \\(x\\) – then \\(f\\) has a global minimum at \\(x\\). Corollary: Let \\(f\\) be a differentiable convex function: 1. if \\(f\\) is unconstrained, then \\(f\\) has a local minimum and hence global minimum at \\(x\\) if \\(\\nabla f(x) = 0\\). 2. if \\(f\\) is constrained by equalities, then \\(f\\) has a global minimum at \\(x\\) if \\(\\nabla J(x, \\lambda) = 0\\), where \\(J(x, \\lambda)\\) is the Lagrangian of the constrained optimization problem. Note: we can also characterize the global minimum of inequalities constrained convex optimization problems using the Lagrangian, but the formulation is more complicated. 9.1.1 Convexity of the Logistic Regression Negative Log-Likelihood But why do we care about convex optimization problems? Let’s connect the theory of convex optimization to MLE inference for logistic regression. Recall that the negative log-likelihood of the logistic regression model is \\[\\begin{aligned} -\\ell(\\mathbf{w}) &amp;= -\\sum_{n=1}^N y^{(n)}\\,\\log\\,\\mathrm{sigm}(\\mathbf{w}^\\top \\mathbf{x}^{(n)}) + (1 - y^{(n)}) \\log (1 -\\mathrm{sigm}(\\mathbf{w}^\\top \\mathbf{x}^{(n)}))\\\\ &amp;= -\\sum_{n=1}^N y^{(n)}\\,\\log\\,\\frac{1}{1 + e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}}} + (1 - y^{(n)}) \\log \\left(1 -\\frac{1}{1 + e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}}}\\right)\\\\ &amp;= -\\sum_{n=1}^N y^{(n)}\\left(\\log(1) - \\log(1 + e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}})\\right) + (1 - y^{(n)}) \\log \\left(\\frac{1 + e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}}}{1 + e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}}} -\\frac{1}{1 + e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}}}\\right)\\\\ &amp;= -\\sum_{n=1}^N -y^{(n)} \\log(1 + e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}}) + (1 - y^{(n)}) \\log \\left( \\frac{e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}}}{1 + e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}}}\\right)\\\\ &amp;= -\\sum_{n=1}^N -y^{(n)} \\log(1 + e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}}) + (1 - y^{(n)}) \\left(\\log \\left( {e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}}}\\right) - \\log\\left({1 + e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}}}\\right)\\right)\\\\ &amp;= -\\sum_{n=1}^N -y^{(n)} \\log(1 + e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}}) + (1 - y^{(n)}) \\log \\left( {e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}}}\\right) - (1 - y^{(n)})\\log\\left({1 + e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}}}\\right)\\\\ &amp;= -\\sum_{n=1}^N -y^{(n)} \\log(1 + e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}}) + (1 - y^{(n)}) (-\\mathbf{w}^\\top \\mathbf{x}^{(n)}) - \\log\\left({1 + e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}}}\\right) + y^{(n)}\\log\\left({1 + e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}}}\\right)\\\\ &amp;= -\\sum_{n=1}^N (1 - y^{(n)}) (-\\mathbf{w}^\\top \\mathbf{x}^{(n)}) - \\log\\left({1 + e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}}}\\right) \\\\ &amp;= -\\sum_{n=1}^N (1 - y^{(n)}) (-\\mathbf{w}^\\top \\mathbf{x}^{(n)}) + \\log\\left(\\frac{1}{1 + e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}}}\\right) \\\\ &amp;= -\\sum_{n=1}^N -\\mathbf{w}^\\top \\mathbf{x}^{(n)} + y^{(n)}\\mathbf{w}^\\top \\mathbf{x}^{(n)} + \\log\\left(\\frac{1}{1 + e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}}}\\right) \\\\ &amp;= -\\sum_{n=1}^N \\log e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}} + y^{(n)}\\mathbf{w}^\\top \\mathbf{x}^{(n)} + \\log\\left(\\frac{1}{1 + e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}}}\\right) \\\\ &amp;= -\\sum_{n=1}^N y^{(n)}\\mathbf{w}^\\top \\mathbf{x}^{(n)} + \\log\\left(\\frac{e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}}}{1 + e^{-\\mathbf{w}^\\top \\mathbf{x}^{(n)}}}\\right) \\\\ &amp;= -\\sum_{n=1}^N y^{(n)}\\mathbf{w}^\\top \\mathbf{x}^{(n)} + \\log\\left(\\frac{1}{e^{\\mathbf{w}^\\top \\mathbf{x}^{(n)}} + 1}\\right) \\\\ &amp;=-\\sum_{n=1}^N y^{(n)}\\mathbf{w}^\\top \\mathbf{x}^{(n)} + \\log\\left({1}\\right) - \\log \\left({e^{\\mathbf{w}^\\top \\mathbf{x}^{(n)}} + 1}\\right) \\\\ &amp;=-\\sum_{n=1}^N y^{(n)}\\mathbf{w}^\\top \\mathbf{x}^{(n)} - \\log \\left({e^{\\mathbf{w}^\\top \\mathbf{x}^{(n)}} + 1}\\right) \\\\ &amp;=\\sum_{n=1}^N \\log \\left({e^{\\mathbf{w}^\\top \\mathbf{x}^{(n)}} + 1}\\right) - y^{(n)}\\mathbf{w}^\\top \\mathbf{x}^{(n)} \\\\ &amp;=\\sum_{n=1}^N \\log \\left({e^{\\mathbf{w}^\\top \\mathbf{x}^{(n)}} +e^{0}}\\right) - y^{(n)}\\mathbf{w}^\\top \\mathbf{x}^{(n)} \\\\ \\end{aligned}\\] Proposition: The negative log-likelihood of logistic regression \\(-\\ell(\\mathbf{w})\\) is convex. What does this mean for gradient descent? If gradient descent finds that \\(\\mathbf{w}^*\\) is a stationary point of \\(-\\nabla_{\\mathbf{w}}\\ell(\\mathbf{w})\\) then \\(-\\ell(\\mathbf{w})\\) has a global minimum at \\(\\mathbf{w}^*\\). Hence, \\(\\ell(\\mathbf{w})\\) is maximized at \\(\\mathbf{w}^*\\). Proof of the Proposition: Note that 1. \\(- \\mathbf{w}^\\top \\mathbf{x}^{(n)}\\) and \\(y^{(n)}(\\mathbf{w}^\\top \\mathbf{x}^{(n)})\\) are convex, since they are linear 2. \\(\\log(e^0 + e^{\\mathbf{w}^\\top \\mathbf{x}^{(n)}})\\) is convex since it is the composition of a log-sum-exp function (which is convex) and a convex function \\(\\mathbf{w}^\\top \\mathbf{x}^{(n)}\\) 3. \\(\\sum_{n=1}^N \\log(e^0 + e^{\\mathbf{w}^\\top \\mathbf{x}^{(n)}})\\) is convex since it is a nonnegative linear combination of convex functions 4. \\(-\\ell(\\mathbf{w})\\) is convex since it is the sum of two convex functions 9.2 Important Mathy Details of Gradient Descent Gradient descent as an algorithm is intuitive to understand: follow the arrow pointing to the fastest way down (i.e. the negative gradient). Intuitively, it seems like this heuristic would get us to at least the bottom of a valley. But can we formally prove that gradient descent has desirable properties? 9.2.1 Does It Converge? We’ve seen that if we choose the learning rate to be too large (say for example 1e10 then gradient descent can fail to converge even if the function \\(f\\) is convex. But how large is “too large”. There are two cases to consider You have some prior knowledge about how smooth the function \\(f\\) is – i.e. how quickly \\(f\\) can increase or decrease. Then using this you can choose a learning rate that will provably guaratee convergence In most cases, the objective function (like the log-likelihood) may be too complex to reason about. In which case, we do a scientific “guess-and-check” to determine the learning rate: - we find a learning rate that is large enough to cause gradient descent to diverge - we find a leanring rate that is small enough to cause gradient descent to converge too slowly - we choose a range of values between the large rate and the small rate and try them all to determine the optimal rate alternatively, we can choose the step-size \\(\\eta\\) adaptively (e.g. when the gradient is large we can set \\(\\eta\\) to be moderate to small and when the gradient is small we can set \\(\\eta\\) to be larger). There are a number of adaptive step-size regimes that you may want to look up and implement for your specific problem. The prior knowledge required to choose \\(\\eta\\) for provable convergence is called Lipschitz continuity. If we knew that \\(f\\) is convex, differentiable and that there is a constant \\(L&gt;0\\) such that \\(\\|\\nabla f(x) - \\nabla f(y)\\|_2 \\leq L\\|x -y\\|_2\\), then if we choose a fixed step size to be \\(\\eta \\leq \\frac{1}{L}\\) then gradient descent provably converges to the global minimum of \\(f\\) as the number of iterations \\(N\\) goes to infinity. The constant \\(L\\) is called the Lipschitz constant. 9.2.2 How Quickly Can We Get There? Just because we know gradient descent will converge it doesn’t mean that it will give us a good enough approximation of the global minimum within our time limit. This is why studying the rate of convergence of gradient descent is extremely important. Again there are two cases to consider You have prior knowledge that \\(f\\) is convex, differentiable and its Lipschitz constant is \\(L\\) and suppose that \\(f\\) has a global minimum at \\(x^*\\), then for gradient descent to get within \\(\\epsilon\\) of \\(f(x^*)\\), we need \\(O(1/\\epsilon)\\) number of iterations. In most cases, the objective function will fail to be convex and its Lipschitz constant may be too difficult to compute. In this case, we simply stop the gradient descent when the gradient is sufficiently small. 9.2.3 Does It Scale? Gradient descent is such a simple algorithm that can be applied to any optimization problem for which you can compute the gradient of the objective function. Question: Does this mean that maximum likelihood inference for statistical models is now an easy task (i.e. just use gradient descent)? For every likelihood optimization problem, evaluating the gradient at a set of parameters \\(\\mathbf{w}\\) requires evaluating the likelihood of the entire dataset using \\(\\mathbf{w}\\): \\[ \\nabla_{\\mathbf{w}} \\ell(\\mathbf{w}) = -\\sum_{n=1}^N \\left(y^{(n)} - \\frac{1}{1 + e^{-\\mathbf{w}^\\top\\mathbf{x}^{(n)}}} \\right) \\mathbf{x}^{(n)} =\\mathbf{0} \\] Imagine if the size of your dataset \\(N\\) is in the millions. Naively evaluating the gardient just once may take up to seconds or minutes, thus running gradient descent until convergence may be unachievable in practice! Idea: Maybe we don’t need to use the entire data set to evaluate the gradient during each step of gradient descent. Maybe we can approximate the gradient at \\(\\mathbf{w}\\) well enough with just a subset of the data. 9.3 Interpreting a Logistic Regression Model: Log-Odds A more formal way of interpreting the parameters of the logistic regression model is through the log-odds. That is, we solve for \\(\\mathbf{w}^\\top\\mathbf{x}\\) in terms of \\(\\text{Prob}(y = 1 | \\mathbf{x})\\). \\[\\begin{aligned} \\text{Prob}(y = 1 | \\mathbf{x}) &amp;= \\text{sigm}(\\mathbf{w}^\\top\\mathbf{x})\\\\ \\text{sigm}^{-1}(\\text{Prob}(y = 1 | \\mathbf{x})) &amp;= \\mathbf{w}^\\top\\mathbf{x}\\\\ \\log \\left( \\frac{\\text{Prob}(y = 1 | \\mathbf{x})}{1 - \\text{Prob}(y = 1 | \\mathbf{x})}\\right)&amp;= \\mathbf{w}^\\top\\mathbf{x}\\\\ \\log \\left( \\frac{\\text{Prob}(y = 1 | \\mathbf{x})}{\\text{Prob}(y = 0 | \\mathbf{x})}\\right)&amp;= \\mathbf{w}^\\top\\mathbf{x} \\end{aligned}\\] where we used the fact that \\(\\text{sigm}^{-1}(z) = \\log\\left(\\frac{z}{1 - z}\\right)\\). The term \\(\\log \\left( \\frac{\\text{Prob}(y = 1 | \\mathbf{x})}{\\text{Prob}(y = 0 | \\mathbf{x})}\\right)\\) is essentially a ratio of the probability of \\(y=1\\) and the probability of \\(y=0\\), we can interpret this quantity as the (log) odds of you winning if you’d bet that \\(y=1\\). Thus, we can imagine the parameter \\(w_d\\) for the covariate \\(x_d\\) as telling us if the odd of winning a bet on \\(y=1\\) is good. if \\(w_d &lt; 0\\), then by increasing \\(x_d\\) (while holding all other covariates constant) we make \\(\\mathbf{w}^\\top\\mathbf{x}\\) more negative, and hence the ratio \\(\\frac{\\text{Prob}(y = 1 | \\mathbf{x})}{\\text{Prob}(y = 0 | \\mathbf{x})}\\) closer to 0. That is, when \\(w_d &lt; 0\\), increasing \\(x_d\\) decreases our odds. if \\(w_d &gt; 0\\), then by increasing \\(x_d\\) (while holding all other covariates constant) we make \\(\\mathbf{w}^\\top\\mathbf{x}\\) more positive, and hence the ratio \\(\\frac{\\text{Prob}(y = 1 | \\mathbf{x})}{\\text{Prob}(y = 0 | \\mathbf{x})}\\) larger. That is, when \\(w_d &gt; 0\\), increasing \\(x_d\\) increases our odds. "],["what-are-neural-networks.html", "Chapter 10 What are Neural Networks? 10.1 Neural Network as Universal Function Approximators 10.2 Neural Networks as Regression on Learned Feature Map 10.3 Everything is a Neural Network 10.4 Neural Network Optimization 10.5 Bias-Variance Trade-off for Neural Networks 10.6 Interpretation of Neural Networks 10.7 The Difficulty with Interpretable Machine Learning", " Chapter 10 What are Neural Networks? 10.1 Neural Network as Universal Function Approximators Neural networks are not exotic or that interesting – they are just a way to approximate a complex function by adding and composing a bunch of super simple non-linear functions. This composition of simple functions is represented graphically. Neural network training is done by gradient descent, where the gradient is computed by iteratively applying the chain rule. We can show that feedforward networks can approximate any continuous function on a bounded domain! This is why we like to use neural networks for a wide range of ML tasks where we don’t know the right functional form for the trend in the data. See More on the Universal Approximation Theorem for Feedforward Neural Networks. 10.2 Neural Networks as Regression on Learned Feature Map In fact, you’ve see neural network models in this class already – linear or logistic regression on top of a fixed feature map \\(\\phi\\) is a type of neural nework model! Conversely, neural nework regression or classification is just linear or logistic regression on top of a learned feature map \\(\\phi\\) (the hidden layers of the network learns the feature map and the output node, assuming a linear activation, combines the features linearly)! But actually, neural networks are pretty interesting! Because learning the feature map \\(\\phi\\) for a task (regression or classification) rather than fixing \\(\\phi\\) can show us which aspects of the data is helpful for this task! Extracting aspects of the data that are task-relevant is called feature learning or representation learning. The feature maps learnt by neural networks are called representations or embeddings. 10.3 Everything is a Neural Network Many of the ML models you’re hearing about in the news are neural network based models. However, note that outside of the tech industry, it is still not common practice to use neural network based models (because these models pose a number of new challenges for practitioners, see discussion later in this document). Generative AI is all the rage now, for a list of current popular generative models see ChatGPT is not all you need. A State of the Art Review of large Generative AI models. We will study generative models after Midterm #1. 10.3.1 Architecture Zoo One does not design or reason about neural networks one node or one weight at a time! Design of neural networks are done at the global level by playing around with different architectures – i.e. different graphical representations and different activations – and different objective functions. 10.3.2 ChatGPT “We’ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.” – OpenAI ChatGPT is based on a large language model, Generative Pre-trained Transformer (GPT), built from transformers – a type of neural network, recurrent neural network with a fancy architecture called “attention”– that takes an input and predicts likely words (or tokens) that should follow the input. Just so you know: the highly technical part of transformers involves the architecture (the different types of layers we stack together) and our intuitive as well as empirical understanding of what each part of the architecture does. I.e. it’s not math based. The language model, GPT, is then tuned to output appropriate responses during “chat”, by using Reinforcement Learning and human input. See Training language models to follow instructions with human feedback. So what can large language models like ChatGPT3 do? We are just beginning to explore the potential of these models: 1. Is Learning python for ML Obsolete? ChatGPT Can Write Code! 2. The End of Programming Is the Beginning of Something Cool: Programming for All 3. An Easy and Visual Way to Program Using Large Language Models 4. It’s Also the End of Essay Assignments 5. How to Hack Large Language Models (and Is It Ethical to Torture and Threaten AI?) 6. Now You Can Generate Hate Speech and Misinforamtion at Scale 7. Are We Making Ethical Choices When Making AI Ethical? 10.3.3 Stable Diffusion Apps like Lensa are built on the backs of the Stable Diffusion model. Stable Diffusion is a type of diffusion models, which are a type of neural network model that have bi-directional connections. The network is trained by adding noise to the input and then requiring the network to denoise it, with the goal of teaching the network to model the distribution of realistic data. Just so you know: The hard part of diffusion models is the math! We will see a simple instantiation of this type of math when we study latent variable models after Midterm #1. 10.4 Neural Network Optimization Training neural network models (or any modern ML models) means spending 80% of your time diagnosing issues with gradient descent and tuning the hyperparameters: learning rate, stopping condition. It’s a necessary skill to be able to read a plot of the loss function during training and extract insight about how to improve optimization. You need to learn to recognize common signs of gradient descent failure: (1) divergence (above graph) (2) slow/non convergence (below). You also need to know intuitively the “right” shape of a training loss plot. Neural network optimization is non-convex for any loss function that only depends on the output of the network. There is a simple proof of this (Hint: what happens when you flip the hidden layers of a neural network horizontally?), it’s worth memorizing! In fact, neural network optimization is as non-convex as optimization can get! For most networks, there are so many different settings of weights that parametrize the SAME function on a finite data set! Often times, careful architecture design can make NN optimization more “convex-like”. In practice, we find a set of different weights through training and choose amongst them the best model: 1. Random restart 2. Cyclic learning rates 3. More fancy techniques… 10.5 Bias-Variance Trade-off for Neural Networks 10.5.0.1 Overparametrization is Bad Complex models like NNs have low bias – they can model a wide range of functions, given enough samples. But complex models have high variance – they are very sensitive to small changes in the data distribution, leading to drastic performance decrease from train to test settings. Polynomial Model with Modest Degree Neural Network Model Just as in the case of linear and polynomial models, we can prevent nerual networks from overfitting (i.e. poor generalization due to high variance) by: 1. Regularization - \\(\\ell_1, \\ell_2\\) on weights - \\(\\ell_1, \\ell_2\\) on layer outputs - early-stopping 3. Ensembling - ensembling by random restart (and potentially bootstrapping) - dropout ensembling 10.5.0.2 Overparametrization Can be Good However, a new body of work like Implicit Regularization in Over-parameterized Neural Networks show that very wide neural networks (with far more parameters than there are data observations) actually ceases to overfit as the width surpasses a certain threshold. In fact, as the width of a neural network approaches infinity, training the neural network becomes kernel regression (this kernel is called the neural tangent kernel)! 10.5.0.3 Overparametrization is Complicated In Deep Double Descent: Where Bigger Models and More Data Hurt For more works on comparing the behaviors of NNs with different sizes, see An Empirical Analysis of the Advantages of Finite- v.s. Infinite-Width Bayesian Neural Networks and Wide Mean-Field Bayesian Neural Networks Ignore the Data. 10.6 Interpretation of Neural Networks In The Mythos of Model Interpretability, the authors survey a large number of methods for interpreting deep models. In fact, interpretable machine learning is a growing and extremely interesting subfield of ML. For example, see works like Towards A Rigorous Science of Interpretable Machine Learning. There’s even a textbook for interpretable ML now – Interpretable Machine Learning: A Guide for Making Black Box Models Explainable! 10.6.1 Example 1: Can Neural Network Models Make Use of Human Concepts? (with Anita Mahinpei, Justin Clark, Ike Lage, Finale Doshi-Velez) What if instead building complex non-linear models based on raw inputs, we instead build simple linear models based on human interpretable concepts? We use a neural network to predict concepts from inputs and then use a linear model to predict the outcome from the concepts. We interpret the relationship between the outcome and the concepts via the linear model. These models are called concept bottleneck models. In The Promises and Pitfalls of Black-box Concept Learning Models, we examine the advantages and drawbacks of these models. 10.6.2 Example 2: Can Neural Network Models Learn to Explore Hypothetical Scenarios? (with Michael Downs, Jonathan Chu, Wisoo Song, Yaniv Yacoby, Finale Doshi-Velez) Rather than explaining why the model made a decision, it’s often more helpful to explain how to change the data in order to change the model’s decision. This modified input is a counter-factual. In CRUDS: Counterfactual Recourse Using Disentangled Subspaces, we study how to automatically generate counter-factual explanations that can help users achieve a favorable outcome from a decision system. 10.6.3 Example 3: A Powerful Generalization of Feature Importance for Neural Network Models In An explainable deep-learning algorithm for the detection of acute intracranial haemorrhage from small datasets, the authors build a neural network model to detect acute intracranial haemorrhage (ICH) and classifies five ICH subtypes. Model classifications are explained by highlighting the pixels that contributed the most to the decision. The highligthed regions tends to overlapped with ‘bleeding points’ annotated by neuroradiologists on the images. 10.7 The Difficulty with Interpretable Machine Learning The sheer number of methods for interpreting neural network (and other ML) models is overwhelming and poses a significant challenge for model selection: which explanation method should I choose for my model and how do I know if the explanation is good? 10.7.1 Example 4: Not All Explanations are Created Equal (with Zixi Chen, Varshini Subhash, Marton Havasi, Finale Doshi-Velez) Not only is there an overwhelming number of ways to explain ML models, there is also an overwhelming number of ways to formalize whether or not an explanation is “good”. In What Makes a Good Explanation?: A Harmonized View of Properties of Explanations, we attempt to provide a much needed taxonomy of existing metrics to quantify the “goodness” of ML explanations. We use the taxonomy to differentiate between metrics and provide guidance for how to choose a metric that’s task-appropriate. 10.7.2 Example 5: Explanations Can Lie Just as not accounting for confounding, causal relationships and colinearity can make our interpretations of linear and logistic regression misleading, intrepretations of neural networks are prone to lie to us! In works like Interpretation of Neural Networks is Fragile, we see that saliency maps (i.e. visualizations of feature importance based on gradients) can be unreliable indicators of how the model is really making decisions. 10.7.3 Example 6: The Perils of Explanations in Socio-Technical Systems In How machine-learning recommendations influence clinician treatment selections: the example of antidepressant selection, the authors found that clinicians interacting with incorrect recommendations paired with simple explanations experienced significant reduction in treatment selection accuracy. Take-away: Incorrect ML recommendations may adversely impact clinician treatment selections and that explanations are insufficient for addressing overreliance on imperfect ML algorithms. "],["the-math-and-interpretation-of-neural-network-models.html", "Chapter 11 The Math and Interpretation of Neural Network Models 11.1 Neural Networks Regression 11.2 Interpreting Neural Networks 11.3 Neural Network Models and Generalization", " Chapter 11 The Math and Interpretation of Neural Network Models 11.1 Neural Networks Regression Model for Regression: \\(y = f_\\mathbf{w}(\\mathbf{X}) + \\epsilon\\), where \\(\\epsilon\\sim \\mathcal{N}(0, \\sigma^2)\\) and \\(f_\\mathbf{w}\\) is a neural network with parameters \\(\\mathbf{w}\\). Training Objective: find \\(\\mathbf{w}\\) to maximize the joint log-likelihood of our data. This is equivalent to minimizing the Mean Square Error, \\[ \\min_{\\mathbf{w}}\\, \\mathcal{L}(\\mathbf{w}) = \\frac{1}{N}\\sum^N_{n=1} \\left(y_n - f_\\mathbf{w}(x_n)\\right)^2 \\] Optimizing the Training Objective: For linear regression (when \\(f_\\mathbf{w}\\) is a linear function), we computed the gradient of the \\(\\mathcal{L}\\) with respective to the model parameters \\(\\mathbf{w}\\), set it equal to zero and solved for the optimal \\(\\mathbf{w}\\) analytically. For logistic regression, we computed the gradient and used (stochastic) gradient descent to “solve for where the gradient is zero”. Can we do the same when \\(f_\\mathbf{w}\\) is a neural network? 11.1.1 Why It’s Hard to Differentiate a Neural Network Computing the gradient for any parameter in a neural network requires us to use the chain rule: \\[\\begin{align} \\frac{\\partial}{\\partial t} g(h(t)) = g&#39;(h(t))h&#39;(t),\\quad&amp; \\text{or}\\quad\\frac{\\partial g}{\\partial t} = \\frac{\\partial g}{\\partial h} \\frac{\\partial h}{\\partial t} \\end{align}\\] This is because a neural network is just a big composition of functions. Let’s try to differentiate the following neural network, with activation function \\(h(t) = e^{-0.5 t^2}\\) seen as a composition of functions by hand. We will first give a name to each parameter or weight in the neural network, \\(w_{ij}^l\\), where the superscript \\(l\\) indicates the layer from which the weight originates and the subscript \\(ij\\) indicate that this is a weight on an edge going from the \\(i\\)-th node in the \\(l\\)-th layer to the \\(j\\)-th node in the \\(l+1\\)-layer. We will denote the output of the \\(i\\)-th node in the \\(l\\)-th layer as \\(a_i^l\\). We will denote the input to the \\(is\\)-th node in the \\(l\\)-th layer (a linear combination of the outputs of nodes in the previous layer) as \\(s_i^l\\). In the “Forwards” column, we show you how each quantity \\(a_i^l\\) and \\(s_i^l\\) is computed as a function of the quantities in the previous layer. To compute each partial derivative \\(\\frac{\\partial \\mathcal{L}}{w_{ij}^l}\\), we start with the definition of the loss function \\(\\mathcal{L}\\) in terms of \\(a_0^3\\), and then apply the chain-rule \\[ \\frac{\\partial \\mathcal{L}}{s_{0}^3} = \\frac{\\partial \\mathcal{L}}{a_{0}^3}\\frac{\\partial a_{0}^3}{s_{0}^3} \\] to get a partial derivative of the loss function with respect to a quantity further down the neural network graph. By repeating the application of the chain-rule to each layer of the neural network, we eventually get all partial derivatives \\(\\frac{\\partial \\mathcal{L}}{w_{ij}^l}\\). You see that differentiating even a tiny network is a very complex time-consuming process! 11.1.2 Differentiating Neural Networks: Backpropagation Luckily, the process of differentiating a neural network by iteratively applying the chain-rule can be algorithmatized! The backpropagation algorithm automatically computes the gradient of a neural network and consists of three phases: 0. (Initialize) intialize the network parameters \\(\\mathbf{W}\\) 1. Repeat: 1. (Forward Pass) compute all intermediate values \\(s_{ij}^l\\) and \\(a_{ij}^l\\) for the given covariates \\(\\mathbf{X}\\) 2. (Backward Pass) compute all the gradients \\(\\frac{\\partial \\mathcal{L}}{\\partial w^l_{ij}}\\) 3. (Update Parameters) update each parameter by \\(-\\eta \\frac{\\partial \\mathcal{L}}{\\partial w^l_{ij}}\\) We will see on Thursday that backpropagation is a special instance of reverse mode automatic differentiation – a method of algorithmically computing exact gradients for functions defined by combinations of simple functions, by drawing graphical models of the composition of functions and then taking gradients by going forwards-backwards. 11.2 Interpreting Neural Networks Linear models are easy to interpret. Once we’ve found the MLE of the model parameters, we can formulate scientific hypotheses about the relationship between the outcome \\(Y\\) and the covariates \\(\\mathbf{X}\\): \\[\\begin{align} \\widehat{\\text{income}} = 2 * \\text{education (yr)} + 3.1 * \\text{married} - 1.5 * \\text{gaps in work history} \\end{align}\\] What do the weights of a neural network tell you about the relationship between the covariates and the outcome? We might be tempted to conclude that neural networks are uninterpretable due to their complexity. But just because we can’t understand neural networks by inspecting the value of the individual weights, it does not mean that we can’t understand them. In The Mythos of Model Interpretability, the authors survey a large number of methods for interpreting deep models. 11.2.1 Example 1: Can Neural Network Models Make Use of Human Concepts? (with Anita Mahinpei, Justin Clark, Ike Lage, Finale Doshi-Velez) What if instead building complex non-linear models based on raw inputs, we instead build simple linear models based on human interpretable concepts? We use a neural network to predict concepts from inputs and then use a linear model to predict the outcome from the concepts. We interpret the relationship between the outcome and the concepts via the linear model. These models are called concept bottleneck models. In The Promises and Pitfalls of Black-box Concept Learning Models, we examine the advantages and drawbacks of these models. 11.2.2 Example 2: Can Neural Network Models Learn to Explore Hypothetical Scenarios? (with Michael Downs, Jonathan Chu, Wisoo Song, Yaniv Yacoby, Finale Doshi-Velez) Rather than explaining why the model made a decision, it’s often more helpful to explain how to change the data in order to change the model’s decision. This modified input is a counter-factual. In CRUDS: Counterfactual Recourse Using Disentangled Subspaces, we study how to automatically generate counter-factual explanations that can help users achieve a favorable outcome from a decision system. 11.2.3 Example 3: A Powerful Generalization of Feature Importance for Neural Network Models In An explainable deep-learning algorithm for the detection of acute intracranial haemorrhage from small datasets, the authors build a neural network model to detect acute intracranial haemorrhage (ICH) and classifies five ICH subtypes. Model classifications are explained by highlighting the pixels that contributed the most to the decision. The highligthed regions tends to overlapped with ‘bleeding points’ annotated by neuroradiologists on the images. 11.2.4 Example 4: The Perils of Explanations In How machine-learning recommendations influence clinician treatment selections: the example of antidepressant selection, the authors found that clinicians interacting with incorrect recommendations paired with simple explanations experienced significant reduction in treatment selection accuracy. Take-away: Incorrect ML recommendations may adversely impact clinician treatment selections and that explanations are insufficient for addressing overreliance on imperfect ML algorithms. 11.3 Neural Network Models and Generalization Complex models have low bias – they can model a wide range of functions, given enough samples. But complex models like neural networks can use their ‘extra’ capacity to explain non-meaningful features of the training data that are unlikely to appear in the test data (i.e. noise). These models have high variance – they are very sensitive to small changes in the data distribution, leading to drastic performance decrease from train to test settings. Polynomial Model with Modest Degree Neural Network Model Just as in the case of linear and polynomial models, we can prevent nerual networks from overfitting (i.e. poor generalization due to high variance) by regularization or by ensembling a large number of models. However, a new body of work like Deep Double Descent: Where Bigger Models and More Data Hurt show that very wide neural networks (with far more parameters than there are data observations) actually ceases to overfit as the width surpasses a certain threshold. In fact, as the width of a neural network approaches infinity, training the neural network becomes kernel regression (this kernel is called the neural tangent kernel)! "],["the-math-behind-bayesian-regression.html", "Chapter 12 The Math Behind Bayesian Regression 12.1 Bayesian Linear Regression 12.2 Bayesian Linear Regression over Arbitrary Bases", " Chapter 12 The Math Behind Bayesian Regression Let say we have a training data set \\(\\mathcal{D} = \\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_N, y_N)\\}\\) of \\(N\\) number of observations where \\(\\mathbf{x}_n\\in \\mathbb{R}^D\\). 12.1 Bayesian Linear Regression Our Likelihood: We will assume a probabilistic model for regression \\[ y\\vert \\mathbf{x}, \\mathbf{w} \\sim \\mathcal{N}(\\mathbf{w}^\\top \\mathbf{x}, \\sigma^2). \\] We will use \\(\\mathbf{y}\\) to denote the vector of the \\(N\\) number of \\(y\\)’s in the training set, and \\(\\mathbf{X}\\) denote the \\(N\\times D\\) dimensional matrix whose \\(n\\)-th row is \\(\\mathbf{x}_n\\). Using matrix notation, we can simplify our expression of the joint likelihood: \\[\\begin{aligned} \\prod_{n=1}^Np(y_n | \\mathbf{x}_n, \\mathbf{w}) &amp;= \\prod_{n=1}^N\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\mathrm{exp}\\left\\{ -\\frac{1}{2\\sigma^2}(y_n - (\\mathbf{w}^\\top \\mathbf{x}_n)^2)\\right\\}\\\\ &amp;= \\frac{1}{\\sqrt{2\\pi\\sigma^2}^N} \\mathrm{exp}\\left\\{ -\\frac{1}{2\\sigma^2}\\sum_{n=1}^N(y_n - (\\mathbf{w}^\\top \\mathbf{x}_n)^2)\\right\\}\\\\ &amp;= \\frac{1}{\\sqrt{2\\pi\\sigma^2}^N} \\mathrm{exp}\\left\\{ -\\frac{1}{2\\sigma^2}(\\mathbf{y} - \\mathbf{X}\\mathbf{w})^\\top (\\mathbf{y} - \\mathbf{X}\\mathbf{w})\\right\\}\\\\ &amp;= \\frac{1}{\\sqrt{2\\pi\\sigma^2}^N} \\mathrm{exp}\\left\\{ -\\frac{1}{2}(\\mathbf{y} - \\mathbf{X}\\mathbf{w})^\\top(\\sigma^2I_{N\\times N})^{-1} (\\mathbf{y} - \\mathbf{X}\\mathbf{w})\\right\\}\\\\ &amp;= \\mathcal{N}(\\mathbf{y}; \\mathbf{X}\\mathbf{w}, \\sigma^2I_{2\\times 2}) \\end{aligned}\\] where \\(I_{N\\times N}\\) is the \\(N\\times N\\) identity matrix. The expression in the second line is derived from the first by applying the rule \\(e^{a}e^{b} = e^{a+b}\\); the expression in the third line is derived from the second by expanding out \\(\\mathbf{X}\\mathbf{w}\\) as a sum of inner products \\(\\mathbf{w}^\\top \\mathbf{x}_n\\); the fourth line is derived from the third by noting that inserting the inverse of an identity matrix (which is equal to the identity matrix) does not change the computation. We see that instead of treating the joint likelihood as a product of univariate Gaussian pdfs, we can rewrite it as a multivariate Gaussian with mean vector \\(\\mathbf{X}w \\in \\mathbb{R}^N\\) and a \\(N\\times N\\) diagonal covariance matrix (the entries on the diagonal of this matrix is \\(\\sigma^2\\)). Let’s denote \\(\\sigma^2I_{N\\times N}\\) by \\(\\Sigma_0\\), so that the likelihood is: \\[ p(\\mathbf{y} | \\mathbf{X}, \\mathbf{w}) = \\frac{1}{\\sqrt{2\\pi|\\Sigma_0|}^N} \\mathrm{exp}\\left\\{-\\frac{1}{2} (\\mathbf{X}\\mathbf{w} - \\mathbf{y})^\\top\\Sigma_0^{-1}(\\mathbf{X}\\mathbf{w} - \\mathbf{y})\\right\\} \\] Our Prior: Let’s add a prior to our model \\[ \\mathbf{w} \\sim \\mathcal{N}(\\mathbf{m}, \\mathbf{S}) \\] where \\(\\mathbf{m} \\in \\mathbb{R}^D\\) is the mean vector and \\(\\mathbf{S}\\in \\mathbb{R}^{D\\times D}\\) is the covariance matrix. Typically, we assume that \\(\\mathbf{m}\\) is the zero vector and \\(\\mathbf{S}\\) is of the form \\(\\sigma_\\mathbf{w}^2 I_{D\\times D}\\). Our Posterior: The posterior distribution is now given by the Bayes Rule \\[ p(\\mathbf{w} | \\mathbf{y}, \\mathbf{X}) = \\frac{p(\\mathbf{y} | \\mathbf{X}, \\mathbf{w})p(\\mathbf{w})}{p(\\mathbf{y} | \\mathbf{X})} \\] In this case, we can write out the analytical form of the posterior pdf using matrix algebra: \\[\\begin{aligned} p(\\mathbf{w} | \\mathbf{y}, \\mathbf{X}) &amp;= \\frac{p(\\mathbf{y} | \\mathbf{X}, \\mathbf{w})p(\\mathbf{w})}{p(\\mathbf{y} | \\mathbf{X})}\\\\ &amp;= \\frac{\\frac{1}{\\sqrt{2\\pi|\\Sigma_0|}^N} \\mathrm{exp}\\left\\{-\\frac{1}{2} (\\mathbf{X}\\mathbf{w} - \\mathbf{y})^\\top\\Sigma_0^{-1}(\\mathbf{X}\\mathbf{w} - \\mathbf{y})\\right\\} \\frac{1}{\\sqrt{2\\pi|\\mathbf{S}|}^2} \\mathrm{exp}\\left\\{-\\frac{1}{2} (\\mathbf{w} - \\mathbf{m})^\\top \\mathbf{S}^{-1}(\\mathbf{w} - \\mathbf{m})\\right\\}}{p(\\mathbf{y} | \\mathbf{X})}\\\\ &amp;= \\frac{const * \\mathrm{exp}\\left\\{-\\frac{1}{2} (\\mathbf{X}\\mathbf{w} - \\mathbf{y})^\\top\\Sigma_0^{-1}(\\mathbf{X}\\mathbf{w} - \\mathbf{y}) + -\\frac{1}{2} (\\mathbf{w} - \\mathbf{m})^\\top S^{-1}(\\mathbf{w} - \\mathbf{m})\\right\\}}{const}\\\\ &amp;=const * \\mathrm{exp}\\left\\{-\\frac{1}{2}\\left(\\mathbf{y}^\\top\\Sigma_0^{-1}\\mathbf{y} - 2\\mathbf{y}^\\top \\Sigma_0^{-1}\\mathbf{X}\\mathbf{w} + \\mathbf{w}^\\top\\mathbf{X}^\\top\\Sigma_0^{-1}\\mathbf{X}\\mathbf{w} + \\mathbf{w}^\\top \\mathbf{S}^{-1}\\mathbf{w} - 2\\mathbf{m}^\\top \\mathbf{S}^{-1} \\mathbf{w} + \\mathbf{m}^\\top \\mathbf{S}^{-1}\\mathbf{m} \\right) \\right\\}\\\\ &amp;= const * \\mathrm{exp}\\left\\{ \\mathbf{y}^\\top\\Sigma^{-1}\\mathbf{y} + \\mathbf{m}^\\top \\mathbf{S}^{-1}\\mathbf{m} \\right\\}\\mathrm{exp}\\left\\{ \\mathbf{w}^\\top \\left(\\mathbf{S}^{-1} + \\mathbf{X}^\\top\\Sigma^{-1}\\mathbf{X}\\right) \\mathbf{w} - 2\\left(\\mathbf{y}^\\top \\Sigma^{-1}\\mathbf{X} + \\mathbf{m}^\\top \\mathbf{S}^{-1} \\right) \\mathbf{w}\\right\\}\\\\ &amp;= const * \\mathrm{exp}\\left\\{ \\mathbf{w}^\\top \\left(\\mathbf{S}^{-1} + \\mathbf{X}^\\top\\Sigma^{-1}\\mathbf{X}\\right) \\mathbf{w} - 2\\left(\\mathbf{y}^\\top \\Sigma^{-1}\\mathbf{X} + \\mathbf{m}^\\top S^{-1} \\right) \\mathbf{w}\\right\\}\\\\ &amp;= const * \\mathrm{exp}\\left\\{ \\mathbf{w}^\\top A \\mathbf{w} - 2b \\mathbf{w}\\right\\} \\end{aligned}\\] where \\(A = \\left(\\mathbf{S}^{-1} + \\mathbf{X}^\\top\\Sigma^{-1}\\mathbf{X}\\right)\\) and \\(b = \\left(\\mathbf{y}^\\top \\Sigma^{-1}\\mathbf{X} + \\mathbf{m}^\\top \\mathbf{S}^{-1} \\right)\\). In the last line of the above, we see that the inside of the exponential function is almost a matrix square, to complete the square (and factor the expression) we need to add and subtract the term \\(bA b^\\top\\): \\[\\begin{aligned} p(\\mathbf{w} | \\mathbf{y}, \\mathbf{X}) &amp;= const * \\mathrm{exp}\\left\\{ \\mathbf{w}^\\top A \\mathbf{w} - 2b \\mathbf{w}\\right\\}\\\\ &amp;= const * \\mathrm{exp}\\left\\{ \\mathbf{w}^\\top A \\mathbf{w} - 2b \\mathbf{w} + bA^{-1} b^\\top- bA^{-1} b^\\top\\right\\}\\\\ &amp;= const * \\mathrm{exp}\\left\\{ (\\mathbf{w} - A^{-1}b^\\top)^\\top A(\\mathbf{w} - A^{-1}b^\\top) - bA^{-1} b^\\top\\right\\}\\\\ &amp;= const * \\mathrm{exp}\\left\\{ - b^\\top A b^\\top\\right\\}\\mathrm{exp}\\left\\{ (\\mathbf{w} - A^{-1}b^\\top)^\\top A(\\mathbf{w} - A^{-1}b^\\top)\\right\\}\\\\ &amp;= const * \\mathrm{exp}\\left\\{ (\\mathbf{w} - A^{-1}b^\\top)^\\top A(\\mathbf{w} - A^{-1}b^\\top)\\right\\}. \\end{aligned}\\] Finally, we see that the posterior \\(p(\\mathbf{w} | \\mathbf{y}, \\mathbf{X})\\) has the form of a multivariate Gaussian, with mean \\(A^{-1}b\\) and covariance \\(A^{-1}\\). That is, \\[ p(\\mathbf{w} | \\mathbf{y}, \\mathbf{X}) = \\mathcal{N}\\left(\\mu, \\Sigma\\right), \\] where \\(\\Sigma = \\left(\\mathbf{S}^{-1} + \\mathbf{X}^\\top\\Sigma_0^{-1}\\mathbf{X}\\right)^{-1}\\) and \\(\\mu=\\Sigma \\left(\\mathbf{y}^\\top \\Sigma_0^{-1}\\mathbf{X} + \\mathbf{m}^\\top \\mathbf{S}^{-1} \\right)^\\top\\) 12.2 Bayesian Linear Regression over Arbitrary Bases The math of Bayesian linear regression applies to Bayesian linear regression for any feature map \\(\\phi:\\mathbb{R}^D \\to \\mathbb{R}^{D&#39;}\\) applied to the training inputs. Let \\(\\mathbf{\\Phi}\\) denote the feature map \\(\\phi\\) applied to the matrix \\(\\mathbf{X}\\). Then we can set up a Bayesian linear model over \\(\\phi(\\mathbf{x})\\): \\[\\begin{aligned} \\mathbf{y}\\vert \\mathbf{w}, \\mathbf{X} &amp;\\sim \\mathcal{N}(\\mathbf{\\Phi}\\mathbf{w}, \\Sigma_0)&amp;\\text{(Likelihood)}\\\\ \\mathbf{w} &amp;\\sim \\mathcal{N}(\\mathbf{m}, \\mathbf{S})&amp; \\text{(Prior)} \\end{aligned}\\] and the posterior is exactly that of Bayesian linear regression with \\(\\mathbf{\\Phi}\\) in place of \\(\\mathbf{X}\\): \\[ p(\\mathbf{w} | \\mathbf{y}, \\mathbf{X}) = \\mathcal{N}\\left(\\mu, \\Sigma\\right), \\] where \\(\\Sigma = \\left(\\mathbf{S}^{-1} + \\mathbf{\\Phi}^\\top\\Sigma_0^{-1}\\mathbf{\\Phi}\\right)^{-1}\\) and \\(\\mu=\\Sigma \\left(\\mathbf{y}^\\top \\Sigma_0^{-1}\\mathbf{\\Phi} + \\mathbf{m}^\\top \\mathbf{S}^{-1} \\right)^\\top\\). "],["bayesian-modeling-framework.html", "Chapter 13 Bayesian Modeling Framework 13.1 Components of Machine Learning Reasoning 13.2 Bayesian Modeling Paradigm", " Chapter 13 Bayesian Modeling Framework 13.1 Components of Machine Learning Reasoning There are three important components of machine learning reasoning, regardless of whether you are a theoretician, practioner or student: Low level reasoning: you need to be technically fluent in a specific set of mathematical notation, manipulations, coding paradigms and frameworks to reason about the low level details of models. Dervivation: are the math formulae correctly derived? Implementation: is your code logical and bug free? Computation: is your computation numerically stable, efficient and correct (e.g. did you optimize your model)? Examples: you need to be able to verify the correctness of the gradient formula for logistic regression instead of just copying it from somewhere. you need to be able to do backprop or reverse mode autodiff by hand (and implement it from scratch), so you debug issue that arise in pytorch autodifferentiation of computational graphs (they will arise!!). you need to be able to verify the correctness of the formula for the posterior distribution in Bayesian linear regression (with Gaussian noise and Gaussian likelihood) instead of copying it from somewhere. Why you care: You need low level reasoning to make models that work correctly and to fix your models when they break! High level reasoning: you need to be able to chain a complex sequence of concepts and facts together to deductively reason about the emergent properties of models. Inductive bias of models: what types of properties are inherent or more likely in your learnt models given your model assumptions? Model comparison: in what ways do the inductive biases of models differ? Model evaluation: in what ways can we measure the inductive biases of models (e.g. what do our metrics really measure)? Examples: you need to be able to reason about how your design choices affect emergent properties of your model (bias, variance, uncertainty, overfitting, underfitting, interpretability). you need to be able to reasons about the relative strengths and weweaknesses of model in comparison. you need to be able to choose the right metric to measure the right aspects of model inductive bias Why you care: You need high level reasoning to design new models and discover understand model behaviors. Task level reasoning: you need to be able to reason about model properties in the context of a task or an end goal. Task performance: what does the inductive bias of your model imply about its task performance? Task-based evaluation: do our metrics capture important aspects of task performance? Why you care: You need task level reasoning to make tech that people care about and won’t get you sued. It’s hard to practice all three – reflect frequently on what you feel like you need to work on and we are here to help! 13.2 Bayesian Modeling Paradigm Modeling noise: We start with modeling both the data trend and the observation noise: \\[y = f_\\mathbf{w}(\\mathbf{x}) + \\epsilon,\\; \\epsilon \\sim p(\\epsilon),\\] giving us a likelihood \\(p(y | \\mathbf{x}, \\mathbf{w})\\). Example: Let’s pick a Gaussian likelihood (i.e. additive Gaussian noise) for linear regression: \\[ y | \\mathbf{x}, \\mathbf{w} \\sim \\mathcal{N}(\\mathbf{w}^\\top \\mathbf{x}, 0.5)\\] where \\(\\mathbf{w} = [w_0\\;\\; w_1]^\\top\\) is the vector consisting of the slope \\(w_1\\) and y-intercept \\(w_0\\). Modeling belief: We then encode our beliefs about \\(\\mathbf{w}\\) as well as our uncertainty in a distribution, the prior over \\(\\mathbf{w}\\): \\(p(\\mathbf{w})\\). Example: Let’s pick Gaussian piors for \\(w_0, w_1\\). \\[w_0 \\sim \\mathcal{N}(0, 0.5),\\;\\;\\; w_1 \\sim \\mathcal{N}(0, 1)\\] Since the \\(p(w_0), p(w_1)\\) are both univariate Gaussians and we’ve assumed in our prior that they are independent, the joint distribution over \\(w_0\\) and \\(w_1\\) is a bivariate Gaussian with mean \\(m = [0\\;\\; 0]^\\top\\) and covariance matrix \\(S = \\left[\\begin{array}{cc} 0.5 &amp; 0\\\\ 0 &amp; 1 \\end{array} \\right]\\): \\[ w\\sim \\mathcal{N}\\left([0\\;\\; 0]^\\top, \\left[\\begin{array}{cc} 0.5 &amp; 0\\\\ 0 &amp; 1 \\end{array} \\right] \\right). \\] Question: What beliefs do our priors translate into? Update belief using data: We use Baye’s rule to derive the posterior over \\(\\mathbf{w}\\), \\(p(\\mathbf{w}| y, \\mathbf{x})\\), which gives us the likelihood of a model \\(\\mathbf{w}\\) given the observed data: \\[ p(\\mathbf{w}| y, \\mathbf{x}) \\propto p(y| \\mathbf{w}, \\mathbf{x}) p(\\mathbf{w}) \\] Example: Given a training set, we can show that the posterior \\(p(\\mathbf{w} | \\mathbf{y}, \\mathbf{X})\\) is a bivariate Gaussian with mean \\(\\mu\\) and covariance \\(\\Sigma\\) defined as below: \\[\\begin{aligned} \\mu &amp;= \\left(S^{-1} + 2\\mathbf{X}^\\top\\mathbf{X}\\right)^{-1} \\left(2\\mathbf{X}^\\top\\mathbf{y} \\right)\\\\ \\Sigma &amp;= \\left(S^{-1} + 2\\mathbf{X}^\\top\\mathbf{X}\\right)^{-1}\\\\ \\end{aligned}\\] where \\(\\mathbf{y}\\) is the vector of target values, \\(\\mathbf{X}\\) is the augmented input matrix for the training data set and \\(S = \\left[\\begin{array}{cc} 0.5 &amp; 0\\\\ 0 &amp; 1 \\end{array} \\right]\\). Remember that the MLE solution is \\(\\mathbf{w}^\\mathrm{MLE} = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{y}\\). Question: how does our prior (mean or variance) affect our posterior (mean or variance)? How does the number of data observations affect our posterior (mean or variance)? How does the observation noise level affect our posterior (mean or variance)? Hint: Note that \\(S^{-1} = \\left[\\begin{array}{cc} \\frac{1}{0.5} &amp; 0\\\\ 0 &amp; \\frac{1}{1} \\end{array} \\right]\\). Make Predictions Under Uncertain Belief We can use models in our posterior to make predictions and quantify uncertainty: Posterior Predictive Sampling: 1. Sample \\(\\mathbf{w}^{(s)}\\) from posterior \\(p(\\mathbf{w})\\) 2. Sample prediction \\(y\\) from likelihood \\(p(y | \\mathbf{w}^{(s)}, \\mathbf{x})\\). That is, we sample a noise \\(\\epsilon^{(s)}\\) from \\(p(\\epsilon)\\) and generate: \\[ \\hat{y} = f_{\\mathbf{w}^{(s)}}(\\mathbf{x}) + \\epsilon^{(s)} \\] We can use our posterior to reason about new data – e.g. how likely is this new data given our posterior belief? We can compute the likelihood of a new data point give our training data with the following integral: \\[ p(y^* |\\mathbf{x}^*, \\mathcal{D}) = \\int_\\mathbf{w} p(y^*, \\mathbf{w}|\\mathbf{x}^*, \\mathcal{D}) d\\mathbf{w} = \\int_\\mathbf{w} p(y^*|\\mathbf{w}, \\mathbf{x}^*)p(\\mathbf{w} | \\mathcal{D}) d\\mathbf{w} \\] where \\((\\mathbf{x}^*, y^*)\\) represents new data. In the above \\(p(y^* |\\mathbf{x}^*, \\mathcal{D})\\) is called the posterior predictive. Question: how does our prior (mean or variance) affect our posterior predictive (mean or variance)? How does the number of data observations affect our posterior predictive (mean or variance)? How does the observation noise level affect our posterior predictive (mean or variance)? "],["bayesain-vs-frequentist-inference.html", "Chapter 14 Bayesain vs Frequentist Inference? 14.1 The Bayesian Modeling Process 14.2 Bayesian vs Frequentist Inference", " Chapter 14 Bayesain vs Frequentist Inference? 14.1 The Bayesian Modeling Process The key insight about Bayesian modeling is that we treat all unknown quantities as random variables. (Everything Is an RV) (Likelihood) \\(p(y | \\mathbf{x}, \\mathbf{w})\\) Choose whatever distribution you want!!! (Prior) \\(p(\\mathbf{w})\\) Choose whatever distribution you want!!! (Make the Joint Distribution) we form the joint distribution over all RVs \\[p(y, \\mathbf{w} | \\mathbf{x}) = p(y | \\mathbf{x}, \\mathbf{w}) p(\\mathbf{w})\\] (Make Inferences About the Unknown Variable) we can condition on the observed RVs to make inferences about unknown RVs, \\[ p(\\mathbf{w}| y, \\mathbf{x}) = \\frac{p(y, \\mathbf{w}|\\mathbf{x})}{p(y|\\mathbf{x})} \\] where \\(p(\\mathbf{w}| y, \\mathbf{x})\\) is called the posterior distribution. (Make Predictions For New Data: Posterior Predictive Sampling) Sample \\(\\mathbf{w}^{(s)}\\) from posterior \\(p(\\mathbf{w})\\) Sample prediction \\(y\\) from likelihood \\(p(y | \\mathbf{w}^{(s)}, \\mathbf{x})\\). That is, we sample a noise \\(\\epsilon^{(s)}\\) from \\(p(\\epsilon)\\) and generate: \\[ \\hat{y} = f_{\\mathbf{w}^{(s)}}(\\mathbf{x}) + \\epsilon^{(s)} \\] (Evaluating Bayesian Models) computing the posterior predictive likelihood of the data is the main way to evaluate how well our model fits the data: \\[ \\sum_{m=1}^M \\log p(y_m |\\mathbf{x}_m, \\mathcal{D}) = \\sum_{m=1}^M \\log \\int_\\mathbf{w} p(y_m|\\mathbf{w}, \\mathbf{x}_m)p(\\mathbf{w} | \\mathcal{D}) d\\mathbf{w} \\] where \\(\\{ (\\mathbf{x}_m, y_m) \\}\\) is the test data set. This quantity is called the test log-likelihood. 14.2 Bayesian vs Frequentist Inference Given some data, should we build a Bayesian model or a non-Bayesian probabilistic model? Again, we have to understand the differences and trade-offs. Point Estimates or Distributions? To compare MLE and a Bayesian posterior, we can summarize the posterior using point estimates: The Posterior Mean Estimate: the “average” estimate of \\(\\mathbf{w}\\) under the posterior distribution: \\[ \\mathbf{w}_{\\text{post mean}} = \\mathbb{E}_{\\mathbf{w}\\sim p(\\mathbf{w}|\\mathcal{D})}\\left[ \\mathbf{w}|\\mathcal{D} \\right] = \\int_\\mathbf{w} \\mathbf{w} \\;p(\\mathbf{w}|\\mathcal{D}) d\\mathbf{w} \\] The Posterior Mode or Maximum a Posterior (MAP) Estimate: the most likely estimate of \\(\\mathbf{w}\\) under the posterior distribution: \\[ \\mathbf{w}_{\\text{MAP}} = \\mathrm{argmax}_{\\mathbf{w}} p(\\mathbf{w}|\\mathcal{D}) \\] Question: Is it a good idea to reduce a posterior distribution to a point estimate? Comparing Posterior Point Estimates and MLE Point estimates of Bayesian posteriors can often be interpreted as some kind of regularized MLE! For example, with a Gaussian prior (with diagonal covariance matrix), the MAP is \\(\\ell_2\\)-regularized MLE: \\[ \\textrm{argmax}_\\mathbf{w} p(\\mathbf{w}|\\mathcal{D}) = \\underbrace{\\sum_{n=1}^N \\log p(y_n | \\mathbf{x}_n, \\mathbf{w})}_{\\text{joint log-likelihood}} - \\lambda\\underbrace{\\| \\mathbf{w}\\|_2^2}_{\\text{regularization}} \\] Comparing Posteriors and MLE The Berstein-von Mises Theorem tells us that: The posterior point estimates (like MAP) approach the MLE, with large samples sizes. It may be valid to approximate an unknown posterior with a Gaussian, with large samples sizes. This will become a very important idea for Bayesian Logistic Regression! What’s Harder, Computing MLE or Bayesian Inference? Relatively speaking, optimization problems are much easier to solve – we have autograd for differentiation, gradient descent to solve for stationary points. Bayesian modeling is, in comparison, orders of magnitude more mathematically and computationally challenging: (Inference) sampling (from priors, posteriors, prior predictives and posterior predictives) is hard, especially for non-conjugate likelihood, prior pairs! (Evaluation) computing integrals (for the log-likelihood of train and test data under the posterior) is intractable, when likelihoods and priors are not conjugate! Why Should You Be Bayesian? See in-class exercise for today, comparing Bayesian and Frequentist uncertainties. "],["the-math-of-posterior-inference.html", "Chapter 15 The Math of Posterior Inference 15.1 The Bayesian Modeling Process 15.2 Point Estimates from the Posterior 15.3 Bayesian Logistic Regression", " Chapter 15 The Math of Posterior Inference 15.1 The Bayesian Modeling Process The key insight about Bayesian modeling is that we treat all unknown quantities as random variables. Thus, in order to make statements about the data, \\((\\mathbf{x}, y)\\), and the model parameters \\(\\mathbf{w}\\) (as well as potentially parameters of the likelihood \\(\\theta\\)), we form the joint distribution over all variables and use the various marginal and conditional distributions to reason about the data \\((\\mathbf{x}, y)\\) and \\(\\mathbf{w}\\). That is, the steps of Bayesian modeling are as follows: (Everything Is an RV) We need to define how the data, as a RV, depends on \\(\\mathbf{w}\\) and what kind of RV is \\(\\mathbf{w}\\): (Likelihood) \\(p(y | \\mathbf{x}, \\mathbf{w})\\) Choose whatever distribution you want!!! (Prior) \\(p(\\mathbf{w})\\) Choose whatever distribution you want!!! (Make the Joint Distribution) we form the joint distribution over all RVs – this usually involves multiplying all the pdf’s together \\[p(y, \\mathbf{w} | \\mathbf{x}) = p(y | \\mathbf{x}, \\mathbf{w}) p(\\mathbf{w})\\] (Make Inferences About the Unknown Variable) we can condition on the observed RVs to make inferences about unknown RVs, \\[ p(\\mathbf{w}| y, \\mathbf{x}) = \\frac{p(y, \\mathbf{w}|\\mathbf{x})}{p(y|\\mathbf{x})} \\] where \\(p(\\mathbf{w}| y, \\mathbf{x})\\) is called the posterior distribution and \\(p(y|\\mathbf{x})\\) is called the evidence. (Make Inferences About New Data, Under Our Prior) before any data is observed, we can use our prior and likelihood to reason about unobserved data: \\[ p(y^* |\\mathbf{x}^*) = \\int_\\mathbf{w} p(y^*, \\mathbf{w} |\\mathbf{x}^*) d\\mathbf{w} = \\int_\\mathbf{w} p(y^* | \\mathbf{w}, \\mathbf{x}^*) p(\\mathbf{w}) d\\mathbf{w} \\] where \\((\\mathbf{x}^*, y^*)\\) represents new data. In the above \\(p(y^* |\\mathbf{x}^*)\\) is called the prior predictive, and tells us how likely any data point is under our prior belief. This is a measure of the appropriateness of the inductive bias (or prior belief) of our model. (Make Inferences About New Data, Under Our Posterior) after observing data \\(\\mathcal{D} = \\{(\\mathbf{x}, y)\\}\\), we can use our posterior to reason about unobserved data: \\[ p(y^* |\\mathbf{x}^*, \\mathcal{D}) = \\int_\\mathbf{w} p(y^*, \\mathbf{w}|\\mathbf{x}^*, \\mathcal{D}) d\\mathbf{w} = \\int_\\mathbf{w} p(y^*|\\mathbf{w}, \\mathbf{x}^*)p(\\mathbf{w} | \\mathcal{D}) d\\mathbf{w} \\] where \\((\\mathbf{x}^*, y^*)\\) represents new data. In the above \\(p(y^* |\\mathbf{x}^*, \\mathcal{D})\\) is called the posterior predictive, and tells us how likely any data point is under our posterior belief. This is a measure of the goodness of the learnt model (or posterior belief). (Evaluating Bayesian Models) computing the posterior predictive likelihood of the data is the main way to evaluate how well our model fits the data: \\[ \\sum_{m=1}^M \\log p(y_m |\\mathbf{x}_m, \\mathcal{D}) = \\sum_{m=1}^M \\log \\int_\\mathbf{w} p(y_m|\\mathbf{w}, \\mathbf{x}_m)p(\\mathbf{w} | \\mathcal{D}) d\\mathbf{w} \\] where \\(\\{ (\\mathbf{x}_m, y_m) \\}\\) is the test data set. This quantity is called the test log-likelihood. 15.2 Point Estimates from the Posterior If you absolutely wanted to derive a point estimate for the parameters \\(\\theta\\) in the likelihood from your Bayesian model, there are two common ways to do it: The Posterior Mean Estimate: the “average” estimate of \\(\\mathbf{w}\\) under the posterior distribution: \\[ \\mathbf{w}_{\\text{post mean}} = \\mathbb{E}_{\\mathbf{w}\\sim p(\\mathbf{w}|\\mathcal{D})}\\left[ \\mathbf{w}|\\mathcal{D} \\right] = \\int_\\mathbf{w} \\mathbf{w} \\;p(\\mathbf{w}|\\mathcal{D}) d\\mathbf{w} \\] The Posterior Mode or Maximum a Posterior (MAP) Estimate: the most likely estimate of \\(\\mathbf{w}\\) under the posterior distribution: \\[ \\mathbf{w}_{\\text{MAP}} = \\mathrm{argmax}_{\\mathbf{w}} p(\\mathbf{w}|\\mathcal{D}) \\] Question: Which point estimate of the posterior do you think is more common to compute in machine learning (Hint: which point estimate would you rather compute, if you were forced to compute one)? Question: is it better to summarize the entire posterior using a point estimate? I.e. why should we keep the posterior distribution around? Answer: point estimates can be extremely misleading! The posterior mode can be an atypical point: The posterior mean can be an unlikely point: 15.2.1 Comparison of Posterior Point Estimates and MLE It turns out that point estimates of Bayesian posteriors can often be interpreted as some kind of regularized MLE! For example, for Bayesian linear regression with Gaussian prior, the posterior mode estimate is \\(\\ell_2\\)-regularized MLE! Let’s recall the loss function for \\(\\ell_2\\)-regularized MLE: \\[ \\ell(\\mathbf{w}) = - \\left(\\underbrace{\\sum_{n=1}^N \\log p(y_n | \\mathbf{x}_n, \\mathbf{w})}_{\\text{joint log-likelihood}} - \\lambda\\underbrace{\\| \\mathbf{w}\\|_2^2}_{\\text{regularization}}\\right) \\] Let’s also recall the posterior pdf of Bayesian regression with Gaussian prior: \\[ p(\\mathbf{w}|y, \\mathbf{x}) \\propto p(y |\\mathbf{w}, \\mathbf{x}) p(\\mathbf{w}) \\] Remember that the posterior mode is defined as: \\[ \\textrm{argmax}_\\mathbf{w} p(\\mathbf{w}|\\mathcal{D}) = \\textrm{argmax}_\\mathbf{w} \\prod_{n=1}^N p(y_n |\\mathbf{w}, \\mathbf{x}_n) p(\\mathbf{w}) \\] In the above equation, we are using that fact that multiplicative constants do not affect the position of global optima. Remember also that in ML we always prefer to work with the log-likelihood because the log simplifies many pdf expressions. Luckily applying logs to our probabilistic loss function also does not affect the position of the global optima. That is, we have that: \\[ \\textrm{argmax}_\\mathbf{w} \\log p(\\mathbf{w}|\\mathcal{D}) = \\textrm{argmax}_\\mathbf{w} \\underbrace{\\sum_{n=1}^N \\log p(y_n | \\mathbf{x}_n, \\mathbf{w})}_{\\text{joint log-likelihood}} + \\log p(\\mathbf{w}), \\] Now if the prior is Gaussian, let’s say: \\[ \\mathbf{w}\\sim \\mathcal{N}(\\mathbf{m}, \\mathbf{S}) \\] where \\(\\mathbf{w}\\in \\mathbb{R}^D\\), then the log prior will simplify nicely: \\[\\begin{aligned} \\textrm{argmax}_\\mathbf{w} \\log p(\\mathbf{w}|\\mathcal{D}) =&amp; \\textrm{argmax}_\\mathbf{w} \\underbrace{\\sum_{n=1}^N \\log p(y_n | \\mathbf{x}_n, \\mathbf{w})}_{\\text{joint log-likelihood}} + \\log p(\\mathbf{w})\\\\ =&amp;\\textrm{argmax}_\\mathbf{w} \\underbrace{\\sum_{n=1}^N \\log p(y_n | \\mathbf{x}_n, \\mathbf{w})}_{\\text{joint log-likelihood}} \\\\ &amp;+ \\log \\frac{1}{\\sqrt{(2\\pi)^D \\mathbf{det}(\\Sigma)}} + \\log\\mathrm{exp}\\left\\{ - \\frac{1}{2}(\\mathbf{w} - \\mathbf{m})^\\top \\mathbf{S}^{-1}(\\mathbf{w} - \\mathbf{m})\\right\\}\\\\ =&amp;\\textrm{argmax}_\\mathbf{w} \\underbrace{\\sum_{n=1}^N \\log p(y_n | \\mathbf{x}_n, \\mathbf{w})}_{\\text{joint log-likelihood}} + \\log C - \\frac{1}{2}(\\mathbf{w} - \\mathbf{m})^\\top \\mathbf{S}^{-1}(\\mathbf{w} - \\mathbf{m}) \\end{aligned}\\] Now if we further assume that the mean \\(\\mathbf{m}\\) is zero and the covariance matrix \\(\\mathbf{S}\\) is diagonal, i.e. \\(\\mathbf{S} = \\left[\\begin{array}{cc} s &amp; 0\\\\ 0 &amp; s \\end{array} \\right]\\), the the log prior will look even simpler: \\[\\begin{aligned} \\textrm{argmax}_\\mathbf{w} \\log p(\\mathbf{w}|\\mathcal{D}) &amp;= \\textrm{argmax}_\\mathbf{w} \\underbrace{\\sum_{n=1}^N \\log p(y_n | \\mathbf{x}_n, \\mathbf{w})}_{\\text{joint log-likelihood}} + \\log C - \\frac{1}{2}(\\mathbf{w} - \\mathbf{m})^\\top \\mathbf{S}^{-1}(\\mathbf{w} - \\mathbf{m})\\\\ &amp;=\\textrm{argmax}_\\mathbf{w} \\underbrace{\\sum_{n=1}^N \\log p(y_n | \\mathbf{x}_n, \\mathbf{w})}_{\\text{joint log-likelihood}} + \\log C - \\frac{1}{2}(\\mathbf{w})^\\top\\left(\\frac{1}{s} I_{2\\times 2}\\right)(\\mathbf{w})\\\\ &amp;= \\textrm{argmax}_\\mathbf{w} \\underbrace{\\sum_{n=1}^N \\log p(y_n | \\mathbf{x}_n, \\mathbf{w})}_{\\text{joint log-likelihood}} + \\log C - \\frac{1}{2s}(\\mathbf{w})^\\top(\\mathbf{w})\\\\ &amp;= \\textrm{argmax}_\\mathbf{w} \\underbrace{\\sum_{n=1}^N \\log p(y_n | \\mathbf{x}_n, \\mathbf{w})}_{\\text{joint log-likelihood}} + \\log C - \\underbrace{\\frac{1}{2s}}_{\\lambda}\\underbrace{\\|\\mathbf{w}\\|_2^2}_{\\text{$\\ell_2$ regularization}}\\\\ &amp;\\equiv \\textrm{argmax}_\\mathbf{w} \\underbrace{\\sum_{n=1}^N \\log p(y_n | \\mathbf{x}_n, \\mathbf{w})}_{\\text{joint log-likelihood}} - \\underbrace{\\frac{1}{2s}}_{\\lambda}\\underbrace{\\|\\mathbf{w}\\|_2^2}_{\\text{$\\ell_2$ regularization}}\\\\ \\end{aligned}\\] In the last line of our derivation, we dropped the constant \\(\\log C\\) since additive constants do not change the location of the optima. 15.2.2 Law of Large Numbers for Bayesian Inference In general, in Bayesian inference we are less interested asymptotic behavior. But the properties of the asymptotic distribution of the posterior can be useful. Theorem: (Berstein-von Mises) “Under some conditions, as \\(N\\to \\infty\\) the posterior distribution converges to a Gaussian distribution centred at the MLE with covariance matrix given by a function of the Fisher information matrix at the true population parameter value.” In English: Why Should You Care About This Theorem? 1. The posterior point estimates (like MAP) approach the MLE, with large samples sizes. 2. It may be valid to approximate an unknown posterior with a Gaussian, with large samples sizes. This will become a very important idea for Bayesian Logistic Regression! 15.3 Bayesian Logistic Regression For Bayesian linear regression with Gaussian likelihood and Gaussian prior, we can derive all the marginal and conditional pdfs analytically – they are all Gaussians! This property – that the posterior pdf form is known and determined by the likelihood and prior – is called conjugacy, in particular, we call the pair of compatible likelihood and prior conjugate. At this point, you might be mislead into thinking that Bayesian modeling involves a bunch of analytic derivations – working with conjugate pairs. Unfortunately, this is not the case! For most Bayesian models, the marginal and conditional distributions of interest cannot be analytically derived or evey evaluated – inference in Bayesian modeling is largely approximate and computational. In fact, Bayesian versions of very common simple models can already yield intractable inference! Let’s recall the likelihood of logistic regression: \\[ p(y|\\mathbf{w}, \\mathbf{x}) = \\sigma(\\mathbf{w}^\\top\\mathbf{x})^y (1 - \\sigma(\\mathbf{w}^\\top\\mathbf{x}))^{1-y}. \\] Now, let’s again choose a Gaussian prior for \\(\\mathbf{w}\\): \\[ \\mathbf{w} \\sim \\mathcal{N}(\\mathbf{m}, \\mathbf{S}). \\] What would the posterior of Bayesian logistic regression look like? \\[\\begin{aligned} p(\\mathbf{w}|y, \\mathbf{x}) &amp;\\propto p(y|\\mathbf{w}, \\mathbf{x})p(\\mathbf{w})\\\\ =&amp; \\sigma(\\mathbf{w}^\\top\\mathbf{x})^y (1 - \\sigma(\\mathbf{w}^\\top\\mathbf{x}))^{1-y}\\frac{1}{\\sqrt{(2\\pi)^D \\mathbf{det}(\\Sigma)}} \\mathrm{exp}\\left\\{ - \\frac{1}{2}(\\mathbf{w} - \\mathbf{m})^\\top \\mathbf{S}^{-1}(\\mathbf{w} - \\mathbf{m})\\right\\}\\\\ =&amp; \\frac{1}{\\sqrt{(2\\pi)^D \\mathbf{det}(\\Sigma)}} \\left(\\frac{1}{1 + \\mathrm{exp}\\{-\\mathbf{w}^\\top\\mathbf{x}\\}}\\right)^y\\left(1-\\frac{1}{1 + \\mathrm{exp}\\{-\\mathbf{w}^\\top\\mathbf{x}\\}}\\right)^{1-y}\\\\ &amp;* \\mathrm{exp}\\left\\{ - \\frac{1}{2}(\\mathbf{w} - \\mathbf{m})^\\top \\mathbf{S}^{-1}(\\mathbf{w} - \\mathbf{m})\\right\\} \\end{aligned}\\] Does this look like a pdf that you know – in particular, does this look like a Gaussian pdf? It turns out, the pdf posterior of Bayesian logistic regression doens’t have an easy known form – that is, is not the pdf of a RV who’s moment generating functions we know. In this case, we say that the likelihood and prior in Bayesian Logistic Regression is non-conjugate. But why do we care that we recognize the posterior as the pdf of a “named” distribution? What’s Hard About Bayesian Inference? In non-Bayesian probabilistic ML, your primary math task is optimization, that is, you spend your time writing down objective functions, finding their gradients (using autograd) and then making gradient descent converge to a reasonable optimum. In Bayesian ML, you have two primary math tasks: 1. (Inference) sampling from priors, posteriors, prior predictives and posterior predictives 2. (Evaluation) computing integrals expressing the log-likelihood of train and test data under the posterior and the log-likelihood of train and test data under the prior Each task – sampling and integration – is incredibly mathematically and computationally difficult! You will see on Tuesday that even sampling from known (“named”) distributions is an incredbily complex problem! Why Should You Care? In practice, for every problem you have to choose a modeling paradigm that suits the characteristics and constraints of the problem as well as suits your goals. When you are choosing between Bayesian and non-Bayesian paradigms, you know that one trade-off you always have to weight is between the relative easy of optimization (autograd + gradient descent) and the computational complexity of sampling and the computational intractability of integration! In short, you need to know what your’re getting into when you go Bayesian and what value being Bayesian brings you! "],["whats-hard-about-sampling.html", "Chapter 16 What’s Hard About Sampling? 16.1 Bayesian vs Frequentist Inference 16.2 What is Sampling and Why do We Care?", " Chapter 16 What’s Hard About Sampling? 16.1 Bayesian vs Frequentist Inference Frequentist Modeling: In non-Bayesian probabilistic modeling we typically perform maximum likelihood inference – that is, solve an optimization problem. Relatively speaking, optimization problems are much easier to solve: (Differentiation) we have autograd (Solving for Stationary Points) we have gradient descent to solve for stationary points Bayesian Modeling: Bayesian modeling is, in comparison, orders of magnitude more mathematically and computationally challenging: (Inference) sampling (from priors, posteriors, prior predictives and posterior predictives) is hard, especially for non-conjugate likelihood, prior pairs! (Evaluation) computing integrals (for the log-likelihood of train and test data under the posterior) is intractable, when likelihoods and priors are not conjugate! 16.2 What is Sampling and Why do We Care? We first need to generating truly random iid sequence of numbers. (Uniform Distribution) Linear Congruence We use simple distributions (that we know how to sample from) to help us sample from complex target distributions. (Inverse CDF) Requires us to compute integrals (i.e. know the CDF) (Rejection Sampling) Uses simple distributions to “propose samples” and uses the complex target distribution to accept or rejct the samples’ (MCMC Samplers) Uses “local rejection sampling”. We need to evaluate samplers. (Correctness) There is no way to verify that a sampling algorithm is correct except through proof (Efficiency) Correctness is useless if it takes too many iterations to produce one valid sample Everything requires Sampling! (Integration Involves Sampling) In ML, intractable integrals are approximated via sampling, this is called Monte Carlo Integration. (Sampling Can Help Optimization) In ML, we can sometimes find better optima for non-convex optimization problems by incorporating sampling into our gradient descent. This is called simulated annealing. Sometimes sampling requires optimization! (Variational Inference) In ML, we often choose to approximate a complex distribution with a simple one (e.g. Gaussian), rather than sample directly from the target. Finding the best simple approximation of a complex target distribution is called variational inference and it’s an optimization problem! (Hamiltonian Monte Carlo Sampling) Sometimes incorporating gradient descent during sampling can help increase the efficiency of our samplers – i.e. speed up the time we need to wait for a valid sample. These methods are called Hamiltonian Monte Carlo Samplers. "],["the-math-of-principal-component-analysis.html", "Chapter 17 The Math of Principal Component Analysis 17.1 PCA as Dimensionality Reduction to Maximize Variance 17.2 PCA as Dimensionality Reduction to Minimize Reconstruction Loss 17.3 A Latent Variable Model for PCA 17.4 Autoencoders and Nonlinear PCA 17.5 A Probabilistic Latent Variable Model for PCA", " Chapter 17 The Math of Principal Component Analysis In these notes, we show you how to formalize Principal Component Analysis (PCA) as two equivalent optimization problems. 17.1 PCA as Dimensionality Reduction to Maximize Variance In the lecture vidoes, we said that PCA is the process of finding a small set of orthogonal (and normal) vectors that indicate the directions of greatest variation in the data. So that when we project the data onto this small set of new features, we capture the most amount of interesting variations in the original data. How do we formalize PCA mathematically? 17.1.1 Finding a Single PCA Component Let’s start by defining what we mean by “variations” in the data when projected onto a single component. Remember that in stats, we measure the spread of data with sample variance: \\[ \\text{sample variance} = \\frac{1}{N-1}\\sum_{n=1}^N (z_n - \\overline{z})^2 \\] Now we can formalize our objective: find a direction given by \\(v\\in \\mathbb{R}^D\\) such that the sample variance of the projection of \\(\\mathbf{X}\\) onto the line (1D linear subspace) determined by \\(v\\) is maximized. In other words, we want: \\[\\begin{aligned} v^* &amp;= \\mathrm{argmax}_v \\frac{1}{N-1}\\sum_{n=1}^N \\left(v^\\top \\mathbf{x}_n - \\frac{1}{N} \\sum_{i=1}^N v^\\top \\mathbf{x}_i\\right)^2\\\\ &amp;= \\mathrm{argmax}_v \\frac{1}{N-1}\\sum_{n=1}^N \\left(v^\\top \\mathbf{x}_n - v^\\top \\left(\\frac{1}{N} \\sum_{i=1}^N \\mathbf{x}_i\\right)\\right)^2\\\\ &amp;= \\mathrm{argmax}_v \\frac{1}{N-1}\\sum_{n=1}^N (v^\\top \\mathbf{x}_n - v^\\top \\overline{\\mathbf{x}})^2 \\end{aligned}\\] subject to the constraint that \\(v\\) is a normal vector \\[ \\|v\\|_2 = 1 \\] To summarize the first principle component, \\(v^*\\) of PCA is the solution to the following: \\[\\begin{aligned} &amp;\\mathrm{argmax}_v \\frac{1}{N-1}\\sum_{n=1}^N (v^\\top \\mathbf{x}_n - v^\\top \\overline{\\mathbf{x}})^2 \\\\ &amp;\\text{such that } \\|v\\|_2 = 1 \\end{aligned}\\] Optimization problems like the one above are called constrained optimization problems, since we want to optimize our objective while satisfying a condition. 17.1.1.1 The Good News The good news is that we’ve seen optimization before, quite a lot, in the first 1/2 of the semester. We know that the general procedure is to: 1. find the stationary points of the objective function: set the gradient to zero 2. check for convexity of objective function: make sure the stationary points are actually global optima 17.1.1.2 The Bad News The problem is that we’ve never worked with constrained optimization problems before! It turns out, for constrained problems, there are analogue steps to solving them: 1. find the stationary points of an “augmented” objective function (the Lagrangian) 2. check for global optimality – this involves a much more complex set of checks Generally speaking, solving constrained optimization problems is way harder than solving unconstrained optimization problems! 17.1.1.3 The Solution: Relating PCA and Singular Value Decomposition (SVD) Luckily, the PCA constrained optimization problem has additional structure/knowledge we can exploit. In particular, we can rewrite the PCA objective function as follows: \\[\\begin{aligned} \\mathrm{argmax}_v\\; \\frac{1}{N-1}\\sum_{n=1}^N (v^\\top \\mathbf{x}_n - v^\\top \\overline{\\mathbf{x}})^2 &amp;= \\mathrm{argmax}_v\\; v^\\top \\mathbf{S} v \\end{aligned}\\] where the matrix \\(\\mathbf{S}\\) is the empirical covariance matrix: \\[ \\mathbf{S} = \\frac{1}{N}\\sum_{n=1}^N (\\mathbf{x}_n - \\overline{\\mathbf{x}})(\\mathbf{x}_n - \\overline{\\mathbf{x}})^\\top. \\] In this case, when we find the stationary points of the Lagrangian of the constrained optimization problem, \\[ \\mathcal{L}(v, \\alpha) = v^\\top \\mathbf{S} v - \\alpha (1- v^\\top v) \\] we get that stationary points must satisfy \\[\\begin{aligned} \\frac{\\partial \\mathcal{L}}{\\partial v} = \\mathbf{S}v - \\alpha v &amp;=0\\\\ \\mathbf{S}v &amp;= \\alpha v \\end{aligned}\\] for some constant \\(\\alpha\\in \\mathbf{R}\\). That is, a stationary point of the Lagrangian must be an eigenvector of the matrix \\(\\mathbf{S}\\). Moreover, we see that at every stationary point, the PCA objective can be rewritten as: \\[\\begin{aligned} v^\\top \\mathbf{S} v = \\alpha v^\\top v = \\alpha \\end{aligned}\\] Thus, the stationary point of the Lagrangian that maximizes the PCA objective must be the eigenvector \\(v\\) of \\(\\mathbf{S}\\) such that the associated eigenvalue \\(\\alpha\\) is the largest. That is, finding the top PCA component \\(v\\) reduces to finding the eigenvector with the largest eigenvalue of the empirical covariance matrix \\(\\mathbf{S}\\). Iterating this math analysis \\(K\\) number of times, shows us that finding the top \\(K\\) PCA components is equivalent to finding the \\(K\\) eigenvectors of \\(\\mathbf{S}\\) that have the largest eigenvalues. Thus, singular value decomposition (SVD) – a linear algebra method for finding all the eigenvectors and eigenvalues of a given matrix \\(\\mathbf{S}\\) – is often used to identify the top \\(K\\) PCA components. 17.2 PCA as Dimensionality Reduction to Minimize Reconstruction Loss But why do we care about finding low-dimensional projections of our data that preserves as much variance in the data as possible? One could argue that preserving variations in the data allows us to potentially reconstruct important properties of the original data. But why not formalize data reconstruction as our objective in the first place? That is, we could have formalized our problem as “find \\(v\\) such that the projection of \\(\\mathbf{X}\\) onto the line determined by \\(v\\) minimizes reconstruction error”: \\[\\begin{aligned} &amp;\\mathrm{argmin}_v \\frac{1}{N}\\sum_{n=1}^N \\|\\mathbf{x}_n - \\widehat{\\mathbf{x}}_n \\|_2^2\\\\ &amp;\\text{such that} \\|v\\|_2 = 1 \\end{aligned}\\] where \\(\\widehat{\\mathbf{x}}_n\\) is related to the projection of \\(\\mathbf{x}_n\\) onto the line determined by \\(v\\), interpreted as a “reconstruction” of \\(\\mathbf{x}_n\\). 17.2.0.1 Centered Data If the data is centered, i.e. \\(\\overline{\\mathbf{x}} = \\mathbf{0}\\), then we can set \\(\\widehat{\\mathbf{x}}_n\\) to be directly the projection of \\(\\mathbf{x}_n\\) onto \\(v\\), \\[ \\widehat{\\mathbf{x}}_n = v(v^\\top \\mathbf{x}_n) \\] then it turns out that minimizing the reconstruction error is equivalent to maximizing the sample variance after projection! That is, minimizing “reconstruction error” is another way to formalize our PCA objective! See Chapter 7 of your textbook for the proof (note that in the textbook, we use \\(w\\) for principle components instead of \\(v\\)). 17.2.0.2 What If the Data is Not Centered? If the data is centered, i.e. \\(\\overline{\\mathbf{x}} \\neq \\mathbf{0}\\), then we set \\(\\widehat{\\mathbf{x}}_n = v(v^\\top (\\mathbf{x}_n - \\overline{\\mathbf{x}})) + \\overline{\\mathbf{x}}\\), i.e. we “center” the data and add the mean back after projection. The reason we need to center the data is that the vector \\(v\\) determines a line through the origin! So if the data is not centered at the origin, no \\(v\\) can do a good job of reconstructing our data. For \\(\\widehat{\\mathbf{x}}_n = v(v^\\top (\\mathbf{x}_n - \\overline{\\mathbf{x}})) + \\overline{\\mathbf{x}}\\), we can again show that minimizing the reconstruction error is equivalent to maximizing the sample variance after projection! 17.2.0.3 So Should I Center My Data??? If you are implementing your own version of PCA from scratch by doing SVD on the empirical covariance matrix \\(\\mathbf{S}\\), then you don’t need to do anything extra to pre-process your data. If you are using someone else’s PCA code (including staff code), check the documentation to see if centering your data is necessary! 17.3 A Latent Variable Model for PCA The main motivation for using PCA is data compression: we think that our data doesn’t occupy the entire \\(D\\)-number of dimensions of the input space; instead, it occupies a smaller \\(K\\)-dimensional linear manifold inside \\(\\mathbb{R}^D\\). 17.3.1 One Principle Component When \\(K=1\\), another way to formalize PCA is to suppose the following (non-probabilistic) model for our data: \\[ \\widehat{\\mathbf{x}}_n = f_v(z_n) = v z_n \\] where \\(v\\in \\mathbb{R}^D\\) and \\(z_n \\in \\mathbb{R}\\), \\(f_v\\) is a function mapping \\(\\mathbb{R}\\) to \\(\\mathbb{R}^D\\). The PCA objective can be rederived for this model: \\[\\begin{aligned} &amp;\\mathrm{argmin}_v \\frac{1}{N}\\sum_{n=1}^N \\|\\mathbf{x}_n - f_v(z_n) \\|_2^2\\\\ &amp;\\text{such that } z_n = g_v(\\mathbf{x}_n) = v^\\top \\mathbf{x}_n, \\; \\|v\\|_2=1 \\end{aligned}\\] Under this model of PCA, we see that PCA is just a special form of linear regression where we know the target \\(\\mathbf{x}_n\\) but we don’t know the covariate \\(v_n\\)! Since the covariate \\(v_n\\) is unobserved, we call it a latent variable. The PCA objective gets around the fact that we are not given \\(z_n\\), by imposing that we can infer \\(z_n\\) by “reversing” the map \\(f\\) (that is, we find \\(z_n\\) and \\(f\\) simultaneously). 17.4 Autoencoders and Nonlinear PCA The above formulation of PCA is extremely important, since if we replace \\(f\\) and \\(g\\) by arbitrary functions represented by neural networks, then we have derived the autoencoder model: \\[\\begin{aligned} &amp;\\mathrm{argmin}_{\\mathbf{V}, \\mathbf{W}} \\frac{1}{N}\\sum_{n=1}^N \\|\\mathbf{x}_n - f_\\mathbf{V}(\\mathbf{z}_n) \\|_2^2\\\\ &amp;\\text{such that } \\mathbf{z}_n = g_\\mathbf{W}(\\mathbf{x}_n) \\end{aligned}\\] where \\(f_\\mathbf{V}: \\mathbf{R}^K \\to \\mathbf{R}^D\\) is called the decoder and \\(g_\\mathbf{W}: \\mathbf{R}^D \\to \\mathbf{R}^K\\) is called the encoder. In other words, the autoencoder can be considered to be a nonlinear version of PCA. 17.5 A Probabilistic Latent Variable Model for PCA Remember for linear regression we created probabilistic models by positing a source of observation noise. We can derive a probabilistic model for PCA in a similar fashion. That is, we suppose that our data started as some distribution in the low-dimensional latent space and was mapped to noisy observations: \\[\\begin{aligned} \\mathbf{z}_n &amp;\\sim \\mathcal{N}(\\mathbf{0}, I_{K\\times K})\\\\ \\mathbf{x}_n &amp;= f_\\mathbf{V}(\\mathbf{z}_n) + \\epsilon = \\mathbf{V} \\mathbf{z}_n + \\epsilon,\\;\\; \\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 I_{D\\times D}) \\end{aligned}\\] This model is called probabilistic PCA (pPCA). "],["the-math-of-expectation-maximization.html", "Chapter 18 The Math of Expectation Maximization", " Chapter 18 The Math of Expectation Maximization "],["motivation-for-latent-variable-models.html", "Chapter 19 Motivation for Latent Variable Models 19.1 Latent Variable Models 19.2 Maximum Likelihood Estimation for Latent Variable Models: Expectation Maximization 19.3 The Expectation Maximization Algorithm 19.4 Monotonicity and Convergence of EM", " Chapter 19 Motivation for Latent Variable Models 19.0.0.1 A Model for Birth Weights Recall our model for birth weigths, \\(Y_1,\\ldots, Y_N\\). We posited that the birth weights are iid normally distributed with known \\(\\sigma^2\\), \\(Y_n \\sim \\mathcal{N}(\\mu, 1)\\). Compare the maximum likelihood model and the Bayesian model for bith weight. Which model would you use to make clinical decisions? What’s hard about this comparison? 19.0.0.2 A Similarity Measure for Distributions: Kullback–Leibler Divergence Visually comparing models to the empirical distribution of the data is impractical. Fortunately, there are a large number of quantitative measures for comparing two distributions, these are called divergence measures. For example, the Kullback–Leibler (KL) Divergence is defined for two distributions \\(p(\\theta)\\) and \\(q(\\theta)\\) supported on \\(\\Theta\\) as: \\[ D_{\\text{KL}}[q \\,\\|\\, p] = \\int_{\\Theta} \\log\\left[\\frac{q(\\theta)}{p(\\theta)} \\right] q(\\theta)d\\theta \\] The KL-divergence \\(D_{\\text{KL}}[q \\,\\|\\, p]\\) is bounded below by 0, which happens if and only if \\(q=p\\). The KL-divergence has information theoretic interpretations that we will explore later in the course. Note: The KL-divergence is defined in terms of the pdf’s of \\(p\\) and \\(q\\). If \\(p\\) is a distribution from which we only have samples and not the pdf (like the empirical distribution), we can nontheless estimate \\(D_{\\text{KL}}[q \\,\\|\\, p]\\). Techniques that estimate the KL-divergence from samples are called non-parametric. We will use them later in the course. (OPTIONAL!!!) Why is the KL bounded below by 0? First let’s see why the answer isn’t obvious. Recall that the KL divergence is the expected log ratio between two distribution: \\[ D_{\\text{KL}} [q\\| p] = \\mathbb{E}_{q}\\left[ \\log &gt; \\frac{q}{p}\\right] \\] Now, we know that when \\(q\\) is less than \\(p\\) (i.e. \\(q/p &lt; 1\\)) then the log can be an arbitrarily negative number. So it’s not immediately obvious that the expected value of this fraction should always be non-negative! An Intuitive Explanation Let the blue curve be q and the red be p. We have \\(q &lt; p\\) from \\((-\\infty, 55)\\), on this part of the domain \\(\\log(q/p)\\) is negative. On \\([55, \\infty)\\), \\(\\log(q/p)\\) is nonnegative. However, since we are sampling from \\(q\\), and \\(q\\)’s mass is largely over \\([55, \\infty)\\), the log fraction \\(\\log(q/p\\)) will tend to be nonnegative. A Formal Argument There are many proofs of the non-negativity of the KL. Ranging from the very complex to the very simple. Here is one that just involves a bit of algebra: We want to show that \\(D_{\\text{KL}}[q\\|p] \\geq 0\\). Instead we’ll show, equivalently, that \\(-D_{\\text{KL}}[q\\|p] \\leq 0\\) (we’re choosing show the statement about the negative KL, just so we can flip the fraction on the inside of the log and cancel terms): 19.1 Latent Variable Models Models that include an observed variable \\(Y\\) and at least one unobserved variable \\(Z\\) are called latent variable models. In general, our model can allow \\(Y\\) and \\(Z\\) to interact in many different ways. Today, we will study models with one type of interaction: 19.1.1 Example: Gaussian Mixture Models (GMMs) In a Gaussian Mixture Model (GMM), we posit that the observed data \\(Y\\) is generated by a mixture, \\(\\pi=[\\pi_1, \\ldots, \\pi_K]\\), of \\(K\\) number of Gaussians with means \\(\\mu = [\\mu_1, \\ldots, \\mu_K]\\) and covariances \\(\\Sigma = [\\Sigma_1, \\ldots, \\Sigma_K]\\). For each observation \\(Y_n\\) the class of the observation \\(Z_n\\) is a latent variable that indicates which of the \\(K\\) Gaussian is responsible for generating \\(Y_n\\): \\[\\begin{aligned} Z_n &amp;\\sim Cat(\\pi),\\\\ Y_n | Z_n&amp;\\sim \\mathcal{N}(\\mu_{Z_n}, \\Sigma_{Z_n}), \\end{aligned}\\] where \\(n=1, \\ldots, N\\) and \\(\\sum_{k=1}^K \\pi_k = 1\\). GMMs are examples of model based clustering - breaking up a data set into natural clusters based on a statistical model fitted to the data. 19.1.2 Item-Response Models In item-response models, we measure an real-valued unobserved trait \\(Z\\) of a subject by performing a series of experiments with binary observable outcomes, \\(Y\\): \\[\\begin{aligned} Z_n &amp;\\sim \\mathcal{N}(\\mu, \\sigma^2),\\\\ \\theta_n &amp;= g(Z_n)\\\\ Y_n|Z_n &amp;\\sim Ber(\\theta_n), \\end{aligned}\\] where \\(n=1, \\ldots, N\\) and \\(g\\) is some fixed function of \\(Z_n\\). 19.1.2.1 Applications Item response models are used to model the way “underlying intelligence” \\(Z\\) relates to scores \\(Y\\) on IQ tests. Item response models can also be used to model the way “suicidality” \\(Z\\) relates to answers on mental health surveys. Building a good model may help to infer when a patient is at psychiatric risk based on in-take surveys at points of care through out the health-care system. 19.1.3 Example: Factor Analysis Models In factor analysis models, we posit that the observed data \\(Y\\) with many measurements is generated by a small set of unobserved factors \\(Z\\): \\[\\begin{aligned} Z_n &amp;\\sim \\mathcal{N}(0, I),\\\\ Y_n|Z_n &amp;\\sim \\mathcal{N}(\\mu + \\Lambda Z_n, \\Phi), \\end{aligned}\\] where \\(n=1, \\ldots, N\\), \\(Z_n\\in \\mathbb{R}^{D&#39;}\\) and \\(Y_n\\in \\mathbb{R}^{D}\\). We typically assume that \\(D&#39;\\) is much smaller than \\(D\\). 19.1.3.1 Applications Factor analysis models are useful for biomedical data, where we typically measure a large number of characteristics of a patient (e.g. blood pressure, heart rate, etc), but these characteristics are all generated by a small list of health factors (e.g. diabetes, cancer, hypertension etc). Building a good model means we may be able to infer the list of health factors of a patient from their observed measurements. 19.2 Maximum Likelihood Estimation for Latent Variable Models: Expectation Maximization Given a latent variable model \\(p(Y, Z| \\phi, \\theta) = p(Y | Z, \\phi) p(Z|\\theta)\\), we are interested computing the MLE of parameters \\(\\phi\\) and \\(\\theta\\): \\[\\begin{aligned} \\theta_{\\text{MLE}}, \\phi_{\\text{MLE}} &amp;= \\underset{\\theta, \\phi}{\\mathrm{argmax}}\\; \\ell(\\theta, \\phi)\\\\ &amp;= \\underset{\\theta, \\phi}{\\mathrm{argmax}}\\; \\log \\prod_{n=1}^N \\int_{\\Omega_Z} p(y_n, z_n | \\theta, \\phi) dz\\\\ &amp;= \\underset{\\theta, \\phi}{\\mathrm{argmax}}\\; \\log \\prod_{n=1}^N \\int_{\\Omega_Z} p(y_n| z_n, \\phi)p(z_n| \\theta) dz \\end{aligned}\\] where \\(\\Omega_Z\\) is the domain of \\(Z\\). Why is this an hard optimization problem? There are two major problems: 1. the product in the integrand 2. gradients cannot be past the integral (i.e. we cannot easily compute the gradient to solve the optimization problem). We solve these two problems by: 1. pushing the log past the integral so that it can be applied to the integrand (Jensen’s Inequality) 2. introducing an auxiliary variables \\(q(Z_n)\\) to allow the gradient to be pushed past the integral. \\[\\begin{aligned} \\underset{\\theta, \\phi}{\\mathrm{max}}\\; \\ell(\\theta, \\phi) &amp;= \\underset{\\theta, \\phi, q}{\\mathrm{max}}\\; \\log \\prod_{n=1}^N\\int_{\\Omega_Z} \\left(\\frac{p(y_n, z_n|\\theta, \\phi)}{q(z_n)}q(z_n)\\right) dz\\\\ &amp;= \\underset{\\theta, \\phi, q}{\\mathrm{max}}\\; \\log\\,\\prod_{n=1}^N\\mathbb{E}_{Z\\sim q(Z)} \\left[ \\frac{p(y_n, Z|\\theta, \\phi)}{q(Z)}\\right]\\\\ &amp;= \\underset{\\theta, \\phi, q}{\\mathrm{max}}\\; \\sum_{n=1}^N \\log \\mathbb{E}_{Z\\sim q(Z)} \\left[\\,\\left( \\frac{p(y_n, Z|\\theta, \\phi)}{q(Z)}\\right)\\right]\\\\ &amp;\\geq \\underset{\\theta, \\phi, q}{\\mathrm{max}}\\; \\underbrace{\\sum_{n=1}^N\\mathbb{E}_{Z_n\\sim q(Z)} \\left[ \\log\\,\\left(\\frac{p(y_n, Z_n|\\theta, \\phi)}{q(Z_n)}\\right)\\right]}_{ELBO(\\theta, \\phi)}, \\quad (\\text{Jensen&#39;s Inequality})\\\\ \\end{aligned}\\] We call \\(\\sum_{n=1}^N\\mathbb{E}_{Z_n\\sim q(Z)} \\left[ \\log\\,\\left(\\frac{p(y_n, Z_n|\\theta, \\phi)}{q(Z)}\\right)\\right]\\) the Evidence Lower Bound (ELBO). Note that maximizing the ELBO will yield a lower bound of the maximum value of the log likelihood. Although the optimal point of the ELBO may not be the optimal point of the log likelihood, we nontheless prefer to optimize the ELBO because the gradients, with respect to \\(\\theta, \\phi\\), of the ELBO are easier to compute: \\[\\begin{aligned} \\nabla_{\\theta, \\phi} ELBO(\\theta, \\phi) &amp;= \\nabla_{\\theta, \\phi}\\left[ \\sum_{n=1}^N\\mathbb{E}_{Z_n\\sim q(Z)} \\left[ \\log\\,\\left(\\frac{p(y_n, Z_n|\\theta, \\phi)}{q(Z_n)}\\right)\\right]\\right] \\\\ &amp;= \\sum_{n=1}^N\\mathbb{E}_{Z_n\\sim q(Z)} \\left[ \\nabla_{\\theta, \\phi} \\left( \\log\\,\\left(\\frac{p(y_n, Z_n|\\theta, \\phi)}{q(Z_n)}\\right)\\right)\\right] \\end{aligned}\\] Note that we can push the gradient \\(\\nabla_{\\theta, \\phi}\\) past the expectation \\(\\mathbb{E}_{Z_n\\sim q(Z)}\\) since the expectation is not computed with respect to our optimization variables! Rather than optimizing the ELBO over all variables \\(\\theta, \\phi, q\\) (this would be hard), we optimize one set of variables at a time: 19.2.0.1 Step I: the M-step Optimize the ELBO with respect to \\(\\theta, \\phi\\): \\[\\begin{aligned} \\theta^*, \\phi^* = \\underset{\\theta, \\phi}{\\mathrm{max}}\\; ELBO(\\theta, \\phi, q) &amp;= \\underset{\\theta, \\phi}{\\mathrm{max}}\\; \\sum_{n=1}^N\\mathbb{E}_{Z_n\\sim q(Z)} \\left[ \\log\\,\\left(\\frac{p(y_n, Z_n|\\theta, \\phi)}{q(Z_n)}\\right)\\right]\\\\ &amp;= \\underset{\\theta, \\phi}{\\mathrm{max}}\\; \\sum_{n=1}^N \\int_{\\Omega_Z} \\log\\,\\left(\\frac{p(y_n, z_n|\\theta, \\phi)}{q(z_n)}\\right)q(z_n) dz_n\\\\ &amp;= \\underset{\\theta, \\phi}{\\mathrm{max}}\\; \\sum_{n=1}^N \\int_{\\Omega_Z} \\log\\,\\left(p(y_n, z_n|\\theta, \\phi)\\right) q(z_n)dz_n - \\underbrace{\\int_{\\Omega_Z} \\log \\left(q(z_n)\\right)q(z_n) dz_n}_{\\text{constant with respect to }\\theta, \\phi}\\\\ &amp;\\equiv \\underset{\\theta, \\phi}{\\mathrm{max}}\\;\\sum_{n=1}^N \\int_{\\Omega_Z} \\log\\,\\left(p(y_n, z_n|\\theta, \\phi)\\right) q(z_n)dz_n\\\\ &amp;= \\underset{\\theta, \\phi}{\\mathrm{max}}\\;\\sum_{n=1}^N \\mathbb{E}_{Z_n\\sim q(Z)} \\left[ \\log\\left(p(y_n, z_n|\\theta, \\phi)\\right)\\right] \\end{aligned}\\] 19.2.0.2 Step II: the E-step Optimize the ELBO with respect to \\(q\\): \\[\\begin{aligned} q^*(Z_n) = \\underset{q}{\\mathrm{argmax}}\\;\\left(\\underset{\\theta, \\phi}{\\mathrm{argmax}}\\; ELBO(\\theta, \\phi, q) \\right) = \\underset{q}{\\mathrm{argmax}}\\; ELBO(\\theta^*, \\phi^*, q) \\end{aligned}\\] Rather than optimizing the ELBO with respect to \\(q\\), which seems hard, we will argue that optimizing the ELBO is equivalent to optimizing another function of \\(q\\), one whose optimum is easy for us to compute. Note: We can recognize the difference between the log likelihood and the ELBO as a function we’ve seen: \\[\\begin{aligned} \\ell(\\theta, \\phi) - ELBO(\\theta, \\phi, q) &amp;= \\sum_{n=1}^N \\log p(y_n| \\theta, \\phi) - \\sum_{n=1}^N \\int_{\\Omega_Z} \\log\\left(\\frac{p(y_n, z_n|\\theta, \\phi)}{q(z_n)}\\right)q(z_n) dz_n\\\\ &amp;= \\sum_{n=1}^N \\int_{\\Omega_Z} \\log\\left(p(y_n| \\theta, \\phi)\\right) q(z_n) dz_n - \\sum_{n=1}^N \\int_{\\Omega_Z} \\log\\left(\\frac{p(y_n, z_n|\\theta, \\phi)}{q(z_n)}\\right)q(z_n) dz_n\\\\ &amp;= \\sum_{n=1}^N \\int_{\\Omega_Z} \\left(\\log\\left(p(y_n| \\theta, \\phi)\\right) - \\log\\left(\\frac{p(y_n, z_n|\\theta, \\phi)}{q(z_n)}\\right) \\right)q(z_n) dz_n\\\\ &amp;= \\sum_{n=1}^N \\int_{\\Omega_Z} \\log\\left(\\frac{p(y_n| \\theta, \\phi)q(z_n)}{p(y_n, z_n|\\theta, \\phi)} \\right)q(z_n) dz_n\\\\ &amp;= \\sum_{n=1}^N \\int_{\\Omega_Z} \\log\\left(\\frac{q(z_n)}{p(z_n| y_n, \\theta, \\phi)} \\right)q(z_n) dz_n, \\\\ &amp;\\quad\\left(\\text{Baye&#39;s Rule: } \\frac{p(y_n, z_n|\\theta, \\phi)}{p(y_n| \\theta, \\phi)} = p(z_n| y_n, \\theta, \\phi)\\right)\\\\ &amp;= \\sum_{n=1}^N D_{\\text{KL}} \\left[ q(Z_n) \\| p(Z_n| Y_n, \\theta, \\phi)\\right]. \\end{aligned}\\] Since \\(\\ell(\\theta, \\phi)\\) is a constant, the difference \\[\\sum_{n=1}^N D_{\\text{KL}} \\left[ q(Z_n) \\| p(Z_n| Y_n, \\theta, \\phi)\\right] = \\ell(\\theta, \\phi) - ELBO(\\theta, \\phi, q)\\] descreases when \\(ELBO(\\theta, \\phi, q)\\) increases (and vice versa). Thus, maximizing the ELBO is equivalent to minimizing \\(D_{\\text{KL}} \\left[ q(Z_n) \\| p(Y_n| Z_n, \\theta, \\phi)\\right]\\): \\[ \\underset{q}{\\mathrm{argmax}}\\, ELBO(\\theta, \\phi, q) = \\underset{q}{\\mathrm{argmin}}\\sum_{n=1}^N D_{\\text{KL}} \\left[ q(Z_n) \\| p(Z_n| Y_n, \\theta, \\phi)\\right]. \\] Thus, we see that \\[\\begin{aligned} q^*(Z_n) &amp;= \\underset{q}{\\mathrm{argmax}}\\; ELBO(\\theta^*, \\phi^*, q) \\\\ &amp;= \\underset{q}{\\mathrm{argmin}}\\sum_{n=1}^N D_{\\text{KL}} \\left[ q(Z_n) \\| p(Z_n| Y_n, \\theta, \\phi)\\right] = p(Z_n| Y_n, \\theta, \\phi) \\end{aligned}\\] That is, we should set the optimal distribution \\(q\\) to be the posterior \\(p(Z_n| Y_n, \\theta, \\phi)\\). 19.2.0.3 Iteration Of course, we know that optimizing a function with respect to each variable is not sufficient for finding the global optimum over all the variables, considered together! Thus, performing one E-step and one M-step is not enough to maximize the ELBO. We need to repeat the two steps over and over. (OPTIONAL!!!!) Why don’t gradients commute with expectation? We have the following property of expectations: \\[ \\nabla_z \\mathbb{E}_{x\\sim p(x)}[f(x, z)] = \\mathbb{E}_{x\\sim p(x)}[ \\nabla_z f(x, z)] \\] That is, when the gradient is with respect to a variable that does not appear in the distribution with respect to which you are taking the expectation, then you can push the gradient past the expectation. The intuition: the gradient with respect to \\(z\\) is computing the changes in a function by making infinitesimally small changes to \\(z\\), the expectation is computing the average value of a function by sampling \\(x\\) from a distribution that does not depend on \\(z\\). Each operation is making an independent change to two different variables and hence can be done in any order. Why can’t you do this in general? I.e. why is it that, \\[ \\nabla_z\\mathbb{E}_{x\\sim p(x|z)}[f(x, z)] \\neq \\mathbb{E}_{x\\sim p(x|z)}[ \\nabla_z f(x, z)]?\\] The intuition: the gradient with respect to z is computing the changes in a function by making infinitesimally small changes to z, which in turn affects the samples produced by p(x|z), these samples finally affect the output of f. This is a chain of effects and the order matters. The formal proof: Consider the following case, \\[ p(x\\vert z) = (z+1)x^z,\\; x\\in [0, 1] \\] and \\[ f(x, z) = xzf ( x , z ) = x z. \\] Then, we have \\[\\begin{aligned} \\nabla_z \\mathbb{E}_{x\\sim p(x|z)} [f(x, z)] &amp;= \\nabla_z \\int_0^1 f(x, z) p(x|z) dx \\\\ &amp;= \\nabla_z\\int_0^1 xz \\cdot (z+1)x^z dx \\\\ &amp;= \\nabla_z z (z+1)\\int_0^1x^{z+1} dx \\\\ &amp;= \\nabla_z \\frac{z (z+1)}{z+2} [x^{z+2} ]_0^1 \\\\ &amp;= \\nabla_z \\frac{z (z+1)}{z+2} \\\\ &amp;= \\frac{z^2 + 4z + 2}{(z+2)^2} \\end{aligned}\\] On the other hand, we have \\[ \\mathbb{E}_{x\\sim p(x|z)}\\left[ \\nabla_z f(x, z) \\right] = \\int_0^1 \\nabla_z[ xz] (z+1)x^zdx = \\int_0^1(z+1)x^{z+1}dx = \\frac{z+1}{z+2} [x^{z+2}]_0^1 = \\frac{z+1}{z+2}. \\] Note that: \\[ \\nabla_z \\mathbb{E}_{x\\sim p(x|z)} [f(x, z)] = \\frac{z^2 + 4z+ 2}{(z+2)^2} \\neq \\frac{z+1}{z+2} = \\mathbb{E}_{x\\sim p(x|z)}\\left[ \\nabla_z f(x, z) \\right]. \\] (Optional!!!) Why do we need to maximize the ELBO with respect to q? Recall that in the derivation of the ELBO, we first introduced an auxiliary variable q to rewrite the observed log-likelihood: \\[ \\log p(y|\\theta, \\phi) = \\log \\int_\\Omega p(y, z| \\theta, \\phi) dz = \\log \\int_\\Omega \\frac{p(y, z| \\theta, \\phi}{q(z)}q(z) dz = \\log \\mathbb{E}_{q(z)} \\left[ \\frac{p(y, z|\\theta, \\phi)}{q(z)} \\right] \\] Again, the reason why we do this is because: when we eventually take the gradient wrt to \\(\\theta, \\phi\\) during optimization we can use the identity \\[ \\nabla_{\\theta, \\phi} \\mathbb{E}_{q(z)}\\left[\\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right] = \\mathbb{E}_{q(z)}\\left[\\nabla_{\\theta, \\phi} \\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right] \\] At this point, there is no need to maximize over q, that is: \\[ \\max_{\\theta, \\phi, q}\\log \\mathbb{E}_{q(z)}\\left[\\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right] = \\max_{\\theta, \\phi}\\log \\mathbb{E}_{q(z)}\\left[\\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right] \\] The \\(q\\) cancels and has no effect on the outcome or process of the optimization (but you can’t just choose any \\(q\\) you want - can you see what are the constraints on \\(q\\)?). Now, the problem is that the log is on the outside of the expectation. This isn’t a problem in the sense that we don’t know how to take the derivative of a logarithm of a complex function (this is just the chain rule ), the problem is that \\[ \\nabla_{\\phi, \\theta} \\frac{p(y, z|\\theta, \\phi)}{q(z)} \\] can be very complex (since p and q are pdf’s) and so over all the gradient of the log expectation is not something you can compute roots for. Here is where we push the log inside the expectation using Jensen’s inequality: \\[ \\log \\mathbb{E}_{q(z)}\\left[\\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right] \\geq \\mathbb{E}_{q(z)}\\left[\\log \\left(\\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right)\\right] \\overset{\\text{def}}{=} ELBO(\\phi, \\theta, q) \\] When we push the log inside the expectation, we obtain the Evidence Lower Bound (ELBO). Now, for any choice of \\(q\\), we always have: \\[ \\max_{\\theta, \\phi}\\log \\mathbb{E}_{q(z)}\\left[\\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right] \\geq \\max_{\\theta, \\phi}ELBO(\\phi, \\theta, q) \\] But the ELBO is not necessarily a tight bound (i.e. maximizing the ELBO can be very far from maximizing the log-likelihood!)! In particular, some choices of \\(q\\) might give you a tighter bound on the log-likelihood than others. Thus, we want to select the \\(q\\) that give us the tightest bound: \\[ \\max_{\\theta, \\phi}\\log \\mathbb{E}_{q(z)}\\left[\\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right] \\geq \\max_{\\theta, \\phi, q}ELBO(\\phi, \\theta, q). \\] 19.3 The Expectation Maximization Algorithm The exepectation maximization (EM) algorithm maximize the ELBO of the model, Initialization: Pick \\(\\theta_0\\), \\(\\phi_0\\). Repeat \\(i=1, \\ldots, I\\) times: E-Step: \\[q_{\\text{new}}(Z_n) = \\underset{q}{\\mathrm{argmax}}\\; ELBO(\\theta_{\\text{old}}, \\phi_{\\text{old}}, q) = p(Z_n|Y_n, \\theta_{\\text{old}}, \\phi_{\\text{old}})\\] M-Step: \\[\\begin{aligned} \\theta_{\\text{new}}, \\phi_{\\text{new}} &amp;= \\underset{\\theta, \\phi}{\\mathrm{argmax}}\\; ELBO(\\theta, \\phi, q_{\\text{new}})\\\\ &amp;= \\underset{\\theta, \\phi}{\\mathrm{argmax}}\\; \\sum_{n=1}^N\\mathbb{E}_{Z_n\\sim p(Z_n|Y_n, \\theta_{\\text{old}}, \\phi_{\\text{old}})}\\left[\\log \\left( p(y_n, Z_n | \\phi, \\theta\\right) \\right]. \\end{aligned}\\] (Optional!!!) Another View of Expectation Maximization: The Auxiliary Function We often denote the expectation in the M-step by \\(Q\\left(\\theta, \\phi| \\theta^{\\text{old}}, \\phi^{\\text{old}}\\right)\\) \\[ Q\\left(\\theta, \\phi| \\theta^{\\text{old}}, \\phi^{\\text{old}}\\right) = \\sum_{n=1}^N\\mathbb{E}_{Z_n\\sim p(Z_n|Y_n, \\theta_{\\text{old}}, \\phi_{\\text{old}})}\\left[\\log \\left( p(y_n, Z_n | \\phi, \\theta\\right) \\right] \\] and call \\(Q\\) the auxiliary function. Frequently, the EM algorithm is equivalently presented as - E-step: compute the auxiliary function: \\(Q\\left(\\theta, \\phi| \\theta^{\\text{old}}, \\phi^{\\text{old}}\\right)\\) - M-step: maximize the auxiliary function: \\(\\theta^{\\text{new}}, \\phi^{\\text{new}} = \\underset{\\theta, \\phi}{\\mathrm{argmax}}\\,Q\\left(\\theta, \\phi| \\theta^{\\text{old}}, \\phi^{\\text{old}}\\right)\\). The log of the joint distribution \\(\\prod_{n=1}^N p(Z_n, Y_n, \\theta, \\phi)\\) is called the complete data log-likelihood (since it is the likelihood of both observed and latent variables), whereas \\(\\log \\prod_{n=1}^N p(Y_n| \\theta, \\phi)\\) is called the observed data log-likelihood (since it is the likelihood of only the observed variable). The auxiliary function presentation of EM is easy to interpret: - In the E-step, you fill in the latent variables in the complete data log-likelihood using “average” values, this leaves just an estimate of the observed log-likelihood. - In the M-step, you find parameters \\(\\phi\\) and \\(\\theta\\) that maximizes your estimate of the observed log-likelihood. We chose to derive EM via the ELBO in this lecture because it makes an explicit connection between the EM algorithm for estimating MLE and variational inference method for approximating the posterior of Bayesian models. It is, however, worthwhile to derive EM using the auxiliary function \\(Q\\), as \\(Q\\) makes it convient for us to prove properties of the EM algorithm. 19.4 Monotonicity and Convergence of EM Before we run off estimating MLE parameters of latent variable models with EM, we need to sanity check two points: (Monotonicity) we need to know that repeating the E, M-steps will never decrease the ELBO! (Convergence) we need to know that at some point the EM algorithm will naturally terminate (the algorithm will cease to update the parameters). (Optional!!!) Proof of Properties of EM We first prove the monotonicity of EM. Consider the difference between \\(\\ell(\\theta, \\phi) - \\ell(\\theta^{\\text{old}}, \\phi^{\\text{old}})\\), i.e. the amount by which the log-likelihood can increase or decrease by going from \\(\\theta^{\\text{old}}, \\phi^{\\text{old}}\\) to \\(\\theta, \\phi\\): \\[\\begin{aligned} \\ell(\\theta, \\phi) - \\ell(\\theta^{\\text{old}}, \\phi^{\\text{old}}) &amp;= \\sum_{n=1}^N\\log \\left[ \\frac{p(y_n|\\theta, \\phi)}{p(y_n| \\theta^{\\text{old}}, \\phi^{\\text{old}})}\\right]\\\\ &amp;= \\sum_{n=1}^N \\log\\int \\frac{p(y_n, z_n|\\theta, \\phi)}{p(y_n| \\theta^{\\text{old}}, \\phi^{\\text{old}})} dz_n\\\\ &amp;= \\sum_{n=1}^N \\log\\int \\frac{p(y_n, z_n|\\theta, \\phi)}{p(y_n| \\theta^{\\text{old}}, \\phi^{\\text{old}}) p(z_n|y_n, \\theta^{\\text{old}}, \\phi^{\\text{old}})}p(z_n|y_n, \\theta^{\\text{old}}, \\phi^{\\text{old}}) dz_n\\\\ &amp;= \\sum_{n=1}^N \\log\\int \\frac{p(y_n, z_n|\\theta, \\phi)}{p(y_n, z_n| \\theta^{\\text{old}}, \\phi^{\\text{old}})}p(z_n|y_n, \\theta^{\\text{old}}, \\phi^{\\text{old}}) dz_n\\\\ &amp;= \\sum_{n=1}^N \\log \\mathbb{E}_{p(z_n|y_n, \\theta^{\\text{old}}, \\phi^{\\text{old}})} \\left[\\frac{p(y_n, z_n|\\theta, \\phi)}{p(y_n, z_n| \\theta^{\\text{old}}, \\phi^{\\text{old}})}\\right]\\\\ &amp;\\geq \\sum_{n=1}^N \\mathbb{E}_{p(z_n|y_n, \\theta^{\\text{old}}, \\phi^{\\text{old}})} \\log\\left[\\frac{p(y_n, z_n|\\theta, \\phi)}{p(y_n, z_n| \\theta^{\\text{old}}, \\phi^{\\text{old}})}\\right]\\\\ &amp;= \\sum_{n=1}^N \\mathbb{E}_{p(z_n|y_n, \\theta^{\\text{old}}, \\phi^{\\text{old}})} \\left[\\log p(y_n, z_n|\\theta, \\phi) - \\log p(y_n, z_n| \\theta^{\\text{old}}, \\phi^{\\text{old}})\\right]\\\\ &amp;= \\sum_{n=1}^N \\mathbb{E}_{p(z_n|y_n, \\theta^{\\text{old}}, \\phi^{\\text{old}})} \\left[\\log p(y_n, z_n|\\theta, \\phi)\\right] - \\sum_{n=1}^N \\mathbb{E}_{p(z_n|y_n, \\theta^{\\text{old}}, \\phi^{\\text{old}})}\\left[ \\log p(y_n, z_n| \\theta^{\\text{old}}, \\phi^{\\text{old}})\\right]\\\\ &amp;= Q\\left(\\theta, \\phi| \\theta^{\\text{old}}, \\phi^{\\text{old}}\\right) - Q\\left(\\theta^{\\text{old}}, \\phi^{\\text{old}}| \\theta^{\\text{old}}, \\phi^{\\text{old}}\\right) \\end{aligned}\\] Thus, when we maximize the gain in log-likelihood going from \\(\\theta^{\\text{old}}, \\phi^{\\text{old}}\\) to \\(\\theta, \\phi\\), we get: \\[\\begin{aligned} \\underset{\\theta, \\phi}{\\max} \\left[\\ell(\\theta, \\phi) - \\ell(\\theta^{\\text{old}}, \\phi^{\\text{old}})\\right] \\geq \\underset{\\theta, \\phi}{\\max} \\left[Q\\left(\\theta, \\phi| \\theta^{\\text{old}}, \\phi^{\\text{old}}\\right) - Q\\left(\\theta^{\\text{old}}, \\phi^{\\text{old}}| \\theta^{\\text{old}}, \\phi^{\\text{old}}\\right)\\right] \\end{aligned}\\] or equivalently, \\[\\begin{aligned} \\underset{\\theta, \\phi}{\\max} \\left[\\ell(\\theta, \\phi)\\right] - \\ell(\\theta^{\\text{old}}, \\phi^{\\text{old}}) \\geq \\underset{\\theta, \\phi}{\\max} \\left[Q\\left(\\theta, \\phi| \\theta^{\\text{old}}, \\phi^{\\text{old}}\\right)\\right] - Q\\left(\\theta^{\\text{old}}, \\phi^{\\text{old}}| \\theta^{\\text{old}}, \\phi^{\\text{old}}\\right). \\end{aligned}\\] Note that the above max is always greater than or equal to zero: \\[\\underset{\\theta, \\phi}{\\max} \\left[Q\\left(\\theta, \\phi| \\theta^{\\text{old}}, \\phi^{\\text{old}}\\right)\\right] - Q\\left(\\theta^{\\text{old}}, \\phi^{\\text{old}}| \\theta^{\\text{old}}, \\phi^{\\text{old}}\\right) \\geq 0\\] since we can always maintain the status quo by choosing \\(theta = \\theta^{\\text{old}}\\) \\(\\phi = \\phi^{\\text{old}}\\): \\[ Q\\left(\\theta^{\\text{old}}, \\phi^{\\text{old}}| \\theta^{\\text{old}}, \\phi^{\\text{old}}\\right) - Q\\left(\\theta^{\\text{old}}, \\phi^{\\text{old}}| \\theta^{\\text{old}}, \\phi^{\\text{old}}\\right) = 0.\\] Thus, we have that by maximizing \\(Q\\left(\\theta, \\phi| \\theta^{\\text{old}}, \\phi^{\\text{old}}\\right)\\), we ensure that \\(\\ell(\\theta, \\phi) - \\ell(\\theta^{\\text{old}}, \\phi^{\\text{old}})\\geq 0\\) in each iteration of EM. If the likelihood of the model is bounded above (i.e. \\(\\ell(\\theta, \\phi) \\leq M\\) for some constant \\(M\\)), then EM is guaranteed to convergence. This is because we’ve proved that EM increases (or maintains) log-likelihood in each iteration, therefore, if \\(\\ell(\\theta, \\phi)\\) is bounded, the process must converge. Disclaimer: Although EM converges for bounded likelihoods, it is not guaranteed to converge to the global max of the log-likelihood! By formalizing an analogy between EM and gradient descent, you can show that EM converges to a stationary point of the likelihood. Often times, EM converges only to local optima of the likelihood function and the point to which it converges may be very sensitive to initialization. "],["review-of-latent-variables-compression-and-clustering.html", "Chapter 20 Review of Latent Variables, Compression and Clustering 20.1 PCA Versus Probabilistic PCA (pPCA) 20.2 Non-Probabilistic Clustering Versus Probabilistic Clustering", " Chapter 20 Review of Latent Variables, Compression and Clustering 20.0.1 Example: Gaussian Mixture Models (GMMs) In a Gaussian Mixture Model (GMM), we posit that the observed data \\(Y\\) is generated by a mixture, \\(\\pi=[\\pi_1, \\ldots, \\pi_K]\\), of \\(K\\) number of Gaussians with means \\(\\mu = [\\mu_1, \\ldots, \\mu_K]\\) and covariances \\(\\Sigma = [\\Sigma_1, \\ldots, \\Sigma_K]\\). For each observation \\(Y_n\\) the class of the observation \\(Z_n\\) is a latent variable that indicates which of the \\(K\\) Gaussian is responsible for generating \\(Y_n\\): \\[\\begin{aligned} Z_n &amp;\\sim Cat(\\pi),\\\\ Y_n | Z_n&amp;\\sim \\mathcal{N}(\\mu_{Z_n}, \\Sigma_{Z_n}), \\end{aligned}\\] where \\(n=1, \\ldots, N\\) and \\(\\sum_{k=1}^K \\pi_k = 1\\). GMMs are examples of model based clustering - breaking up a data set into natural clusters based on a statistical model fitted to the data. 20.0.2 Example: Item-Response Models In item-response models, we measure an real-valued unobserved trait \\(Z\\) of a subject by performing a series of experiments with binary observable outcomes, \\(Y\\): \\[\\begin{aligned} Z_n &amp;\\sim \\mathcal{N}(\\mu, \\sigma^2),\\\\ \\theta_n &amp;= g(Z_n)\\\\ Y_n|Z_n &amp;\\sim Ber(\\theta_n), \\end{aligned}\\] where \\(n=1, \\ldots, N\\) and \\(g\\) is some fixed function of \\(Z_n\\). 20.0.2.1 Applications Item response models are used to model the way “underlying intelligence” \\(Z\\) relates to scores \\(Y\\) on IQ tests. Item response models can also be used to model the way “suicidality” \\(Z\\) relates to answers on mental health surveys. Building a good model may help to infer when a patient is at psychiatric risk based on in-take surveys at points of care through out the health-care system. 20.0.3 Example: Factor Analysis Models In factor analysis models, we posit that the observed data \\(Y\\) with many measurements is generated by a small set of unobserved factors \\(Z\\): \\[\\begin{aligned} Z_n &amp;\\sim \\mathcal{N}(0, I),\\\\ Y_n|Z_n &amp;\\sim \\mathcal{N}(\\mu + \\Lambda Z_n, \\Phi), \\end{aligned}\\] where \\(n=1, \\ldots, N\\), \\(Z_n\\in \\mathbb{R}^{D&#39;}\\) and \\(Y_n\\in \\mathbb{R}^{D}\\). We typically assume that \\(D&#39;\\) is much smaller than \\(D\\). 20.0.3.1 Applications Factor analysis models are useful for biomedical data, where we typically measure a large number of characteristics of a patient (e.g. blood pressure, heart rate, etc), but these characteristics are all generated by a small list of health factors (e.g. diabetes, cancer, hypertension etc). Building a good model means we may be able to infer the list of health factors of a patient from their observed measurements. 20.1 PCA Versus Probabilistic PCA (pPCA) Suppose that our data \\(\\mathbf{X}\\) is centered (if our data is not centered then reasoning about PCA as a projection that minimizes reconstruction loss uses uglier formulae). Let’s also assume that \\(z_n\\) is one dimensional and \\(\\mathbf{x}_n\\) is two dimensional. PCA pPCA Model \\(z_n = v^\\top \\mathbf{x}_n \\in \\mathbb{R}\\) (Projection), \\(\\hat{\\mathbf{x}}_n = vz_n \\in \\mathbb{R}^2\\) (Reconstrcution) \\(z_n\\sim \\mathcal{N}(0, 1)\\), \\(\\mathbf{x}_n\\| z_n \\sim \\mathcal{N}(vz_n, \\sigma^2)\\) Goal Minimize Reconstruction Loss Maximize Observed Log-likelihood Training Objective \\(\\mathrm{min}_v \\sum_{n=1}^N\\\\| \\mathbf{x}_n - vz_n\\\\|^2_2\\), \\(\\\\|v\\\\|_2 =1\\) \\(\\mathrm{max}_{v, q({z}_n)}\\; \\sum_{n=1}^N\\mathrm{ELBO}(v, q({z}_n))\\) (Optional: \\(\\\\|v\\\\|_2=1\\)) Good For Compression: \\(\\mathbf{x}_n \\mapsto z_n\\) Modeling data distribution: \\(\\mathbf{x}_n \\sim p(\\mathbf{x}_n\\| v)\\) Generating New Data: \\(z^\\mathrm{new}_n\\sim \\mathcal{N}(0, 1)\\), \\(\\mathbf{x}^\\mathrm{new}_n\\| z^\\mathrm{new}_n \\sim \\mathcal{N}(vz^\\mathrm{new}_n, \\sigma^2)\\) How to “Compress” Compute \\(z_n = v^\\top \\mathbf{x}_n\\) Infer \\(p(z_n \\| \\mathbf{x}_n, v)\\) Training Algorithm SVD (find eigenvalues) Expectation Maximization Deep Model Analogue Autoencoder Variational Autoencoder 20.1.1 What to Know About Expectation Maximization 20.1.1.1 Motivation: Why Can’t We Autodiff Log-likelihood and Gradient Descent? Can’t directly maximize conditional likelihood: \\[\\max_v \\sum_{n=1}^N\\log p(\\mathbf{x}_n | z_n, v),\\] because \\(v^*\\) will be in terms of unobserved variables \\(z_n\\). This makes our \\(v^*\\) useless. Can’t directly maximize observed data likelihood: \\[\\max_v \\sum_{n=1}^N\\log p(\\mathbf{x}_n | v) = \\max_v \\sum_{n=1}^N\\log \\mathbb{E}_{p(z_n)} [p(\\mathbf{x}_n | z_n, v)],\\] because we either: can’t compute \\(\\mathbb{E}_{p(z_n)} [p(\\mathbf{x}_n | z_n, v)]\\) in closed form (the integral is too hard), and/or we cannot push the gradient inside the expectation or alternatively we can compute compute \\(\\mathbb{E}_{p(z_n)} [p(\\mathbf{x}_n | z_n, v)]\\) in closed form (the integral is easy), but gradient descent on \\(\\log p(\\mathbf{x}_n | v)\\) struggles to converge 20.1.1.2 Our Solution We optimize a lower bound for the observed data loglikelihood, that is also easier for us to work with: \\[ \\sum_{n=1}^{N}\\mathrm{ELBO}(v, q(\\mathbf{z}_n)) = \\sum_{n=1}^{N} \\mathbb{E}_{q(z_n)}\\left[ \\log \\frac{p(\\mathbf{x}_n|z_n, v)p(z_n)}{q(z_n)}\\right] \\] The ELBO is a more tractable obejctive to work with because: the expectation is with respect to a distribution we choose, \\(q(z_n)\\), and thus we can choose \\(q(z_n)\\) to not depend on \\(v\\). So the gradient \\(\\nabla_v\\) can be pushed inside the expectation. we can optimize the ELBO via coordinate-wise optimization, taking turns optimizing over each \\(q\\) and \\(v\\), while holding the other constant: (E-Step) when optimizing the ELBO with respect to \\(q\\), a lot of algebra shows us that we just need to set \\(q(z_n) = p(z_n| \\mathbf{x}_n, v^\\mathrm{old})\\). Note: this is “easy” only if we can find the posterior \\(p(z_n| \\mathbf{x}_n, v^\\mathrm{old})\\) in closed form. When we don’t know the posterior in closed form, in the E-step we typically perform variational inference. (M-Step) when optimizing the ELBO with respect to \\(v\\), a lot of algebra shows us that the ELBO greatly simplifies: \\[ v^\\mathrm{new}=\\mathrm{argmax}_v \\mathbb{E}_{p(z_n| \\mathbf{x}_n, v^\\mathrm{old})} \\left[ \\log p(\\mathbf{x}_n|z_n, v) + \\log p(z_n)\\right] \\] Note: this optimization is typically done analytically but you can also optimize in the M-step using gradient descent. 20.2 Non-Probabilistic Clustering Versus Probabilistic Clustering Non-Probabilistic Probabilistic (Mixture Models) Model Uses some notion of similarity to make clusters \\(z_n\\sim \\mathrm{Cat}(\\mathbf{\\pi})\\), \\(\\mathbf{x}_n\\| z_n \\sim \\mathcal{N}(\\mathbf{\\mu}_{z_n}, \\Sigma_{z_n})\\) Goal Maximize cluster “quality” Maximize Observed Log-likelihood Training Objective Minimize some loss \\(\\mathrm{max}_{v, q({z}_n)}\\; \\sum_{n=1}^N\\mathrm{ELBO}(\\pi, \\mu_{z_n}, \\Sigma_{z_n}, q({z}_n))\\), with \\(\\\\|\\pi\\\\|_1=1\\) Good for Producing hard cluster assignments Producing soft cluster assignments (uncertainty) Generating new data: (1) sample \\(z_n = k\\), (2) sample \\(\\mathbf{x}_n\\) from the \\(k\\)-th Gaussian Training Algorithm Various Expectation Maximization "],["topic-models.html", "Chapter 21 Topic Models 21.1 Our First Latent Variable Model 21.2 Reasoning About Text Corpa Using Topic Modeling 21.3 Our Second Latent Variable Model: pLSA 21.4 Our Third Latent Variable Model: LDA", " Chapter 21 Topic Models 21.1 Our First Latent Variable Model So far all of our latent variable models are different instantiations of the same simple graphical model: In probabilistic PCA (pPCA), we assumed that \\(\\mathbf{z}_n\\) is a continuous variable that is lower dimensional than \\(\\mathbf{y}_n\\), and that \\(\\mathbf{y}_n\\) is a (Gaussian) noisy observation of a linear transformation of \\(\\mathbf{z}_n\\). - we want to learn \\(p(\\mathbf{z}_n|\\mathbf{y}_n)\\): because \\(\\mathbf{z}_n\\) represents the low-dimensional “compression” of \\(\\mathbf{y}_n\\) - we want to learn the parameters \\(\\theta\\) and \\(\\phi\\): learning the parameters of the model helps us generate new data In Gaussian Mixture Models (GMMs), we assumed that \\(\\mathbf{z}_n\\) is a categorical (index) variable, and that \\(\\mathbf{y}_n\\) an observation from a Gaussian whose mean and covariance are indexed by \\(\\mathbf{z}_n\\). - we want to learn \\(p(\\mathbf{z}_n|\\mathbf{y}_n)\\): because \\(\\mathbf{z}_n\\) represents our guess of which to which cluster does \\(\\mathbf{y}_n\\) belong - we want to learn the mixture means \\(\\mu\\): because \\(\\mu_k\\) is a natural example of a “typical” point in the \\(k\\)-th cluster - we want to learn the mixture covariances \\(\\Sigma\\): because \\(\\Sigma_k\\) gives us the correlation between samples in the \\(k\\)-th cluster - we want to learn the mixture coefficients \\(\\pi\\): because \\(\\pi\\) gives us an estimate of the relative proportions of the clusters In Item-Response Analysis, we assumed that \\(\\mathbf{z}_n\\) is a continuous variable and that \\(\\mathbf{y}_n\\) is a binary observation whose probability of positive outcome is a function \\(g\\) of \\(\\mathbf{z}_n\\). - we want to learn \\(p(\\mathbf{z}_n|\\mathbf{y}_n)\\): because \\(\\mathbf{z}_n\\) represents our guess of the latent characteristic (e.g. stress level) we are trying to measure in our human subject - we want to learn the parameters of \\(g\\): because we can use \\(g\\) to generate more data (e.g. we can ask, if a subject has this \\(\\mathbf{z}_n\\) value, what would their response \\(\\mathbf{y}_n\\) be on our test?). - we want to learn the mean of prior \\(p(\\mathbf{z}_n)\\): because the mean represents the population average (e.g. average stress level amongst students) - we want to learn the covariance of prior \\(p(\\mathbf{z}_n)\\): because the covariace represents the variations amongst our population (e.g. how stress levels varies in the population) 21.2 Reasoning About Text Corpa Using Topic Modeling When I am given a set of numeric data (even a set of images represented by pixel values), I can easily separate out the data into “clusters” and then interpret each “cluster mean” as a typical example for that cluster. But what if the data is text-based? How can we cluster text documents and how can we interpret what each cluster is about? Clustering text documents usually involves the following components: 1. turning a text document into a vector of numbers \\(\\mathbf{d}\\) - Bag of Words: we take all the unique words in our vocabulary (say of size \\(D\\)), then create a maping between words and indices. For each document, we will simply count how many of each word appears in the document, and organize these counts into a vector \\(D\\) dimensional vector. we define the objects of interest during modeling: Topics: we want to discover a set of \\(K\\) themes, called topics, from the documents. We represent each topic as a distribution over words in the vocabulary. That is, each topic is a \\(D\\) dimensional vector. Topics per Document: we want to know what each document is talking about, in terms of the topics. We represent this information as a distribution over the \\(K\\) topics, i.e. this is a \\(K\\) dimensional vector. A typical model for Topic Modeling will involve: a way of generating the observed data (e.g. bag of words representation of documents) as some combination of \\(K\\) topics and a mixture of topics for each document. a way of learning the \\(K\\) topics (as a distribution over \\(D\\) vocabulary) and a way of learning the topic mixture for each document (as a distribution \\(K\\) topics). Different topic models will accomplish the above two tasks differently. 21.3 Our Second Latent Variable Model: pLSA It turns out that we can recast Topic Modeling as a latent variable model! This latent variable model will finally be more complex than our first model! Correspondingly, EM steps for this latent variable model will be more involved to derive (don’t worry, we’re not doing it in this class!). Our main jobs are: 1. to be able to look at these graphical models and then be able to articulate the how does this model explain the structure in the observed data?. 2. interpret the parameters and latent variables \\(\\mathbf{z}\\) that we learn from EM. Probabilistic Latent Semantic Analysis (pLSA) The structure behind the observed data: - start with \\(J\\) number of documents in my corpus; for each document, there is a mixture of topics; for each topic, there is a distribution over words - for the \\(j\\)-th document, the \\(I\\) number of words is generated as follows: - sample a single topic \\(\\mathbf{z}_{ij}\\) from the mixture of topics for the \\(j\\)-th document - sample a single word from the distribution over words representing this topic Inference (EM): - the observed data log-likelihood is (omitting indices): \\[p(d, w) = \\sum_{z} p(w|z)p(z|d)p(d)\\] or alternatively \\[p(w|d) = \\sum_{z} p(w|z)p(z|d)\\] - we want to learn the mixture of topics for each document, i.e. \\(p(z|d)\\) - we want to learn the distribution over words defined by each topic, i.e. \\(p(w|z)\\) Doing EM to infer this information is equivalent to a form of Nonnegative Matrix Factorization (NMF). 21.4 Our Third Latent Variable Model: LDA In the pre-class prep materials, we introduced another topic model, Latent Dirichlet Allocation. So why do we need yet another topic model (especially one that is so complicated)??? This is because when we learn the mixture of topics for each document, \\(p(z|d)\\), and the distribution over words defined by each topic, \\(p(w|z)\\) by MLE, we can often overfit! For example, even if your corpus is large, for many topics, the number of “examples” may still be small and thus our model may overfit. Rather than regularizing our MLE, we can put priors on both the mixture of topics per document and on the distribution over words defined by each topic. Through these priors, we can express lots of interesting beliefs we have about how the topics should look and how each document should look. Latent Dirichlet Allocation Again, in the above: - \\(w\\) represents single words - \\(z\\) represents single topics Now, instead of supposing that each document has a fixed but unknown mixture of topics, \\(p(z|d)\\), we suppose that each document was randomly assigned a mixture of topics (this is our prior on \\(p(z|d)\\)): - \\(\\theta\\) represents the per document topic mixture Since \\(\\theta\\) is a mixture (the mixture components must sum to 1), it make sense to model \\(\\theta\\) as a Dirichlet random variable with hyperparameters \\(\\alpha\\). Instead of supposing that each topic has a fixed but unknown distribution over words, \\(p(w|z)\\), we suppose that each topic was randomly assigned a distribution over words (this is our prior on \\(p(w|z)\\)): - \\(\\beta\\) represents the per topic word distribution Since \\(\\theta\\) is a distribution over words (the distribution must sum to 1), it make sense to model \\(\\theta\\) as a Dirichlet random variable with hyperparameters \\(\\eta\\). The Generative Story for LDA For the \\(k\\)-th topic: - we sample a distribution over words \\(\\beta_k\\) For the \\(j\\)-th document: - we sample a distribution over topics \\(\\alpha_j\\) - For the \\(i\\)-th word in the \\(j\\)-document: - we sample a single topic \\(z_{ij}\\) from the topic mixture - we sample a single word from the word distribution of the topic \\(z_{ij}\\) Inference (Variational EM): - we learn the hyper-parameters \\(\\alpha\\), \\(\\eta\\) by maximizing the ELBO - unfortunately, in the E-step, the posterior over the latent variables cannot be analytically derived. In this case, we approximate the posterior using variational inference. "],["math-and-intuition-of-hidden-markov-models.html", "Chapter 22 Math and Intuition of Hidden Markov Models 22.1 Markov Models 22.2 Hidden Markov Models 22.3 Learning and Inference for HMMs", " Chapter 22 Math and Intuition of Hidden Markov Models 22.1 Markov Models A discrete time stochastic process is a sequence of random variables \\(\\{Z_n\\}_{n\\geq 0}\\) that takes values in the space \\(\\mathcal{S}\\), this is called the state space. The state space can be continuous or discrete, finite or infinite. A stochastic process is a discrete time Markov model, a Markov chain or a Markov Process, if the process is given by: \\[ \\begin{align} Z_1 &amp;\\sim p(Z_1)\\\\ Z_{n+1} | Z_{n} &amp;\\sim p(Z_{n+1} | Z_{n}) \\end{align} \\] where \\(p(Z_{n+1} | Z_{n})\\) is the probability density associated with transitioning from one time step to the next and \\(p(Z_1)\\) is the probability density associated with the initial time step. A Markov model satisfies the Markov property if \\(Z_n\\) depends only on \\(Z_{n-1}\\) (i.e. \\(Z_n\\) is independent of \\(Z_1, \\ldots, Z_{n-2}\\) conditioned on \\(Z_{n-1}\\)). We will assume that \\(p(X_n | X_{n-1})\\) is the same for all \\(n\\), in this case, we call this Markov chain stationary. 22.1.1 Transition Matrices and Kernels The Markov property ensure that we can describe the dynamics of the entire chain by describing how the chain transitions from state \\(i\\) to state \\(j\\). Why? If the state space is finite, then we can represent the transition from \\(Z_{n-1}\\) to \\(Z_{n}\\) as a transition matrix \\(T\\), where \\(T_{ij}\\) is the probability of the chain transitioning from state \\(i\\) to \\(j\\): \\[ T_{ij} = \\mathbb{P}[Z_n = j | Z_{n-1} = i]. \\] The transition matrix can be represented visually as a finite state diagram. If the state space is continuous, then we can represent the transition from \\(Z_{n-1}\\) to \\(Z_{n}\\) as transition kernel pdf, \\(T(z, z&#39;)\\), representing the likelihood of transitioning from state \\(Z_{n-1}=z\\) to state \\(Z_{n} = z&#39;\\). The probability of transitioning into a region \\(A \\subset \\mathcal{S}\\) from state \\(z\\) is given by \\[ \\mathbb{P}[Z_n \\in A | Z_{n-1} = z] = \\int_{A} T(z, z&#39;) dz&#39;, \\] such that \\(\\int_{\\mathcal{S}} T(z, z&#39;) dz&#39; = 1\\). 22.1.2 Applications of Markov Models Markov models can be used more generally to model any data of a sequential nature – data where the ordering of the observation contains information. Sequential data appears in many domains: 1. biometric readings of patients in a hospital over their stay 2. gps location of autonomous machines over time 3. economic/financial indicators of a system over time 4. the samples obtained by a sampler over iterations Learning how Markov model evolves over time, the dynamics, can lend insights on the mechanics of real-life systems that the model represent. Exercise: Give an example of a stochastic process that is not a Markov chain. Given an example of a stochastic process that is a Markov chain. 22.1.2.1 Example: Smart Phone Market Model Consider a simple model of the dynamics of the smart phone market, where we model the customer loyalty as follows: The transition matrix for the Markov chain is: \\[ T = \\left(\\begin{array}{cc}0.8 &amp; 0.2\\\\ 0.4 &amp; 0.6 \\end{array} \\right) \\] In the above, the state space is \\(S=\\{ \\text{Apple}, \\text{Others}\\}\\). Say that the market is initially \\(\\pi^{(0)} = [0.7\\; 0.3]\\), i.e. 70% Apple. What is the market distribution in the long term? (OPTIONAL) Chapman-Kolmogorov Equations: Dynamics as Matrix Multiplication If the state space is finite, the probability of the \\(n=2\\) state, given the initial \\(n=0\\) state. can be computed by the Chapman-Kolmogorov equation: \\[ \\mathbb{P}[Z_2 = j | Z_{0} = i] = \\sum_{k\\in \\mathcal{S}} \\mathbb{P}[Z_1 = k | Z_{0} = i]\\mathbb{P}[Z_{2} = j|Z_{1}=k] = \\sum_{k\\in \\mathcal{S}}T_{ik}T_{kj} \\] We recognize \\(\\sum_{k\\in \\mathcal{S}}T_{ik}T_{kj}\\) as the \\(ij\\)-the entry in the matrix \\(TT\\). Thus, the Chapman-Kolmogorov equation gives us that the matrix \\(T^{(n)}\\) for a \\(n\\)-step transition is \\[ T^{(n)} = \\underbrace{T\\ldots T}_{\\text{$n$ times}} \\] In particular, when we have the initial distribution \\(\\pi^{(0)}\\) over states, then the unconditional distribution \\(\\pi^{(1)}\\) over the next state is given by: \\[ \\mathbb{P}[Z_1 = i] = \\sum_{k\\in \\mathcal{S}} \\mathbb{P}[Z_1 = i | Z_0 = k]\\mathbb{P}[Z_0 = k] \\] That is, \\(\\pi^{(1)} = \\pi^{(0)} T\\). If the state space is continuous, the likelihood of the \\(n=2\\) state, given the initial \\(n=0\\) state, can be computed by the Chapman-Kolmogorov equation: \\[ T^{(2)}(z, z&#39;) = \\int_{\\mathcal{S}}T(z, y)\\, T(y, z&#39;) dy. \\] In particular, when we have the initial distribution \\(\\pi^{(0)}(z)\\) over states, then the unconditional distribution \\(\\pi^{(1)}(z)\\) over the next state is given by: \\[ \\pi^{(1)}(z) = \\int_{\\mathcal{S}} T(y, z)\\,\\pi^{(0)}(y)dy. \\] 22.2 Hidden Markov Models The problem with types of sequential data observed from real-life dynamic system is that the observation is usually noisy. What we observe is not the true state of the system (e.g. the true state of the patient or the true location of the robot), but some signal that is corrupted by environmental noise. In a hidden Markov model (HMM), we assume that we do not have access to the values in the underlying Markov process \\[ Z_{n+1} | Z_{n} \\sim p(Z_{n+1} | Z_{n}) \\quad \\mathbf{(state\\; model)} \\] and, instead, observe the process \\[ Y_n | Z_n \\sim p(Y_n | Z_n) \\quad \\mathbf{(observation\\; model)} \\] where \\(p(Y_n | Z_n)\\) is the probability density associated with observing \\(Y_n \\in \\mathcal{Y}\\) given the latent value, \\(Z_n\\), of the underlying Markov process at time \\(n\\). If the state space is continuous, Hidden Markov Models are often called State-Space Models. We will observe the following notational conventions: A random variable in the HMM at time index \\(n\\) is denoted \\(Z_n\\) or \\(Y_n\\). A collection of random variables from time index \\(n\\) to time index \\(n + k\\) is denoted \\(Z_{n:n+k}\\) or \\(Y_{n:n+k}\\). The value of the random variable at time index \\(n\\) is denoted \\(z_n\\) or \\(y_n\\). 22.2.0.1 Example: Discrete Space Models Let our state and observation spaces be DNA nucleotides: A, C, G, T. The transitions and observations are then defined by \\(4\\times 4\\) matrices. The state model transition matrix tells us how likely a nucleotide is to be observed given that the previous nucleotide is an A, C, G or T. Since we know that in cell division, DNA can be replicated with “typos”. Thus, the observation model can capture the probability that a given nucleotide will be mistranscribed as another nucleotide. 22.2.0.2 Example: Linear Gaussian Models Let our state and observation spaces be Euclidean, \\(\\mathcal{Z} = \\mathbb{R}^M\\) and \\(\\mathcal{Y} = \\mathbb{R}^{M&#39;}\\). The transitions and observations in a linear Gaussian model are defined by linear transformations of Gaussian variables with the addition of Gaussian noise: \\[\\begin{align} &amp;Z_0 \\sim \\mathcal{N}(0, \\Sigma)\\\\ &amp;Z_{n+1} = AZ_n + B + C\\xi \\quad \\mathbf{(State\\;Model)}\\\\ &amp;Y_{n+1} = DZ_{n+1} + E + F\\epsilon \\quad \\mathbf{(Observation\\;Model)} \\end{align}\\] where \\(\\xi \\sim \\mathcal{N}(0, \\mathbf{I}_M)\\), \\(\\epsilon \\sim \\mathcal{N}(0, \\mathbf{I}_{M&#39;})\\). Thus, the transitions and observations probability densities are: \\[\\begin{align} Z_{n+1}|Z_n &amp;\\sim \\mathcal{N}(AZ_n + B, CC^\\top)\\\\ Y_{n+1} | Z_{n+1} &amp;\\sim \\mathcal{N}(DZ_{n+1} + E, FF^\\top) \\end{align}\\] Linear Gaussian Models are widely used in target tracking and signal processing, since inference for these models are tractible (we only need to manipulate Gaussians). 22.3 Learning and Inference for HMMs There are a number of inference problems associated with HMMs: Learning - learning the dynamics of the state or observation model Example Algorithms: Baum-Welch (i.e. Expectation Maximization) Inference - estimating the probability distribution over one or more of the latent variables, \\(\\{Z_n\\}_{n\\geq 1}\\), given a sequence of observations, \\(\\{Y_n\\}_{n\\geq 1}\\): Filtering: computing \\(p(Z_n| Y_{1:n})\\). Example Algorithms: Kalman Filters, Sequential Monte Carlo Smoothing: computing \\(p(Z_t| Y_{1:n})\\) where \\(t &lt; n\\). Example Algorithms: Rauch-Tung-Striebel (RTS) Most probable explanation computing the joint distribution of all latent variables given all observations, \\(p(Z_{1:n}| Y_{1:n})\\). Or, alternatively, compute the most likely sequence of latent variable values given all observations: \\[ z^*_{1:n} = \\underset{z_{1:n}}{\\mathrm{argmax}}\\; p(z_{1:n} | y_{1:n}) \\] Example Algorithms: Viterbi, Forward-Backward (OPTIONAL) Learning for HMMs Given an HMM model, \\[\\begin{aligned} Z_{n+1} | Z_{n} &amp;\\sim p(Z_{n+1} | Z_{n}) \\quad \\mathbf{(state\\; model)}\\\\ Y_n | Z_n &amp;\\sim p(Y_n | Z_n) \\quad \\mathbf{(observation\\; model)} \\end{aligned}\\] Suppose that \\(p(Z_{n+1} | Z_{n}) = f_\\theta(_{n}, \\epsilon)\\) and \\(p(Y_n | Z_n) = g_\\phi(Z_{n}, \\xi)\\) where \\(f\\), \\(g\\) are functions with parameters \\(\\theta, \\phi\\) and \\(\\epsilon\\) and \\(\\xi\\) are noise variables. The learning task for HMMs is to learn values for \\(\\theta, \\phi\\). We do this by maximizing the observed data log-likelihood: \\[ \\theta_{\\text{MLE}}, \\phi_{\\text{MLE}} = \\mathrm{argmax}_{\\theta, \\phi}\\log p(Y_{1:n}; \\theta, \\phi) \\] Typically, we maximize the observed data log-likelihood indirectly by maximizing a lower bound, the ELBO, via expectation maximization. For linear Gaussian models and discrete state and observation spaces, both the E-step and the M-step have analytical solutions. (E-step) set \\(q(Z_{1:n}) = p(Z_{1:n} | Y_{1:n}; \\theta^*, \\phi^*)\\), where \\(\\theta^*, \\phi^*\\) is from the previous M-step and the posterior \\(p(Z_{1:n} | Y_{1:n}; \\theta^*, \\phi^*)\\) is computed from distributions obtained by smoothing and filtering. (M-step) maximize the ELBO with respect to \\(\\theta\\) and \\(\\phi\\), using the \\(q\\) obtained from the E-step. Since all the distributions are Gaussians, this can be done analytically. See full derivations for linear Gaussian models: 1. Notes on Linear Gaussian State Space Model 2. Parameter Estimation for Linear Dynamical Systems (OPTIONAL) Filtering for HMMs: Infering \\(p(Z_n| Y_{1:n})\\) We make the following simplifying assumptions: we know the model parameters, i.e. we know the parameters in the distribution \\(p(Z_0)\\), \\(p(Z_{n+1} | Z_{n})\\) and \\(p(Y_n | Z_n)\\). the underlying Markov process is homogeneous, that is, the transition probability density function is stationary. That is, for all \\(n\\geq 0\\), we have \\[ p(Y_n | Z_n) = p(Y_{n+1} | Z_{n+1}). \\] given \\(\\{Z_n\\}_{n\\geq 0}\\), the observations \\(\\{Y_n\\}_{n\\geq 0}\\) are independent. By Baye’s Rule, the posterior marginal distribution \\(p(Z_n| Y_{1:n})\\) is given by \\[ p(_n| Y_{1:n}) = \\frac{p(Y_n | Z_n) p(Z_n | Y_{1:n-1})}{p(Y_n | Y_{1:n-1})}. \\] Note that in the above \\(p(Y_n | Z_n)\\) is known; it is just the likelihood (distribution of the observed value given the latent state). Given \\(p(Z_{n-1} | Y_{1:n-1})\\), we can compute the unknown term \\(p(Z_n | Y_{1:n-1})\\) in the numerator: \\[ p(Z_n | Y_{1:n-1}) = \\int p(Z_n | Z_{n-1}) p(Z_{n-1} | Y_{1:n-1}) dZ_{n-1}. \\] Given \\(p(Z_{n-1} | Y_{1:n-1})\\), we can also compute the denominator \\(p(Y_n | Y_{1:n-1})\\): \\[ p(Y_n | Y_{1:n-1}) = \\int p(Z_{n-1} | Y_{1:n-1}) p(Z_n | Z_{n-1}) p(Y_n | Z_n) dZ_{n-1:n}. \\] Note that \\(p(Z_n | Z_{n-1})\\) is known, it is the transition between latent states. #### Inducitive Algorithm for Inference Notice that given \\(p(Z_{n-1} | Y_{1:n-1})\\), we can compute \\(p(Z_n| Y_{1:n}) = \\frac{p(Y_n | Z_n) p(Z_n | Y_{1:n-1})}{p(Y_n | Y_{1:n-1})}\\). This leads to the following inducitive algorithm for infering \\(p(Z_n| Y_{1:n})\\): (Inductive Hypothesis) suppose we have \\(p(Z_{n-1} | Y_{1:n-1})\\) (Prediction step) compute \\(p(Z_n | Y_{1:n-1}) = \\int p(Z_n|Z_{n-1})p(Z_{n-1}|Y_{1:n-1})dZ_{n-1}\\) (Update step) compute \\(p(Z_n| Y_{1:n}) =\\frac{p(Y_n|Z_n)p(Z_n|Y_{1:n-1})}{p(Y_n | Y_{1:n-1})}\\), where \\[p(Y_n | Y_{1:n-1}) = \\int p(Z_{n-1} | Y_{1:n-1}) p(X_n | Z_{n-1}) p(Y_n | Z_n) dZ_{n-1:n}.\\] The problem: save for in very few cases, the integrals we need to compute in both the prediction and update steps are intractable! In the special case of linear Gaussian models, the distributions in the prediction and update steps are Gaussian and can be computed in closed form, the resulting iterative filtering algorithm is known as the Kalman Filter. (OPTIONAL) Smoothing for HMMs Recursive Algorithm for Computing \\(p(Z_{t} | Y_{1:n})\\) If we assume a linear Gaussian model, we can recursively compute \\(p(Z_{t} | Y_{1:n})\\). Compute \\(p(Z_{n} | Y_{1:n}) = \\mathcal{N}(\\hat{z}_{n|n}, \\hat{\\sigma}^2_{n|n})\\) for each \\(n\\) using a Kalman filter. Compute \\(p(Z_{n + 1} | Y_{1:n}) = \\mathcal{N}(\\hat{z}_{n+1|n}, \\hat{\\sigma}^2_{n+1|n})\\) for each \\(n\\) using a Kalman filter. (Induction Hypothesis) Suppose that \\(p(_{t+1} | Y_{1:n}) = \\mathcal{N}(\\hat{xz}_{t+1|n} \\hat{\\sigma}^2_{t+1|n})\\). (Update) using the induction hypothesis, we first compute the conditional \\(p(Z_{t} | Z_{t+1}, Y_{1:n}) = \\mathcal{N}(m, s^2)\\), then we integrate out \\(Z_{t+1}\\) to get: \\[ p(Z_{t} | Y_{1:n}) = \\mathcal{N}(\\hat{z}_{t|n} \\hat{\\sigma}^2_{t|n}) \\] This suggests a recursive algorithm. "],["the-intuition-of-markov-decision-processes.html", "Chapter 23 The Intuition of Markov Decision Processes 23.1 Review: Modeling Sequential Data 23.2 Modeling Sequential Data and Sequential Actions 23.3 Modeling Sequential Decisions: Planning", " Chapter 23 The Intuition of Markov Decision Processes 23.1 Review: Modeling Sequential Data Up to this point, we’ve always assumed that our data points are independently generated. For example, in pPCA, we have \\(N\\) identically and independently distributed latent variables, each generating an observed variable. This assumption of independence of \\(Z_i\\) and \\(Z_j\\) allow us to factorize the joint complete data likelihood as: \\[ p(Z_1, \\ldots, Z_n, Y_1, \\ldots, Y_N|\\theta) = \\prod_{n=1}^N p(Y_n, Z_n|\\theta) = \\prod_{n=1}^N p(Y_n | Z_n, \\theta)p(Z_n) \\] In Markov Models (i.e. Markov Chains, Markov Processes)and Hidden Markov Models, we assume that the order in which the data is generated matters. That is, the data is a sequence and not a set. In particular, we make the Markov assumption, that each variable depends only on it’s parent in the graphical model. For example, in HMM, the Markov assumption means that \\[p(Z_n|Z_{n-1}, \\ldots, Z_1, T) = p(Z_n|Z_{n-1}, T),\\] i.e. knowing \\(Z_{n-1}\\) is sufficient to infer everything about \\(Z_n\\). Similarly, the Markov assumption gives us \\[p(Y_n|Z_n, O) = p(Y_n|Z_n, Z_{n-1}, \\ldots, Z_1, T, O),\\] i.e. knowing \\(Z_n\\) is sufficient to infer everything about \\(Y_n\\). The Markov assumption of dependence allow us to factorize the joint complete data likelihood as: \\[\\begin{aligned} p(Z_1, \\ldots, Z_n, Y_1, \\ldots, Y_N|O, T) &amp;= p(Y_1, Z_1|O)\\prod_{n=2}^N p(Y_n, Z_n|Z_{n-1}, O, T)\\\\ &amp;= p(Y_1| Z_1, O)p(Z_1)\\prod_{n=2}^N p(Y_n| Z_n, O) p(Z_n|Z_{n-1}, T) \\end{aligned}\\] Check Your Understanding: Compare the factorization of the joint complete data likelihood of HMM to the factorization of the joint complete data likelihood of pPCA above. What is the difference between these two factorizations? Where do you see the Markov assumption coming into play? 23.1.1 Why Model Sequential Data (Dynamics)? What is the point of modeling data as a sequence rather than a set? Often, you’ll hear the argument that data \\(Y_t\\) observed over time \\(t\\) (i.e. time series) naturally follow a sequential order and this ordering by time is important. Thus, we must model the dynamics of how this data changes over time: \\[\\begin{aligned} Z_{n+1} &amp;= f_v(Z_{n}, \\epsilon),\\; \\epsilon \\sim p(\\epsilon)\\quad \\textbf{(State Model)}\\\\ Y_{n+1} &amp;= g_w(Z_{n+1}, \\eta),\\; \\eta \\sim p(\\eta) \\quad \\textbf{(Observation Model)} \\end{aligned}\\] But why not directly model the relationship between time \\(n\\) and the observation \\(Y_n\\)? That is, why not learn a function to directly predict \\(Y_n\\) given \\(n\\)? \\[ Y_n = h_u(n) + \\xi,\\; \\xi \\sim p(\\xi) \\] In the following example, the observed data is generated from a Linear Gaussian Model (a type of HMM with continuous state space and linear state and observation models with additive Gaussian noise). The observed data (grey dots) are noisy measurements of the latent data (red trend line). In blue, we visualize the latent data we inferred from the HMM; in yellow, we visualize a regression model fitted directly on the observed data. We see that the regression model captures the general trend in the data – using this model to predict observations into the future would does not seem like a terrible idea. Certainly fitting the regression model is easier, in terms of the math that we need to understand, than inferring the latent states in an HMM. On the other hand, modeling the data using an HMM, gives us not only a way to forcast into the future, we also get an explicit description of how the system evolves – how the future depends on the past. The dynamics of an HMM can provide domain specific insights into the data generating process. Furthermore, the relationship between the observed data \\(Y_n\\) and the time index \\(n\\) is often much more complex than the dynamics – the relationship between \\(Y_n\\) and \\(Z_n\\), the relationship between \\(Z_{n+1}\\) and \\(Z_n\\). Thus, we may do better modeling the simple dynamics rather than the complex function of \\(Y_n\\) versus \\(n\\). 23.2 Modeling Sequential Data and Sequential Actions 23.2.1 Describing a Dynamic World With Markov Models (or Markov Chains, Markov Processes) and HMMs, we can describe a wide range of real-life phenomena that evolve through time. The key machine learning skill here is to be able to translate between the real-world and the model formalisms. That is, you must become fluent in: 1. Designing state spaces, \\(S\\), that capture important aspects of the real-life phenomena 2. Designing, learning and interpreting the dynamics. In the following example, we translate a medical study of personal fitness into a Markov Process. In this study, we are investigate the evolution of fitness in participants enrolled in a personal training program. We observe the participants’ performance on a number of physical activities and divide them into two categories: fit and unfit. Over 11 weeks, we observe how the number of fit and unfit participants change from week to week. In particular, we find that from week \\(n\\) to week \\(n+1\\): 1. 80% of participants who started the week as unfit, stayed unfit 2. 60% of participants who started the week as fit, became unfit. We formalize our observation in the following Markov process: Check your understanding: What design choices did we make in constructing our model? What are the pro’s and con’s of these choices? If we want to make different design choices, what additional data do we need to collect (e.g. what if we wanted a state space that had three different fitness levels)? 23.2.2 Acting in a Dynamic World While our Markov process for the fitness study captures how participant fitness level evolve, this model does not tell us why fitness evolves this way. That is, we are treating our participants as passive vessels for fitness rather than agents that actively make choices to change their worlds. If we want a more complete picture of participant behavior in our fitness study, we also need to model participant actions. One model for sequential data resulting from sequential decision making (action) in the real-world is a Markov Decision Process (MDP). An MDP is defined by a set of 5 parameters, \\(\\{S, A, T, R, \\gamma\\}\\): 1. The state space, \\(S\\), describing properties of the real-life phenomenon under our study (e.g. fitness) 2. The action space, \\(A\\), describing posible actions that agents with free will can perform. 3. The dynamics or transition, \\(T\\), which describes the probability of transitioning to state \\(s&#39;\\) given that the agent starts at state \\(s\\) and performs action \\(a\\). That is, the transition is a function, \\(T: S\\times A \\times S \\to [0,1]\\), where \\(T^a(s, s&#39;) = \\mathbb{P}[s&#39; | s, a]\\). When the state space \\(S\\) is finite, we often represent \\(T\\) as a set of matrices, \\(T^a \\in \\mathbb{R}^{|S| \\times |S|}\\), where \\(T^a_{ij} = \\mathbb{P}[s_i | s_j, a]\\), for \\(a\\in A\\). 4. The reward function, \\(R: S\\times A \\times S \\to \\mathbb{R}\\), which defines the reward received by the agent by transitioning to state \\(s&#39;\\) given that the agent starts at state \\(s\\) and performs action \\(a\\). The reward function encapsulates the agent’s goals and motivations for acting. Note: in some texts, the reward function is defined only in terms of the current state and the action, i.e. \\(R: S\\times A \\to \\mathbb{R}\\). We can always translate a model using \\(R: S\\times A \\times S \\to \\mathbb{R}\\) into a model using \\(R: S\\times A \\to \\mathbb{R}\\). You’ll see that defining the reward as \\(R: S\\times A \\times S \\to \\mathbb{R}\\), gives us an intuitive advantage in the Grid World Example. 5. The discount factor, \\(\\gamma\\in [0,1]\\), which describes how the agent prioritizes immediate versus future rewards. In other words, a Markov Decision Process (MDP) is a Markov Process in which the agent is not simply evolving through time, but actively guiding this evolution via actions. Let’s revisit our fitness study. This time, rather than just modeling the week to week changes in the fitness levels of participants, we also model what the participants are doing to affect their own fitness levels. That is, each week, we observe if the participant is working out or not. Check your understanding: Translate the MDP formalism above into statements about our fitness study. What design choices did we make in defining this model? What are the pro’s and con’s of these choices? If we wanted to make different choices, what additional data do we need to collect, and how would these design choices affect the rest of the model? 23.2.3 Describing Worlds as MDP’s Note that in the fitness example, we used a graphical representation, a finite state diagram, for both the state space as well as the dynamics (how one state transitions to others via different actions). These diagrams are useful in designing MDP’s, since they allow us to reason easily and intuitively about the logic underlying our model. For more complex worlds, with a large number of states or actions, it becomes impractical to represent our MDP using finite state diagrams – diagrams that have a large number of nodes for states and large number of arrows for actions would be difficult to read and interpret. Here, we introduce another useful tool for graphically (and intuitively) representing changing environments – the grid world. A grid world is a bounded 2-dimensional (often discretized) environment in which the states are positions (grid-coordinates) and actions represent intended movement between positions (e.g. left, right, up, down). We can reason about the dynamics of grid worlds using analogies with physical movement. For example, if we wanted to study dynamics in which it’s impossible to move from one position to another, we can imagine this is because there is a physical barrier between the two positions. That is, grid worlds can be represented as a 2-D map, where the actions are physical movements and the dynamics are barriers to or conduits for movements. Moreover, reward functions can be graphically represented in grid worlds as goal states (states where we want to direct movement) and hazard states (states into which we want to avoid moving). In the following example grid world, we have a 2-D map with 2 rows by 3 columns of grids. We have three different types of grids: river, parking lot and forest. The different types of grids represents different types of barriers to movement and different motivations for movement. We use this grid world to model an example where an agent is lost in the woods and must navigate to the parking lot – our goal state. Movement through forest grids is unimpeded; moving through river grids, however, is difficult. The river has a strong north-west current – there is always some chance that you will be swept in some unwanted direction when moving through a river grid. We can formalize the dynamics of this environment as an MDP: Check your understanding: Translate the MDP formalism above into statements about our grid world. What design choices did we make in defining this model? What are the pro’s and con’s of these choices? If we wanted to make different choices, what additional data do we need to collect, and how would these design choices affect the rest of the model? If we wanted to formalize our grid world as an MDP using \\(R: S\\times A \\to \\mathbb{R}\\), how would we need to change our MDP definition? 23.3 Modeling Sequential Decisions: Planning Now that we have choices over actions in a changing world, what actions should we take – what makes some actions better than others? In reinforcement learning, our goal is to choose actions that maximizes the cumulative reward we expect to collect over time. A entity that is interacting with our world and choosing actions according to this goal is called an RL agent. To formalize this goal, we need to formalize two notions: 1. a formal way to “choose” actions 2. a formal way to quantify the cumulative reward we expect to collect over time 23.3.1 Modeling Action Choice In real-life, we choose our actions based on our position in the world. Thus, it’s natural to formalize action choice as a function \\(\\pi\\) that depends on the value of current state, \\(Z_n\\). Check your understanding: In an MDP setting, why can’t \\(\\pi\\) depend also on the values of previous states (i.e. the history)? Can you think of a real-life scenario where the independence of \\(\\pi\\) and the history is reasonable; can you think of a real-life scenario where the independence of \\(\\pi\\) and the history is inappropriate? Given the current state \\(Z_n = s\\), we have two types of action choice: 1. Deterministic - given \\(s\\), you always choose action \\(\\pi(s)\\); that is, \\(\\pi: S \\to A\\). 2. Stochastic - given \\(s\\), you randomly choose sample an action from some distribution \\(\\pi(s)\\); that is, \\(\\pi: S \\to [0,1]^{|A|}\\), where \\(\\pi(s)\\) is a distribution over actions. A function \\(\\pi\\) is called a policy if \\[ \\pi: S \\to A \\] or \\[ \\pi: S \\to [0,1]^{|A|}, \\] where \\(\\pi(s)\\) is a distribution over actions. Check your understanding: Formalize the following strategy for navigating the grid world above as a policy (i.e. a function \\(\\pi\\)): “If I’m in the forest on the north-west side of the river, I go east; if I’m in the forest on the south-west side of the river, I go north. If I’m in the south part of the river, I swim north; if I’m in the north part of the river, I swim east. If I’m in the south-east side of the river, I go north.” If I start at the south-west side of the river and follow this policy, what would my trajectory (as a sequence of states and actions) look like? 23.3.2 Modeling Cumulative Reward Now that we have formalized action choice as a policy, we want to distinguish good policies from bad ones. Given a trajectory as a sequence of states and actions, \\(\\{(Z_0, A_0), (Z_1, A_1), (Z_2, A_2),\\dots\\}\\), we quantify the return for that trajectory as: \\[ G = R_{0} + \\gamma R_{1} + \\gamma^2R_{2} + \\ldots = \\sum_{n=0} \\gamma^n R_{n } \\] where \\(R_{n}\\) is the reward collected at time \\(n\\). Essentially, the return is the (discounted) sum of all the rewards we collected over time. Note that \\(\\gamma\\), the discount factor of the MDP, is used in computing the return - if \\(\\gamma\\) is small, rewards collected later in time will count for less that rewards collected earlier in the trajectory. Check your understanding: Why do we discount? Often we argue that discounting is needed if we allow infinite trajectories. Show that if \\(\\gamma = 1\\), then \\(G\\) can be undefined. Another important reason to discount is to model the tendency of natural intelligence (e.g. animals) to prioritize immediate rewards vs rewards far in the future. If \\(\\gamma \\approx 0\\) what does this imply about the way an RL agent might choose their actions? Can you think of other reasons why discounting can be important in RL – i.e. in what other real-life applications of RL might we wish to choose \\(\\gamma &lt; 1\\)? 23.3.2.1 Value Functions Now, we can quantify the value of a policy, starting at a particular state – the value function \\(V^\\pi: S\\to \\mathbb{R}\\) of an MDP is the expected return starting from state \\(s\\), and then following policy \\(\\pi\\): \\[ V^\\pi(s) = \\mathbb{E}_{\\pi}[G|Z_0 = s] \\] The expectation above is taken over all randomnly varying quantities in \\(G\\). Check your understanding: In a general MDP, which quantities that \\(G\\) depends on are randomly varying? Hint: think about where is the randomness in your reward, transition and policy. For our grid world and the policy \\(\\pi\\) we described, explicitly write out the value function for a few states – that is, explicitly compute some expectations. 23.3.2.2 Action-Value Functions Rather than asking for the value of our policy starting at some state \\(s\\), we can also ask for the value of taking a particular action at a particular state. The action-value function, or the \\(Q\\)-function, \\(Q^\\pi: S \\times A \\to \\mathbb{R}\\) , quantifies the value of a policy, starting at a state \\(s\\) and taking the action \\(a\\), and then following policy \\(\\pi\\): \\[ Q^\\pi(s, a) = \\mathbb{E}_\\pi[G|Z_0=s, A_0 = a] \\] Again, the expectation above is taken over all randomnly varying quantities in \\(G\\). Check your understanding: For our grid world and the policy \\(\\pi\\) we described, explicitly write out the \\(Q\\)-function for a few state-action pairs – that is, explicitly compute some expectations. 23.3.3 Planning: Optimizing Action Choice for Cumulative Reward In reinforcement learning, our goal is to find the policy \\(\\pi^*\\) that achieves the maximum value over all possible policies. That is, we call \\(\\pi^*\\) the optimal policy if \\[ V^{\\pi^*}(s) = \\max_\\pi V^{\\pi}(s),\\; \\text{for all } s\\in S. \\] We call \\(V^{\\pi^*}\\), or \\(V^*\\), the optimal value function. We call an MDP solved when we know the optimal value function. The task of finding the optimal policy or the optimal value function is called planning. Check your understanding: Right now, we don’t have any way of solving MDP’s (i.e. systemmatically finding optimal policies), but in many cases, we can reason intuitively about properties of our optimal policy. In our grid world example, write out a couple of possible policies; for each policy, simulate some trajectories (start at a fixed grid and execute a policy). Based on your simulations, what properties should an optimal policy have? Hint: think about length of trajectory, passage through certain grids or avoidance of certain grids. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
