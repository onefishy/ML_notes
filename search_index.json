[["index.html", "Notes for CS181: Machine Learning Chapter 1 Notes for CS181: Machine Learning 1.1 Spring 2023", " Notes for CS181: Machine Learning Yihui Xie 2023-05-02 Chapter 1 Notes for CS181: Machine Learning 1.1 Spring 2023 This is a set of notes from the Spring 2023 iteration of CS181: Machine Learning "],["intro.html", "Chapter 2 What is CS181? 2.1 Why Is AI a Big Deal? 2.2 Machine Learning is Much More Than Accuracy 2.3 What is CS181? 2.4 What We are Offering You 2.5 What We are Asking From You 2.6 Grading &amp; Evaluation", " Chapter 2 What is CS181? 2.1 Why Is AI a Big Deal? Artifcial intellegence or machine learning models are becoming increasinglly ubiquitous in modern life. These models have already made meaningful impact on many of the most pressing problems we face today. Most impactfully, AI has enabled me to generate endless alternate reality depictions of my animal co-conspirators: 2.1.1 But Is Accuracy Enough? When machine learning models are applied to safety-critical, risk-adverse domains such as health care, reliable measurements of a model’s justifications for a prediction as well as its predictive uncertainty may be as crucial as correctness of its predictions. Predictive uncertainty helps us quantify risk in down-stream tasks: We also care about the source of uncertainty: Reducible uncertainty due to the lack of data is often called epistemic uncertainty, whereas irreducible uncertainty due to the inherent noisiness of the prediciton is called aleatoric uncertainty. Knowing where our uncertainty comes from helps us make decisions about how to mitigate risk. 2.1.2 What Happens When Machine Learning Models are Catastrophically Wrong? Unfortunately, many of the machine learning models that achieve the most impressive performances do not provide any indications to users when they are operating “out of their depth”. In fact, when ML models break, they often do so silently and their failures may go unnoticed for a long time. Image from: Why ReLU Networks Yield High-Confidence Predictions Far Away From the Training Data and How to Mitigate the Problem 2.1.2.1 Example: Detecting When the Model is Operating in Unfamiliar Territories In Efficient Out-of-Distribution Detection in Digital Pathology Using Multi-Head Convolutional Neural Networks (Linmans et al, Medical Imaging with Deep Learning 2020), the authors train an uncertainty-aware neural network model to detect breast cancer metastasis. The epistemic uncertainty is used during test time to detect new types of breast tissue images that were not included in the training data. Classification of these novel types of images can be deferred to a human. Unfamiliar types were caught at test time with AUC of .98. 2.1.3 Are Machine Models Right for the Right Reasons? In order for human decision makers to interact with machine learning models meaningfully – for example, in order to verify their correctness – we need these models to be interpretable. But explaning complex models is difficult. 2.1.3.1 Example: Explainable ML Diagnostic Model In An explainable deep-learning algorithm for the detection of acute intracranial haemorrhage from small datasets (Lee et al, Nature Biomedical Engineering, 2019), the authors build a neural network model to detect acute intracranial haemorrhage (ICH) and classifies five ICH subtypes. Model classifications are explained by highlighting the pixels that contributed the most to the decision. The highligthed regions tends to overlapped with ‘bleeding points’ annotated by neuroradiologists on the images. 2.1.4 What is the Role of the Human Decision Maker? We tend to think of modeling as a purely mathematical or engineering feat, but in many cases the model has a complex interaction with a human decision maker. We not only need to worry about the performance of the model, we also need to worry about the performance the combined system of Human + AI. Image from: Does Higher Prediction Accuracy Guarantee Better Human-AI Collaboration? Of course, disasterous combinations of AI and humans have long been fodder for cinematic imagination. 2.1.4.1 Example: The Promises of Human + AI Systems In Consistent Estimators for Learning to Defer to an Expert (Mozannar et al, International Conference on Machine Learning, 2020), the authors trains a model that decides when (and how) to classify an input and when to defer the decision to an human expert. The joint Human + AI system can be superior to both components. 2.1.4.2 Example: The Perils of Human + AI Systems In How machine-learning recommendations influence clinician treatment selections: the example of antidepressant selection (Jacobs et al, Translational Psychiatry, 2021), the authors found that clinicians interacting with incorrect recommendations paired with simple explanations experienced significant reduction in treatment selection accuracy. Incorrect ML recommendations may adversely impact clinician treatment selections and that explanations are insufficient for addressing overreliance on imperfect ML algorithms. 2.1.5 What are the Broader Impacts of Tech? The days where we’d looked at Sillicon Valley unicorns and the promise of their tech through the rose-colored glasses of unbridled optimism are long gone. These days tech are still grabbing headlines, but often not for the right reasons. Increasing in prominence are calls for more diverse (in terms of domain expertise, discipline and lived-experiences) lenses to be applied to AI technology. 2.2 Machine Learning is Much More Than Accuracy Machine learning is much more than engineering for abstract performance metrics. Increasingly the field is grappling with the role digital technology is playing in our entire socio-technical system. Machine learning and data science are interdisciplinary fields that require people with diverse skill-sets/backgrounds to work closely and cooperatively. There is also increasing calls for machine learning/data science researchers to engage meaningfully with social, economic, political, cultural and ethical impacts of their work when embedded in complex human institutions. 2.3 What is CS181? Build statistical (Bayesian and non-Bayesian) models for: continuous, ordinal and categorical data Study algorithms for model fitting and inference Study paradigms for model evaluation and critique Understand ways models can fail or produce unintended negative impact in real-life settings Goal: students become familiar with standard statistical models and modern techniques of inference. At the end of the course you should be able to productively engage with current machine learning developments and apply a number of models to solve real-life problems. You should also be able to anticipate model failure modes and perform nuanced analyses of the broader social impact of your model. Focus: - Why: theory should serve a concrete purpose. - How: emphasize computational aspects of inference. - But Should I?: anticipate failure modes and negative social impacts. 2.4 What We are Offering You We have structured CS181 to accomodate a number of different learning goals and learning styles. We have videos in traditional lecture format, textbooks, notes and additional resources for those of you who learn best in traditional classrooms. We have plenty of interactive, synchronous course components where you can ask questions, practice what you learn and extend your knowledge. There is content for both students looking for a higher level overview of ML as well as links to resources for students looking for more depth on particular topics. There are access points to different types of research areas in ML (the “Beyond Sections” sections). 2.5 What We are Asking From You We’ve put in a substantial amount of work to structure CS181 to support your learning. We are asking you also put in work in order to make this a successful learning experience: Talk to course humans: Come to instructor and TF office hours Come to Sections to review and reinforce learning Come to Beyond Sections to contextualize and broaden your knowledge Ask questions: Ask questions to understand. There is no such thing as an obvious fact or a trivial question. Don’t let shyness of intimidation prevent you from asking for help to understand something. Ask questions to dig deeper. Every single concept in this course serves a purpose and has a justification. Don’t settle for knowing facts, there’s always a questions you can ask about something you already know that will show you something new and something deep. Focus on creating connections, relationships between and syntheses of different concepts. Don’t worry about memorizing lines of math. Challenge yourself! Machine learning is currently challenging our understanding of sentience/intelligence, ethics, law, labor, social/cultural dynamics. Machine learning should be challenging you! Make space for everyone’s perspective – there is no “solving machine learning”, “winning at machine learning” and no such thing as the “best solution” or “right answer”. Make space for your own perspective – your background, whatever it is, is an important perspective for machine learning. Look beyond the class – the goal of the class isn’t to get you to be fluent in math, stats or pytorch; the goal is to help you see how ML figures into your life (e.g. research, academic/professional interests/plans, empowered citizenship) 2.6 Grading &amp; Evaluation Generally speaking, the grading in this course is formative not punitive. We are looking to see that you’ve grasped basic skills and fundamental concepts, we are not looking to deduct points for various mistakes. Our evaluation philosophy is standard-based. That is, just as there is a basic level of fluency and familiarity you neeed with operation of motor vehicles to obtain a driver’s license, there is a basic level of fluency and familiarity with theory/implementation/impact analysis you are expected to gain in order to responsibly operate machine learning models in the real world. Course staff is here to support your learning in order to meet these standards and engage with ML meaningfully in your own way. In particular, we are not interested in: 1. competition 2. ranking of students along arbitrary axes 3. making anything hard for “hardness” sake "],["what-is-regression.html", "Chapter 3 What is Regression? 3.1 What is Machine Learning? 3.2 What is Regression? 3.3 (Almost) Everything is Linear Regression 3.4 What is Model Evaluation? 3.5 What is Model Critique? 3.6 Limitations and Connections", " Chapter 3 What is Regression? 3.1 What is Machine Learning? Machine learning is typically the task of learning a function, \\(f\\), given a set of training data, \\(\\mathcal{D}\\). This learnt function \\(f\\) can be used to make predictions on new data in regression and classification, this function can be used to explain how the observed data is structured in unsupervised learning. In this class, this function \\(f\\) can be linear (linear, polynomial, basis regression models), represented by a neural network (deep learning models), or defined without an explicit formula in terms of the input (non-parametric models)! 3.2 What is Regression? Thus far in the course, a regression problem is the task of predicting an output value \\(y \\in \\mathbb{R}\\), given an input \\(\\mathbf{x} \\in \\mathbb{R}^D\\). We have seen one way to solve a regression problem: 1. choose a training data set of \\(N\\) number of observations, \\(\\mathcal{D} = \\{(\\mathbf{x}_1, y_1),\\ldots,(\\mathbf{x}_N, y_N)\\}\\). 2. hypothesize that the relationship between \\(\\mathbf{x}\\) and \\(y\\) is captured by \\[ y = f_\\mathbf{w}(\\mathbf{x}) \\] where \\(\\mathbf{w} \\in \\mathbb{R}^M\\) are the parameters (i.e. unknown coefficients) of the function \\(f\\). 3. choose a math notion for “how well \\(f_\\mathbf{w}\\) fits the data”, i.e. a loss function, \\(\\mathcal{L}(\\mathbf{w})\\). 4. choose a way to solve for the parameters \\(\\mathbf{w}\\) that minimizes the loss function: \\[ \\mathbf{w}^* = \\mathrm{argmin}_\\mathbf{w}\\; \\mathcal{L}(\\mathbf{w}) \\] This framework applies to linear, polynomial and basis regression as well as fancy neural network regression. Note: every step in the above solution is a design choice, which means that you can chose wrongly for your given real-life problem. In your model evaluation step, you must revisit and critique each design choice. 3.3 (Almost) Everything is Linear Regression In linear regression, we assume that our function \\(f_\\mathbf{w}\\) as the form \\[ y = f_\\mathbf{w}(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} \\] If we choose Mean Square Error as our loss function, then \\[ \\mathcal{L}(\\mathbf{w}) = \\frac{1}{N}\\sum_{n=1}^N(y_n - f_\\mathbf{w}(\\mathbf{x}))^2. \\] If we choose to analytically minimize \\(\\mathcal{L}\\) over possible values of \\(\\mathbf{w}\\) (using calculus), then we get \\[ \\mathbf{w}^* = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y} \\] where \\(\\mathbf{X} \\in \\mathbb{R}^{N\\times D}\\) is the matrix of all your training input and \\(\\mathbf{y} \\in \\mathbb{R}^N\\) is the vector of all your training target values. For each new input \\(\\mathbf{x}\\), our model prediction would be \\[ \\hat{y} = (\\mathbf{w}^*)^\\top\\mathbf{x} = \\left[(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}\\right]^\\top\\mathbf{x}. \\] Once we have solved the linear regression problem, it turns out we have solved the regression problem for a large number of non-linear regression problems! Polynomial Regression: Polynomial regression of degree \\(K\\) is just linear regression on top of input data that’s been augmented with polynomial features: \\[ y = \\mathbf{w}^\\top \\phi(\\mathbf{x}) \\] where \\(\\phi: \\mathbb{R}^D \\to \\mathbb{R}^{D * K + 1}\\) is defined by \\[ \\phi((1\\; x_1\\; \\ldots\\; x_D)^\\top) = (1\\; x_1\\; x^2_1\\; \\ldots\\; x^K_1\\; x_2\\; x^2_2\\; \\ldots\\; x^K_2\\; \\ldots \\; x_D\\; x^2_D\\; \\ldots\\; x^K_D)^\\top. \\] Thus, the solution to polynomial regression is \\[ \\mathbf{w}^* = (\\phi(\\mathbf{X})^\\top\\phi(\\mathbf{X}))^{-1}\\phi(\\mathbf{X})^\\top\\mathbf{y} \\] For each new input \\(\\mathbf{x}\\), our model prediction would be \\[ \\hat{y} = (\\mathbf{w}^*)^\\top\\mathbf{x} = \\left[(\\mathbf{\\Phi}^\\top\\mathbf{\\Phi})^{-1}\\mathbf{\\Phi}^\\top\\mathbf{y}\\right]^\\top\\mathbf{x}, \\] where we write \\(\\mathbf{\\Phi}\\) for \\(\\phi(\\mathbf{X})\\). Regression on Arbitrary Bases: We can choose any non-linear feature map \\(\\phi: \\mathbb{R}^{D} \\to \\mathbb{R}^{D&#39;}\\) and capture non-linear trends in the data by performing linear regression on \\(\\phi(\\mathbf{x})\\). Neural Network Regression: Later, we will view neural network regression as: 1. first learning, rather than choosing, a feature map \\(\\phi\\) 2. then performing linear regression on \\(\\phi(\\mathbf{x})\\). 3.4 What is Model Evaluation? So, we’ve solved (linear) regression, i.e. we found \\(\\mathbf{w}^*\\) that minimizes the Mean Square Error on \\(\\mathcal{D}\\). Are we done? No! The machine learning has only just started! We still need to evaluate our model: what is the Mean Square Error of our model on the test data? Why do we need to check this? &gt; What if train MSE is 0.9 and test MSE is 10? can we visually inspect our model on training and test data (i.e. plot the model against the data)? Why visually inspect if we’ve computed MSE? we need to interpret the model. Why interpret when we have the MSE? &gt; What if we saw that selling price was related to square footage and number of rooms by \\(y = -5 * \\mathrm{Sqft} + \\mathrm{Rooms}\\)? does the model help us solve the problem? That is, who is making the final decision on the problem? How are they going to be using the model? What information do they need from the model to make good decisions? &gt; What we found the literal model that generated the data, but the MSE will always be around 10 mg (why MSE not be zero)? Say our task is to prescribe a new seizure medication for epileptic patients, where the safe dosage range is between 20 mg and 30 mg. Is this a helpful model? what are the limitations of our models – what are the assumptions we made when building/testing our model? How will our model behave when those assumptions are violated? When our model fails or succeeds, what will be the negative or positive real-life consequences and how will those consequences be distributed amongst the population? &gt; If our model causes physicians in a clinical trial to make poor decisions or draw incorrect conclusions, which patient groups will be the most impacted? How will they be impacted? 3.5 What is Model Critique? If the model fails any of the above four evaluation steps, we need to revisit and potentially re-do each design decision: Is our training data the “right” data set? Is the functional form (linear or polynomial of degree 2) we assumed correct? Does our loss function capture what we really want to happen in real-life? Did we optimize well? Did we provide the right information about the model to the human decision maker? 3.6 Limitations and Connections Our current approach to regression is limited in a number of ways: 1. We need to guess the “right” functional form of the data trend. (Non-parametric regression and Neural Network Regression will help.) 2. Our choice to always minimize MSE seem arbitrary. (Probabilistic Regression will help.) 3. We needed to do lots of fancy calculus to minimize our loss function – what if we can’t solve for the stationary points?! (Gradient Descent will help) 3. Minimizing the loss function seems to involve differentiation – what if it’s too hard to write out the gradient of my loss function?! (Automatic differentiation will help) 3. We can’t explain why observed \\(y\\)’s don’t agree with the predictions of a perfect model \\(f_\\mathbf{w}\\). (Probabilistic Regression will help.) 4. We have important domain knowledge (i.e. higher square footage should not negatively impact sales price) that we are not incorporating into our model. (Bayesian Regression will help) 5. We have no uncertainty, just predictions! (Confidence and predictive intervals will help, so will Bayesian models) 6. Is printing out the model parameters the best way to interpret the model? (Different XAI techniques will help) &gt; Say we find that selling price is related to square footage and number of rooms by \\[ y = 5\\; * \\mathrm{SqFt} + 0.1\\; * \\mathrm{Rooms} \\] What would you say is the most important factor in determining selling price? If you realized that \\(\\mathrm{SqFt} = 15 * \\mathrm{Rooms}\\), would this change your interpretation of the model? 7. Our discussion of model evaluation and critique have been in the abstract – we have not considered any real-life evaluations of or constraints on our models. (Working with domain experts will help) "],["what-are-probablistic-and-non-probablistic-regression.html", "Chapter 4 What are Probablistic and Non-Probablistic Regression? 4.1 What is Probabilistic Regression? 4.2 (Almost) Everything is Linear Regression 4.3 The Cube: A Model Comparison Paradigm", " Chapter 4 What are Probablistic and Non-Probablistic Regression? 4.1 What is Probabilistic Regression? As we saw in Lecture #2, in regression, we can choose amongst a very large number of loss functions (i.e. functions that quantify the fit of our model). One way to justify our choice is to reason explicitly about how residuals (prediction errors) arise. If we include in our model specification a theory of how residuals arise as a random variable, then we have a probabilistic regression model. A probabilistic regression problem is the task of predicting an output value \\(y \\in \\mathbb{R}\\), given an input \\(\\mathbf{x} \\in \\mathbb{R}^D\\), where the data generating process includes a source of random noise \\(\\epsilon\\). We have seen one way to solve a probabilistic regression problem, where the observation noise is additive: 1. choose a training data set of \\(N\\) number of observations, \\(\\mathcal{D} = \\{(\\mathbf{x}_1, y_1),\\ldots,(\\mathbf{x}_N, y_N)\\}\\). 2. hypothesize that the relationship between \\(\\mathbf{x}\\) and \\(y\\) is captured by \\[ f_\\mathbf{w}(\\mathbf{x}) \\] where \\(\\mathbf{w} \\in \\mathbb{R}^M\\) are the parameters (i.e. unknown coefficients) of the function \\(f\\). 3. choose a RV \\(\\epsilon\\) that describes the additive observation noise: \\[ y = f_\\mathbf{w}(\\mathbf{x}) + \\epsilon,\\; \\epsilon \\sim p(\\epsilon | \\theta) \\] This choice defines a likelihood \\(p(y| \\mathbf{w}, \\mathbf{x})\\), describing the likelihood of observing \\((y, \\mathbf{x})\\) given that the model is \\(f_\\mathbf{w}\\). 5. choose a math notion for “how well \\(f_\\mathbf{w}\\) fits the noisy data”, i.e. a loss function, \\(\\mathcal{L}(\\mathbf{w})\\). 6. choose a way to solve for the parameters \\(\\mathbf{w}\\) that minimizes the loss function: \\[ \\mathbf{w}^* = \\mathrm{argmin}_\\mathbf{w}\\; \\mathcal{L}(\\mathbf{w}) \\] This framework applies to probabilistic regression with any functional form for \\(f\\) (linear, polynomial, neural network etc). Note: while \\(\\mathbf{w}\\) is learnt, the parameters \\(\\theta\\) of the noise distribution \\(p(\\epsilon | \\theta)\\) is typically set by the ML engineer prior to learning \\(\\mathbf{w}\\). Parameters like \\(\\theta\\) that must be chosen prior to learning are called hyperparameters. Technically, \\(y\\) is conditioned on \\(\\mathbf{w}\\), \\(\\mathbf{x}\\) and \\(\\theta\\) in the likelihood, \\(p(y| \\mathbf{w}, \\mathbf{x}, \\theta)\\), but typically we drop the \\(\\theta\\) and write \\(p(y| \\mathbf{w}, \\mathbf{x})\\) if we are not intending to infer it from the data. 4.2 (Almost) Everything is Linear Regression Overwhelmingly frequently in machine learning, we assume additive zero-mean homoskedastic Gaussian noise, that is \\[ y = f_\\mathbf{w}(\\mathbf{x}) + \\epsilon,\\; \\epsilon \\sim \\mathcal{N}(0, \\sigma^2) \\] which implies a Gaussian likelihood: \\[ y | \\mathbf{w}, \\mathbf{x} \\sim \\mathcal{N}(f_\\mathbf{w}(\\mathbf{x}), \\sigma^2). \\] We choose the negative joint log-likelihood of the data as the metric for how well our model fits the data \\[ \\mathcal{L}(\\mathbf{w}) = -\\sum_{n=1}^N \\log p(y_n | \\mathbf{w}, \\mathbf{x}_n). \\] Choosing this loss follows the Maximum Likelihood Principle, encoding our belief that models that renders the observed data more likely are better models. Now, we learn parameters of \\(f\\) to minimize \\(\\mathcal{L}\\) (equivalently, maximizing the joint log-likelihood of the data): \\[ \\mathbf{w}^{\\text{MLE}} = \\mathrm{argmin}_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}) \\] The solution \\(\\mathbf{w}^{\\text{MLE}}\\) is called the maximum likelihood estimate of \\(\\mathbf{w}\\). We showed that under this set of assumption and choices, probabilistic regression is equivalent to non-probabilistic regression: \\[ \\mathbf{w}^{\\text{MLE}} = \\mathbf{w}^{\\text{OLS}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y} \\] Maximizing log-likelihood is the same as minimizing the MSE. Question 1: If the solution to probablistic regression is equivalent to non-probabilistic regression, does this mean that probabilistic regression models are equivalent to non-probabilistic models? Question 2: We saw that MSE: 1. cannot be used alone to evaluate the “goodness” of a model 2. can be helpful to detect overfitting 3. may be unhelpful in detecting underfitting What about the log-likelihood? Question 3: How does one evaluate a probabilistic model? Question 4: How do we choose hyperparameters of our probabilistic model? Validation Prior or Domain Knowledge Analytically or Algorithmically Optimizing an Objective Function: \\[ \\sigma^{\\text{MLE}} = \\mathrm{argmax}_\\sigma\\; \\ell(\\mathbf{w}^{\\text{MLE}}, \\sigma) = \\mathrm{argmax}_\\sigma\\; \\log\\left[\\prod_{n=1}^N p(y_n | \\mathbf{x}_n, \\mathbf{w}^{\\text{MLE}}, \\sigma)\\right] \\] 4.3 The Cube: A Model Comparison Paradigm 4.3.0.1 I. Axis: Probabilistic vs Non-probabilistic Model Definition Pro Con Probabilistic Specifying a distribution for the data (and potentially the model), explicitly defining the source and type of noise When model is “wrong”, we have more assumptions we can interrogate and change in order to improve the model We need to make more choices (e.g. assuming a distribution for the noise); more choices means more chances to be wrong Non-Probabilistic Specify a functional form for the data; does not explicitly define sources and types of data randomness We make less assumptions about the data (which may be wrong); not all useful objective functions (e.g. human personal preference) can be easily given a probabilistic interpretation When the “model” is wrong, we have less assumptions to attribute the problem and hence fewer obvious ways to fix the model Probabilistic: regression by specifying likelihood Non-probabilistic: regression by minimizing MSE, KNN, kernel regression (for now), regression trees (for now) Question: When is it better to use probabilistic regression, when is it better to use non-probabilistic regression? 4.3.0.2 II. Axis: Parametric vs Non-Parametric Model Definition Pro Con Parametric Assumes a fixed, finite functional form \\(f(\\mathbf{x})\\) for regressor Once the parameters of the functional form is learnt, only the parameters needs to be saved Need to guess the right functional form for the regressor Non-Parametric Functional form is implicit and can adapt to observed data No need to specify an explicit functional form Typically, the training data needs to be saved for every prediction Parametric: linear, polynomial, arbitrary basis regression (probabilistic and non-probabilistic); regression trees Non-Parametric: KNN, kernel regression Question: When is it better to use parametric regression, when is it better to use non-parametric regression? 4.3.0.3 III. Axis: Ease of Interpretation Model Example Potential Pro Potential Con Simple Models Models that can be easily inspected in its entirety, models whose decision process can be easily described Through interpretation these models can provide scientific insight, be validated by humans, encourage user trust Simple models may be, though are not necessarily, lacking in capacity to capture complex trends in data Complex Models Models with too many parameters, complex non-linear transformation of data, or whose decision process is lengthy Complex models often have the capacity to capture very interesting trends in the data Complex models may be, though are not necessarily, more difficult to interpret, explain, validate and trust Model Interpretation: interpretability isn’t a binary label, every model can be “interpreted” in some way: - linear regression: looking at regression weights - arbitrary basis regression (including neural networks): looking at regression weights and the features \\(\\phi(\\mathbf{x})\\) - probabilistic regression: looking at regression weights and the noise distribution - regression tree: printing out the tree as a sequence of branching decisions - KNN: looking at the \\(k\\) nearest neighbors - kernel regression: interpreting the kernel \\(k\\) as a measure of similarity between two inputs \\(\\mathbf{x}\\) and \\(\\mathbf{x}&#39;\\) Question: When do we need interpretable models, when does interpretability not matter? Who needs interpretable models? What does interpretable mean? "],["what-matters-in-ml-besides-prediction.html", "Chapter 5 What Matters in ML Besides Prediction? 5.1 What is Machine Learning? Revisited 5.2 What Are We Uncertain About? 5.3 Where is Uncertainty Coming From? 5.4 How Do We Compute Uncertainty? 5.5 Mathematizing Uncertainty: Starting with Bias and Variance 5.6 The Bias-Variance Trade-off in Machine Learning", " Chapter 5 What Matters in ML Besides Prediction? 5.1 What is Machine Learning? Revisited In Lecture #2 we defined machine learning as learning the parameters of function (or of a distribution, if we are being probabilistic) that best fits with observed data. This definition needs refinement! In reality, finding parameters is just sub-goal of a much more complex goal: What does “doing Machine Learning” look like? The short answer: it looks like making and justifying a sequence of choices, while making our assumptions and biases as explicit as possible: Choosing a training data set \\(\\mathcal{D}\\). Question: What assumptions do we make when we make this choice? Choosing a model for the trend in the data \\(f\\) or for the distribution of the data (trend and noise), i.e. the likelihood, \\(p(y \\vert f, \\theta)\\) Question: What assumptions do we make when we make this choice? Choosing a loss function – i.e. a way to measure the fit of \\(f\\) (and potentially \\(\\theta\\)) Question: What assumptions do we make when we make this choice? Choosing a way to optimize the loss function Question: What assumptions do we make when we make this choice? Choosing a way to evaluate the model we learned – we may choose to evaluate the model using a different metric than the loss function! Question: What assumptions do we make when we make this choice? What does a Machine Learning product look like? The short answer: it looks like a technical artifact as well as a recommended policy to guide the appropriate, ethical and responsible usage of it (and potentially much more!). For example, see Model Cards for Model Reporting 5.2 What Are We Uncertain About? If we really work with the idea that everything in machine learning is a choice, including the training data, this means that we could have chosen a different training data set. For example: if we collected our training data from patients in one hospital, we can ask what would have happened if we collected data from a different hospital? Generally speaking, data sets collected at different times, from different locations or from different populations will be slightly (or significantly) different. Thus, the functions \\(f\\) we learn on these datasets will differ and these different functions will produce different predictions for the same test point! So, we should be uncertain about: 1. (Math) the function \\(f\\) we learned (e.g. the parameters \\(\\mathbf{w}\\) for \\(f\\) or the function \\(f\\) itself when our model is non-parametric) 2. (Application) our interpretation of \\(f\\) 3. (Math) our prediction \\(\\hat{y}\\) for a new point \\(\\mathbf{x}\\) 4. (Application) our recommendation for how to make decisions based on our model Question: Why do we care about uncertainty? 5.3 Where is Uncertainty Coming From? Generally speaking: 1. uncertainty in \\(f\\) comes from us not having enough data to uniquely determine a function \\(f\\), this could be because of a combination of the below - \\(f\\) is a complex model (e.g. lots of parameters) compared to the number of training data points (the model is under determined) - the data is very noisy and there are very few observations (so that the trend isn’t clear) 2. uncertainty in our prediction \\(\\hat{y}\\) comes from a combination of the above: - uncertainty in \\(f\\) – if we aren’t sure about \\(f\\) we can’t be sure about \\(\\hat{y}\\) - noise in data – even if we are 100% certain that we have the right \\(f\\), we can still be uncertain about the prediction \\(\\hat{y}\\) due to observation noise Question: Why do we care about what’s causing uncertainty? 5.4 How Do We Compute Uncertainty? If we make some strong assumptions about \\(f\\), as well as the distribution of the data, we can analytically compute the uncertainty in \\(f\\) as well as the uncertainty in \\(\\hat{y}\\). Realistically, we often empirically estimate the uncertainty in \\(f\\) and \\(\\hat{y}\\) through simulating drawing new training data sets by resampling our existing data – this is called bootstrapping. For the different bootstrap training data sets, we learn different functions \\(f\\) and make different predictions \\(\\hat{y}\\). The empirical variance of learnt parameters \\(\\mathbf{w}\\) of \\(f\\) gives us an estimate of the confidence interval of our estimate of \\(\\mathbf{w}\\). The empirical variance of our prediction \\(\\hat{y}\\) gives us an estimate of the predictive interval. 5.5 Mathematizing Uncertainty: Starting with Bias and Variance So far we’ve been describing uncertainty purely in intuitive terms. In order to quantify and analyze uncertainty, we need mathematical formalism! One way to formalize our uncertainty over our prediction is to reason about why our prediction might be wrong. We do so by defining and decomposing the generalization error of our model. \\[\\begin{aligned} \\underbrace{\\mathbb{E}_{(\\mathbf{x}, y), \\mathcal{D}}\\left[ (y- f_\\mathbf{w}(\\mathbf{x}))^2\\right]}_{\\text{Generalization Error}} =&amp; \\mathbb{E}_{\\mathbf{x}}\\underbrace{\\mathrm{Var}[y|\\mathbf{x}]}_{\\text{Observation Noise}}\\\\ &amp; + \\mathbb{E}_{\\mathbf{x}}[(\\underbrace{\\mathbb{E}_{y|\\mathbf{x}}[y|\\mathbf{x}]}_{\\text{Average true $y$}\\\\ \\text{over noisy observations}} - \\underbrace{\\mathbb{E}_\\mathcal{D}[f_\\mathbf{w}(\\mathbf{x})]}_{\\text{Average prediction}\\\\\\text{over all possible training sets}})^2]\\\\ &amp;+\\mathbb{E}_{\\mathbf{x}} \\underbrace{\\mathrm{Var}[f_\\mathbf{w}(\\mathbf{x})]}_{\\text{Variance of Model}}\\\\ &amp;= \\text{Observation Noise} + \\text{Model Bias} + \\text{Model Variance} \\end{aligned}\\] From the math, we see that we have three reasons to be uncertain about our model predictions: 1. the observation noise – even if we are 100% certain that we have the right model, our prediction can still be wrong due to noise 2. model bias – we could have been wildly wrong in our guess of the form of the model (e.g. assuming linear function when modeling quadratic data) 3. model variance – the number of data points is insufficient to uniquely determine the model 5.6 The Bias-Variance Trade-off in Machine Learning One reason we want to work with the formalism of the generalization error is that by decomposing the generalization error, we see how we can reduce our uncertainty in our predictions. Immediately, we see that there is nothing we can do to reduce generalization error arising from observation noise – this error is irreducible. We can, however, choose our model so we have some control over model bias and model variance – these errors are reducible. Unfortunately, generally speaking, when we reduce model bias by making our models more complex, the complexity increases model variance (and vice versa): 5.6.1 Examples of the Bias-Variance Trade-off Many modification we perform on machine models are frequently just ways to manage the Bias-Variance Trade-off. Regularization: adding a penalty term to the MSE loss introduces bias (reduces the ability of the model to fit the data), in order to reduce variance (by biasing the optimization towards simpler models) Ensembling: creating a large set of very different complex models (low bias but high variance), and then reducing the variance by average the model predictions Boosting: iteratively making a simple base model (high bias but low variance) more complex and thereby reducing the bias without significantly increasing the variance "],["what-is-logistic-regression.html", "Chapter 6 What is Logistic Regression? 6.1 Logistic Regression and Soft-Classification 6.2 Logistic Regression and Bernoulli Likelihood 6.3 How to Perform Maximum Likelihood Inference for Logistic Regression 6.4 How (Not) to Evaluate Classifiers 6.5 How to Interpret Logistic Regression", " Chapter 6 What is Logistic Regression? 6.1 Logistic Regression and Soft-Classification One way to motivate and develop logistic regression is by casting it as “soft classification”. That is, instead of find a decision boundary that separates the input domain into two distinct classes, in logistic regression we assign a classification probability of a class to an input based on its distance to the boundary. The math of translating (signed) distance (any real number) into a probability (a number between 0 and 1) requires us to choose a function \\(\\sigma: \\mathbb{R} \\to (0, 1)\\), we typically choose \\(\\sigma\\) to be the sigmoid function, but many other choices are available. This gives us a model for the probability of giving a point \\(\\mathbf{x}\\) the label \\(y=1\\): \\[ p(y = 1 | \\mathbf{x}) = \\sigma(f_{\\mathbf{w}}(\\mathbf{x})) \\] 6.2 Logistic Regression and Bernoulli Likelihood Another way to motivate logistic regression is by: 1. first, model the binary outcome \\(y\\) as a Bernoulli RV, \\(\\mathrm{Ber}(\\theta)\\), where \\(\\theta\\) is the probability that \\(y=1\\). Note: assuming a Bernoulli distribution is assuming a noise distribution. 2. second, incorporate covariates, \\(\\mathbf{x}\\), into our model so that we might have a way to explain our prediction, giving us a likelihood: \\[ y \\vert \\mathbf{x} \\sim \\mathrm{Ber}(\\sigma(f_{\\mathbf{w}}(\\mathbf{x}))) \\] or alternatively, \\[ p(y = 1 | \\mathbf{x}) = \\sigma(f_{\\mathbf{w}} (\\mathbf{x})) \\] 6.3 How to Perform Maximum Likelihood Inference for Logistic Regression Again, we can choose to find \\(\\mathbf{w}\\) by maximizing the joint log-likelihood of the data \\[\\begin{aligned} \\ell(\\mathbf{w}) &amp;= \\log\\left[ \\prod_{n=1}^N \\sigma(f_{\\mathbf{w}} (\\mathbf{x}_n))^{y_n} (1 - \\sigma(f_{\\mathbf{w}} (\\mathbf{x}_n)))^{1-y_n}\\right]\\\\ &amp;= \\sum_{n=1}^N \\left[y_n \\log\\sigma(f_{\\mathbf{w}} (\\mathbf{x}_n)) + (1- y_n)\\log(1 - \\sigma(f_{\\mathbf{w}} (\\mathbf{x}_n)))\\right] \\end{aligned}\\] The Problem: While it’s still possible to write out the gradient of \\(\\ell(\\mathbf{w})\\) (this is already much harder than for basis regression), we can no longer analytically solve for the zero’s of the gradient. The “Solution”: Even if we can’t get the exact stationary points from the gradient. The gradient still contains useful information – i.e. the negative gradient at a point \\(\\mathbf{w}\\) is the direction of the fastest instantaneous increase in \\(\\ell(\\mathbf{w})\\). By following the gradient “directions”, we can “climb down” the graph of \\(\\ell(\\mathbf{w})\\). 6.4 How (Not) to Evaluate Classifiers Rule 1: Never just look at accuracy. Rule 2: Look at all possible trade-offs that a classifier makes (for whom is the classifier correct and for whom it is not). 6.5 How to Interpret Logistic Regression For logistic regression with linear boundaries, there are very intuitive ways to interpre the model: But are these “easy” interpretations reliable? "],["how-do-we-responsibly-use-conditional-models.html", "Chapter 7 How Do We Responsibly Use Conditional Models? 7.1 Everything We’ve Done So Far in Probabilistic ML", " Chapter 7 How Do We Responsibly Use Conditional Models? 7.1 Everything We’ve Done So Far in Probabilistic ML For a set of observations \\(y\\), we can collect additional data \\(\\mathbf{x}\\) and model the conditional distribution \\(p(y|\\mathbf{x})\\). This is exactly regression and classification models do, they use covariates \\(\\mathbf{x}\\) to predict the outcome \\(y\\). (Conditional Models) Why do we need additional data \\(\\mathbf{x}\\)? Why isn’t it sufficient to just model \\(y\\) by itself (as a binomial, Guassian or a mixture of Gaussians)? to predict to explain to model finer grain variations in y (i.e. model data heterogeneity) (Classification) For binary data (like whether or not an individual has kidney cancer), we can model \\(p(y|\\mathbf{x})\\) as a Bernoulli \\[ y \\sim \\mathrm{Ber}\\left(\\mathrm{sigm}\\left(f_\\mathbf{w}\\left(\\mathbf{x} \\right)\\right)\\right). \\] (Maximizing the Likelihood) We want to maximize the log Bernoulli likelihood of \\(y\\) (conditioned on \\(\\mathbf{x}\\)) over model parameters \\(\\mathbf{w}\\), but the zeros of the gradient of the log-likelihood cannot be analytically solved for! \\[ \\nabla_{\\mathbf{w}} \\ell(\\mathbf{w}) = -\\sum_{n=1}^N \\left(y_n - \\frac{1}{1 + e^{-\\mathbf{w}^\\top\\phi(\\mathbf{x}_{n})}} \\right) \\phi(\\mathbf{x}_{n}) =\\mathbf{0} \\] But just because we can’t analytically find the zeros of the gradient, it doesn’t mean that the gradient is useless! The gradient of a loss function points to the direction of the greatest rate of increase, hence the negative gradient points to the direction of the fastest way of decreasing the loss function. When we iteratively decrease the loss function by changing \\(\\mathbf{w}\\) in the direction of the negative gradient, this is called gradient descent. (Properties of Gradient Descent) Gradient descent sounds like we can optimize any function without work, is it as good as it sounds? What do I need to know in order to use gradient descent? Is gradient descent guaranteed to converge? Is gradient descent efficient? (Model Evaluation) How do I know my model is “correct”? Choose a meaningful numeric predictive metric (e.g. MSE, log-likelihood, accuracy, AUC etc) – which metric measures the model behavior that I actually care about? Evaluate your model along other important real-life axes: Probabilistic vs Non-probabilistic Parametric vs Non-parametric Interpretability (The Challenges of Conditional Models) Conditional models have so much potential to generate scientific understanding of our data but they are also the most frequently misused! Suppose that you fit a logistic regression model to predict whether a loan application should be approved. Suppose that you have three covariates: x_1 representing gender: 0 for male, 1 for female x_2 for the income x_3 for the loan amount Suppose that the parameters you found are: \\[ p(y=1 | x_1, x_2, x_3) = \\mathrm{sigm}(-1 + 3 x_1 + 1.5 x_2 + 1.75 x_3). \\] What can I conclude about the loan decisions data that this model was fitted on? Can I deploy this model for usage? Bias in your conclusions is exponentially exacerbated by bias in the data collection process. What information are you asking for? If you ask the wrong question, the covariates you collect are at best unpredictive, and at worse they can mask the signal in the data. In many data collection processes, one finds that the questionaire might be using “sex” and “gender” interchangeably, or “race” and “ethnicity” interchageably. But the former (depending on field and context) frequently refers to biological invariants (like chromosomes and genetic lineage), whereas the latter frequently refers to lived experiences (related to identity, culture etc). Caveat: although we collect data on race and sex as if delineation in these categories are more “immutable” or “factual”, many have/do argue that these concepts are also social constructs (see discussion on false binaries and false aggregtaion)! When you ask for “sex” are you asking about a person’s chromosomal information, becaues you’re studying chromosomal related diseases, or are you asking about their experiences with the US medical system and how their health outcome is affected by their gendery identity OR the identity that people tend to assume of them? For example: women, especially women of color, tend to be under-diagnosed and mis-diagnosed due to gendered treatments of patients. To complicate matters, the terms “gender” and “sex” can have very specific legal definitions – that may not align with defintions in medical or sociological contexts! For example, the language of the Equal Credit Opportunity Act is written in terms of “sex”, however, this term is not defined in the ECOA. The Consumer Financial Protection Bureau interprets (in their operating philosophy) this term to include both sexual orientation and gender identity. Did you already bias the answer? If you gave confusing, misleading or incorrect options for answers (think about when you were frustrated with the concept quizzes!), then the covariates you collect are at best unpredictive, and at worse they can mask the signal in the data. In many data collection processes, one finds that the respondent is forced into a category that is unrepresentative of their experience, like having “Asian” as a broad racial category that includes South Asian, East Asian, South East Asian, like providing binary choices for gender or sex. If you are looking for disparaties in health outcomes, you might miss disparaties amongst important smaller subgroups within broad group of “Asian”, since health outcomes are often affected by income and Asian-Americans have the largest in-group income disparity. Should you be asking for this information? The complexity of asking a seemingly simple question can seem overwhelming, often we resort to over-collecting (collecting every piece of information that we can think of) or under-collecting (not collecting any sensitive attribute). Why you might not want to But respondents (e.g. patients, applicants for benefits) can be suspicious that their data can be used precisely to discriminate against them on the basis of protected attributes. Note that data collection have often historically been used to oppress, not to up-lift, vunerable communities. So collecting this data without clear purpose can compromise user trust and constitute a privacy threat if this data is not well protected. Why you should However, in order to make sure our model is not disproportionately negatively impacting vunerable subgroups and that we are in compliancenb with laws like the EOCA, we need to check for disparate impact on protected groups. But in order to do that, we need to have access to sensitive attributes. Is correlation causation? It appears that the sex of the applicant the logistic regression model has the greatest influence on the prediction of the model, does this mean that the human decision makers were biased? (Hint: not necssarily, but it might be a hypothesis to explore to retroactively detect potential regulatory violation in this dataset.) If the interpretations of the coefficients of the logistic regression model does not imply human bias in the data, then can we use this model for real life loan decisions? (Hint: the answer is NO. It would be an violation!) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
