<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 The Math of Training and Interpreting Logistic Regression Models | Notes for CS181: Machine Learning</title>
  <meta name="description" content="This is a set of note for CS181: Machine Learning (Spring 2023)" />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 The Math of Training and Interpreting Logistic Regression Models | Notes for CS181: Machine Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a set of note for CS181: Machine Learning (Spring 2023)" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 The Math of Training and Interpreting Logistic Regression Models | Notes for CS181: Machine Learning" />
  
  <meta name="twitter:description" content="This is a set of note for CS181: Machine Learning (Spring 2023)" />
  

<meta name="author" content="Weiwei Pan" />


<meta name="date" content="2023-05-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="case-study-responsibly-using-logistic-regression.html"/>
<link rel="next" href="what-are-neural-networks.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> What is CS181?</a>
<ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#why-is-ai-a-big-deal"><i class="fa fa-check"></i><b>2.1</b> Why Is AI a Big Deal?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="intro.html"><a href="intro.html#but-is-accuracy-enough"><i class="fa fa-check"></i><b>2.1.1</b> But Is Accuracy Enough?</a></li>
<li class="chapter" data-level="2.1.2" data-path="intro.html"><a href="intro.html#what-happens-when-machine-learning-models-are-catastrophically-wrong"><i class="fa fa-check"></i><b>2.1.2</b> What Happens When Machine Learning Models are Catastrophically Wrong?</a></li>
<li class="chapter" data-level="2.1.3" data-path="intro.html"><a href="intro.html#are-machine-models-right-for-the-right-reasons"><i class="fa fa-check"></i><b>2.1.3</b> Are Machine Models Right for the Right Reasons?</a></li>
<li class="chapter" data-level="2.1.4" data-path="intro.html"><a href="intro.html#what-is-the-role-of-the-human-decision-maker"><i class="fa fa-check"></i><b>2.1.4</b> What is the Role of the Human Decision Maker?</a></li>
<li class="chapter" data-level="2.1.5" data-path="intro.html"><a href="intro.html#what-are-the-broader-impacts-of-tech"><i class="fa fa-check"></i><b>2.1.5</b> What are the Broader Impacts of Tech?</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#machine-learning-is-much-more-than-accuracy"><i class="fa fa-check"></i><b>2.2</b> Machine Learning is Much More Than Accuracy</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#what-is-cs181"><i class="fa fa-check"></i><b>2.3</b> What is CS181?</a></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#what-we-are-offering-you"><i class="fa fa-check"></i><b>2.4</b> What We are Offering You</a></li>
<li class="chapter" data-level="2.5" data-path="intro.html"><a href="intro.html#what-we-are-asking-from-you"><i class="fa fa-check"></i><b>2.5</b> What We are Asking From You</a></li>
<li class="chapter" data-level="2.6" data-path="intro.html"><a href="intro.html#grading-evaluation"><i class="fa fa-check"></i><b>2.6</b> Grading &amp; Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="what-is-regression.html"><a href="what-is-regression.html"><i class="fa fa-check"></i><b>3</b> What is Regression?</a>
<ul>
<li class="chapter" data-level="3.1" data-path="what-is-regression.html"><a href="what-is-regression.html#what-is-machine-learning"><i class="fa fa-check"></i><b>3.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="3.2" data-path="what-is-regression.html"><a href="what-is-regression.html#what-is-regression-1"><i class="fa fa-check"></i><b>3.2</b> What is Regression?</a></li>
<li class="chapter" data-level="3.3" data-path="what-is-regression.html"><a href="what-is-regression.html#almost-everything-is-linear-regression"><i class="fa fa-check"></i><b>3.3</b> (Almost) Everything is Linear Regression</a></li>
<li class="chapter" data-level="3.4" data-path="what-is-regression.html"><a href="what-is-regression.html#what-is-model-evaluation"><i class="fa fa-check"></i><b>3.4</b> What is Model Evaluation?</a></li>
<li class="chapter" data-level="3.5" data-path="what-is-regression.html"><a href="what-is-regression.html#what-is-model-critique"><i class="fa fa-check"></i><b>3.5</b> What is Model Critique?</a></li>
<li class="chapter" data-level="3.6" data-path="what-is-regression.html"><a href="what-is-regression.html#limitations-and-connections"><i class="fa fa-check"></i><b>3.6</b> Limitations and Connections</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="what-are-probablistic-and-non-probablistic-regression.html"><a href="what-are-probablistic-and-non-probablistic-regression.html"><i class="fa fa-check"></i><b>4</b> What are Probablistic and Non-Probablistic Regression?</a>
<ul>
<li class="chapter" data-level="4.1" data-path="what-are-probablistic-and-non-probablistic-regression.html"><a href="what-are-probablistic-and-non-probablistic-regression.html#what-is-probabilistic-regression"><i class="fa fa-check"></i><b>4.1</b> What is Probabilistic Regression?</a></li>
<li class="chapter" data-level="4.2" data-path="what-are-probablistic-and-non-probablistic-regression.html"><a href="what-are-probablistic-and-non-probablistic-regression.html#almost-everything-is-linear-regression-1"><i class="fa fa-check"></i><b>4.2</b> (Almost) Everything is Linear Regression</a></li>
<li class="chapter" data-level="4.3" data-path="what-are-probablistic-and-non-probablistic-regression.html"><a href="what-are-probablistic-and-non-probablistic-regression.html#the-cube-a-model-comparison-paradigm"><i class="fa fa-check"></i><b>4.3</b> The Cube: A Model Comparison Paradigm</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html"><i class="fa fa-check"></i><b>5</b> What Matters in ML Besides Prediction?</a>
<ul>
<li class="chapter" data-level="5.1" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#what-is-machine-learning-revisited"><i class="fa fa-check"></i><b>5.1</b> What is Machine Learning? Revisited</a></li>
<li class="chapter" data-level="5.2" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#what-are-we-uncertain-about"><i class="fa fa-check"></i><b>5.2</b> What Are We Uncertain About?</a></li>
<li class="chapter" data-level="5.3" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#where-is-uncertainty-coming-from"><i class="fa fa-check"></i><b>5.3</b> Where is Uncertainty Coming From?</a></li>
<li class="chapter" data-level="5.4" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#how-do-we-compute-uncertainty"><i class="fa fa-check"></i><b>5.4</b> How Do We Compute Uncertainty?</a></li>
<li class="chapter" data-level="5.5" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#mathematizing-uncertainty-starting-with-bias-and-variance"><i class="fa fa-check"></i><b>5.5</b> Mathematizing Uncertainty: Starting with Bias and Variance</a></li>
<li class="chapter" data-level="5.6" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#the-bias-variance-trade-off-in-machine-learning"><i class="fa fa-check"></i><b>5.6</b> The Bias-Variance Trade-off in Machine Learning</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#examples-of-the-bias-variance-trade-off"><i class="fa fa-check"></i><b>5.6.1</b> Examples of the Bias-Variance Trade-off</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html"><i class="fa fa-check"></i><b>6</b> What is Logistic Regression?</a>
<ul>
<li class="chapter" data-level="6.1" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html#logistic-regression-and-soft-classification"><i class="fa fa-check"></i><b>6.1</b> Logistic Regression and Soft-Classification</a></li>
<li class="chapter" data-level="6.2" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html#logistic-regression-and-bernoulli-likelihood"><i class="fa fa-check"></i><b>6.2</b> Logistic Regression and Bernoulli Likelihood</a></li>
<li class="chapter" data-level="6.3" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html#how-to-perform-maximum-likelihood-inference-for-logistic-regression"><i class="fa fa-check"></i><b>6.3</b> How to Perform Maximum Likelihood Inference for Logistic Regression</a></li>
<li class="chapter" data-level="6.4" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html#how-not-to-evaluate-classifiers"><i class="fa fa-check"></i><b>6.4</b> How (Not) to Evaluate Classifiers</a></li>
<li class="chapter" data-level="6.5" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html#how-to-interpret-logistic-regression"><i class="fa fa-check"></i><b>6.5</b> How to Interpret Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="how-do-we-responsibly-use-conditional-models.html"><a href="how-do-we-responsibly-use-conditional-models.html"><i class="fa fa-check"></i><b>7</b> How Do We Responsibly Use Conditional Models?</a>
<ul>
<li class="chapter" data-level="7.1" data-path="how-do-we-responsibly-use-conditional-models.html"><a href="how-do-we-responsibly-use-conditional-models.html#everything-weve-done-so-far-in-probabilistic-ml"><i class="fa fa-check"></i><b>7.1</b> Everything We’ve Done So Far in Probabilistic ML</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="case-study-responsibly-using-logistic-regression.html"><a href="case-study-responsibly-using-logistic-regression.html"><i class="fa fa-check"></i><b>8</b> Case Study: Responsibly Using Logistic Regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="case-study-responsibly-using-logistic-regression.html"><a href="case-study-responsibly-using-logistic-regression.html#case-study-machine-learning-model-for-loan-approval"><i class="fa fa-check"></i><b>8.1</b> Case Study: Machine Learning Model for Loan Approval</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="case-study-responsibly-using-logistic-regression.html"><a href="case-study-responsibly-using-logistic-regression.html#the-big-vague-question"><i class="fa fa-check"></i><b>8.1.1</b> The Big Vague Question</a></li>
<li class="chapter" data-level="8.1.2" data-path="case-study-responsibly-using-logistic-regression.html"><a href="case-study-responsibly-using-logistic-regression.html#the-concrete-and-rigorous-process-of-post-inference-analysis-of-machine-learning-models"><i class="fa fa-check"></i><b>8.1.2</b> The Concrete and Rigorous Process of Post-Inference Analysis of Machine Learning Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html"><i class="fa fa-check"></i><b>9</b> The Math of Training and Interpreting Logistic Regression Models</a>
<ul>
<li class="chapter" data-level="9.1" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#the-math-of-convex-optimization"><i class="fa fa-check"></i><b>9.1</b> The Math of Convex Optimization</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#convexity-of-the-logistic-regression-negative-log-likelihood"><i class="fa fa-check"></i><b>9.1.1</b> Convexity of the Logistic Regression Negative Log-Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#important-mathy-details-of-gradient-descent"><i class="fa fa-check"></i><b>9.2</b> Important Mathy Details of Gradient Descent</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#does-it-converge"><i class="fa fa-check"></i><b>9.2.1</b> Does It Converge?</a></li>
<li class="chapter" data-level="9.2.2" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#how-quickly-can-we-get-there"><i class="fa fa-check"></i><b>9.2.2</b> How Quickly Can We Get There?</a></li>
<li class="chapter" data-level="9.2.3" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#does-it-scale"><i class="fa fa-check"></i><b>9.2.3</b> Does It Scale?</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#interpreting-a-logistic-regression-model-log-odds"><i class="fa fa-check"></i><b>9.3</b> Interpreting a Logistic Regression Model: Log-Odds</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html"><i class="fa fa-check"></i><b>10</b> What are Neural Networks?</a>
<ul>
<li class="chapter" data-level="10.1" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#neural-network-as-universal-function-approximators"><i class="fa fa-check"></i><b>10.1</b> Neural Network as Universal Function Approximators</a></li>
<li class="chapter" data-level="10.2" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#neural-networks-as-regression-on-learned-feature-map"><i class="fa fa-check"></i><b>10.2</b> Neural Networks as Regression on Learned Feature Map</a></li>
<li class="chapter" data-level="10.3" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#everything-is-a-neural-network"><i class="fa fa-check"></i><b>10.3</b> Everything is a Neural Network</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#architecture-zoo"><i class="fa fa-check"></i><b>10.3.1</b> Architecture Zoo</a></li>
<li class="chapter" data-level="10.3.2" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#chatgpt"><i class="fa fa-check"></i><b>10.3.2</b> ChatGPT</a></li>
<li class="chapter" data-level="10.3.3" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#stable-diffusion"><i class="fa fa-check"></i><b>10.3.3</b> Stable Diffusion</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#neural-network-optimization"><i class="fa fa-check"></i><b>10.4</b> Neural Network Optimization</a></li>
<li class="chapter" data-level="10.5" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#bias-variance-trade-off-for-neural-networks"><i class="fa fa-check"></i><b>10.5</b> Bias-Variance Trade-off for Neural Networks</a></li>
<li class="chapter" data-level="10.6" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#interpretation-of-neural-networks"><i class="fa fa-check"></i><b>10.6</b> Interpretation of Neural Networks</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-1-can-neural-network-models-make-use-of-human-concepts"><i class="fa fa-check"></i><b>10.6.1</b> Example 1: Can Neural Network Models Make Use of Human Concepts?</a></li>
<li class="chapter" data-level="10.6.2" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-2-can-neural-network-models-learn-to-explore-hypothetical-scenarios"><i class="fa fa-check"></i><b>10.6.2</b> Example 2: Can Neural Network Models Learn to Explore Hypothetical Scenarios?</a></li>
<li class="chapter" data-level="10.6.3" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-3-a-powerful-generalization-of-feature-importance-for-neural-network-models"><i class="fa fa-check"></i><b>10.6.3</b> Example 3: A Powerful Generalization of Feature Importance for Neural Network Models</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#the-difficulty-with-interpretable-machine-learning"><i class="fa fa-check"></i><b>10.7</b> The Difficulty with Interpretable Machine Learning</a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-4-not-all-explanations-are-created-equal"><i class="fa fa-check"></i><b>10.7.1</b> Example 4: Not All Explanations are Created Equal</a></li>
<li class="chapter" data-level="10.7.2" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-5-explanations-can-lie"><i class="fa fa-check"></i><b>10.7.2</b> Example 5: Explanations Can Lie</a></li>
<li class="chapter" data-level="10.7.3" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-6-the-perils-of-explanations-in-socio-technical-systems"><i class="fa fa-check"></i><b>10.7.3</b> Example 6: The Perils of Explanations in Socio-Technical Systems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html"><i class="fa fa-check"></i><b>11</b> The Math and Interpretation of Neural Network Models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#neural-networks-regression"><i class="fa fa-check"></i><b>11.1</b> Neural Networks Regression</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#why-its-hard-to-differentiate-a-neural-network"><i class="fa fa-check"></i><b>11.1.1</b> Why It’s Hard to Differentiate a Neural Network</a></li>
<li class="chapter" data-level="11.1.2" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#differentiating-neural-networks-backpropagation"><i class="fa fa-check"></i><b>11.1.2</b> Differentiating Neural Networks: Backpropagation</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#interpreting-neural-networks"><i class="fa fa-check"></i><b>11.2</b> Interpreting Neural Networks</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#example-1-can-neural-network-models-make-use-of-human-concepts-1"><i class="fa fa-check"></i><b>11.2.1</b> Example 1: Can Neural Network Models Make Use of Human Concepts?</a></li>
<li class="chapter" data-level="11.2.2" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#example-2-can-neural-network-models-learn-to-explore-hypothetical-scenarios-1"><i class="fa fa-check"></i><b>11.2.2</b> Example 2: Can Neural Network Models Learn to Explore Hypothetical Scenarios?</a></li>
<li class="chapter" data-level="11.2.3" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#example-3-a-powerful-generalization-of-feature-importance-for-neural-network-models-1"><i class="fa fa-check"></i><b>11.2.3</b> Example 3: A Powerful Generalization of Feature Importance for Neural Network Models</a></li>
<li class="chapter" data-level="11.2.4" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#example-4-the-perils-of-explanations"><i class="fa fa-check"></i><b>11.2.4</b> Example 4: The Perils of Explanations</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#neural-network-models-and-generalization"><i class="fa fa-check"></i><b>11.3</b> Neural Network Models and Generalization</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="the-math-behind-bayesian-regression.html"><a href="the-math-behind-bayesian-regression.html"><i class="fa fa-check"></i><b>12</b> The Math Behind Bayesian Regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="the-math-behind-bayesian-regression.html"><a href="the-math-behind-bayesian-regression.html#bayesian-linear-regression"><i class="fa fa-check"></i><b>12.1</b> Bayesian Linear Regression</a></li>
<li class="chapter" data-level="12.2" data-path="the-math-behind-bayesian-regression.html"><a href="the-math-behind-bayesian-regression.html#bayesian-linear-regression-over-arbitrary-bases"><i class="fa fa-check"></i><b>12.2</b> Bayesian Linear Regression over Arbitrary Bases</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="bayesian-modeling-framework.html"><a href="bayesian-modeling-framework.html"><i class="fa fa-check"></i><b>13</b> Bayesian Modeling Framework</a>
<ul>
<li class="chapter" data-level="13.1" data-path="bayesian-modeling-framework.html"><a href="bayesian-modeling-framework.html#components-of-machine-learning-reasoning"><i class="fa fa-check"></i><b>13.1</b> Components of Machine Learning Reasoning</a></li>
<li class="chapter" data-level="13.2" data-path="bayesian-modeling-framework.html"><a href="bayesian-modeling-framework.html#bayesian-modeling-paradigm"><i class="fa fa-check"></i><b>13.2</b> Bayesian Modeling Paradigm</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="bayesain-vs-frequentist-inference.html"><a href="bayesain-vs-frequentist-inference.html"><i class="fa fa-check"></i><b>14</b> Bayesain vs Frequentist Inference?</a>
<ul>
<li class="chapter" data-level="14.1" data-path="bayesain-vs-frequentist-inference.html"><a href="bayesain-vs-frequentist-inference.html#the-bayesian-modeling-process"><i class="fa fa-check"></i><b>14.1</b> The Bayesian Modeling Process</a></li>
<li class="chapter" data-level="14.2" data-path="bayesain-vs-frequentist-inference.html"><a href="bayesain-vs-frequentist-inference.html#bayesian-vs-frequentist-inference"><i class="fa fa-check"></i><b>14.2</b> Bayesian vs Frequentist Inference</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html"><i class="fa fa-check"></i><b>15</b> The Math of Posterior Inference</a>
<ul>
<li class="chapter" data-level="15.1" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html#the-bayesian-modeling-process-1"><i class="fa fa-check"></i><b>15.1</b> The Bayesian Modeling Process</a></li>
<li class="chapter" data-level="15.2" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html#point-estimates-from-the-posterior"><i class="fa fa-check"></i><b>15.2</b> Point Estimates from the Posterior</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html#comparison-of-posterior-point-estimates-and-mle"><i class="fa fa-check"></i><b>15.2.1</b> Comparison of Posterior Point Estimates and MLE</a></li>
<li class="chapter" data-level="15.2.2" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html#law-of-large-numbers-for-bayesian-inference"><i class="fa fa-check"></i><b>15.2.2</b> Law of Large Numbers for Bayesian Inference</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html#bayesian-logistic-regression"><i class="fa fa-check"></i><b>15.3</b> Bayesian Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="whats-hard-about-sampling.html"><a href="whats-hard-about-sampling.html"><i class="fa fa-check"></i><b>16</b> What’s Hard About Sampling?</a>
<ul>
<li class="chapter" data-level="16.1" data-path="whats-hard-about-sampling.html"><a href="whats-hard-about-sampling.html#bayesian-vs-frequentist-inference-1"><i class="fa fa-check"></i><b>16.1</b> Bayesian vs Frequentist Inference</a></li>
<li class="chapter" data-level="16.2" data-path="whats-hard-about-sampling.html"><a href="whats-hard-about-sampling.html#what-is-sampling-and-why-do-we-care"><i class="fa fa-check"></i><b>16.2</b> What is Sampling and Why do We Care?</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html"><i class="fa fa-check"></i><b>17</b> The Math of Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="17.1" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#pca-as-dimensionality-reduction-to-maximize-variance"><i class="fa fa-check"></i><b>17.1</b> PCA as Dimensionality Reduction to Maximize Variance</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#finding-a-single-pca-component"><i class="fa fa-check"></i><b>17.1.1</b> Finding a Single PCA Component</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#pca-as-dimensionality-reduction-to-minimize-reconstruction-loss"><i class="fa fa-check"></i><b>17.2</b> PCA as Dimensionality Reduction to Minimize Reconstruction Loss</a></li>
<li class="chapter" data-level="17.3" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#a-latent-variable-model-for-pca"><i class="fa fa-check"></i><b>17.3</b> A Latent Variable Model for PCA</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#one-principle-component"><i class="fa fa-check"></i><b>17.3.1</b> One Principle Component</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#autoencoders-and-nonlinear-pca"><i class="fa fa-check"></i><b>17.4</b> Autoencoders and Nonlinear PCA</a></li>
<li class="chapter" data-level="17.5" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#a-probabilistic-latent-variable-model-for-pca"><i class="fa fa-check"></i><b>17.5</b> A Probabilistic Latent Variable Model for PCA</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="the-math-of-expectation-maximization.html"><a href="the-math-of-expectation-maximization.html"><i class="fa fa-check"></i><b>18</b> The Math of Expectation Maximization</a></li>
<li class="chapter" data-level="19" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html"><i class="fa fa-check"></i><b>19</b> Motivation for Latent Variable Models</a>
<ul>
<li class="chapter" data-level="19.1" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#latent-variable-models"><i class="fa fa-check"></i><b>19.1</b> Latent Variable Models</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#example-gaussian-mixture-models-gmms"><i class="fa fa-check"></i><b>19.1.1</b> Example: Gaussian Mixture Models (GMMs)</a></li>
<li class="chapter" data-level="19.1.2" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#item-response-models"><i class="fa fa-check"></i><b>19.1.2</b> Item-Response Models</a></li>
<li class="chapter" data-level="19.1.3" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#example-factor-analysis-models"><i class="fa fa-check"></i><b>19.1.3</b> Example: Factor Analysis Models</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#maximum-likelihood-estimation-for-latent-variable-models-expectation-maximization"><i class="fa fa-check"></i><b>19.2</b> Maximum Likelihood Estimation for Latent Variable Models: Expectation Maximization</a></li>
<li class="chapter" data-level="19.3" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#the-expectation-maximization-algorithm"><i class="fa fa-check"></i><b>19.3</b> The Expectation Maximization Algorithm</a></li>
<li class="chapter" data-level="19.4" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#monotonicity-and-convergence-of-em"><i class="fa fa-check"></i><b>19.4</b> Monotonicity and Convergence of EM</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html"><i class="fa fa-check"></i><b>20</b> Review of Latent Variables, Compression and Clustering</a>
<ul>
<li class="chapter" data-level="20.0.1" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#example-gaussian-mixture-models-gmms-1"><i class="fa fa-check"></i><b>20.0.1</b> Example: Gaussian Mixture Models (GMMs)</a></li>
<li class="chapter" data-level="20.0.2" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#example-item-response-models"><i class="fa fa-check"></i><b>20.0.2</b> Example: Item-Response Models</a></li>
<li class="chapter" data-level="20.0.3" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#example-factor-analysis-models-1"><i class="fa fa-check"></i><b>20.0.3</b> Example: Factor Analysis Models</a></li>
<li class="chapter" data-level="20.1" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#pca-versus-probabilistic-pca-ppca"><i class="fa fa-check"></i><b>20.1</b> PCA Versus Probabilistic PCA (pPCA)</a>
<ul>
<li class="chapter" data-level="20.1.1" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#what-to-know-about-expectation-maximization"><i class="fa fa-check"></i><b>20.1.1</b> What to Know About Expectation Maximization</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#non-probabilistic-clustering-versus-probabilistic-clustering"><i class="fa fa-check"></i><b>20.2</b> Non-Probabilistic Clustering Versus Probabilistic Clustering</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="topic-models.html"><a href="topic-models.html"><i class="fa fa-check"></i><b>21</b> Topic Models</a>
<ul>
<li class="chapter" data-level="21.1" data-path="topic-models.html"><a href="topic-models.html#our-first-latent-variable-model"><i class="fa fa-check"></i><b>21.1</b> Our First Latent Variable Model</a></li>
<li class="chapter" data-level="21.2" data-path="topic-models.html"><a href="topic-models.html#reasoning-about-text-corpa-using-topic-modeling"><i class="fa fa-check"></i><b>21.2</b> Reasoning About Text Corpa Using Topic Modeling</a></li>
<li class="chapter" data-level="21.3" data-path="topic-models.html"><a href="topic-models.html#our-second-latent-variable-model-plsa"><i class="fa fa-check"></i><b>21.3</b> Our Second Latent Variable Model: pLSA</a></li>
<li class="chapter" data-level="21.4" data-path="topic-models.html"><a href="topic-models.html#our-third-latent-variable-model-lda"><i class="fa fa-check"></i><b>21.4</b> Our Third Latent Variable Model: LDA</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html"><i class="fa fa-check"></i><b>22</b> Math and Intuition of Hidden Markov Models</a>
<ul>
<li class="chapter" data-level="22.1" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html#markov-models"><i class="fa fa-check"></i><b>22.1</b> Markov Models</a>
<ul>
<li class="chapter" data-level="22.1.1" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html#transition-matrices-and-kernels"><i class="fa fa-check"></i><b>22.1.1</b> Transition Matrices and Kernels</a></li>
<li class="chapter" data-level="22.1.2" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html#applications-of-markov-models"><i class="fa fa-check"></i><b>22.1.2</b> Applications of Markov Models</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html#hidden-markov-models"><i class="fa fa-check"></i><b>22.2</b> Hidden Markov Models</a></li>
<li class="chapter" data-level="22.3" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html#learning-and-inference-for-hmms"><i class="fa fa-check"></i><b>22.3</b> Learning and Inference for HMMs</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html"><i class="fa fa-check"></i><b>23</b> The Intuition of Markov Decision Processes</a>
<ul>
<li class="chapter" data-level="23.1" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#review-modeling-sequential-data"><i class="fa fa-check"></i><b>23.1</b> Review: Modeling Sequential Data</a>
<ul>
<li class="chapter" data-level="23.1.1" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#why-model-sequential-data-dynamics"><i class="fa fa-check"></i><b>23.1.1</b> Why Model Sequential Data (Dynamics)?</a></li>
</ul></li>
<li class="chapter" data-level="23.2" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#modeling-sequential-data-and-sequential-actions"><i class="fa fa-check"></i><b>23.2</b> Modeling Sequential Data and Sequential Actions</a>
<ul>
<li class="chapter" data-level="23.2.1" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#describing-a-dynamic-world"><i class="fa fa-check"></i><b>23.2.1</b> Describing a Dynamic World</a></li>
<li class="chapter" data-level="23.2.2" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#acting-in-a-dynamic-world"><i class="fa fa-check"></i><b>23.2.2</b> Acting in a Dynamic World</a></li>
<li class="chapter" data-level="23.2.3" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#describing-worlds-as-mdps"><i class="fa fa-check"></i><b>23.2.3</b> Describing Worlds as MDP’s</a></li>
</ul></li>
<li class="chapter" data-level="23.3" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#modeling-sequential-decisions-planning"><i class="fa fa-check"></i><b>23.3</b> Modeling Sequential Decisions: Planning</a>
<ul>
<li class="chapter" data-level="23.3.1" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#modeling-action-choice"><i class="fa fa-check"></i><b>23.3.1</b> Modeling Action Choice</a></li>
<li class="chapter" data-level="23.3.2" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#modeling-cumulative-reward"><i class="fa fa-check"></i><b>23.3.2</b> Modeling Cumulative Reward</a></li>
<li class="chapter" data-level="23.3.3" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#planning-optimizing-action-choice-for-cumulative-reward"><i class="fa fa-check"></i><b>23.3.3</b> Planning: Optimizing Action Choice for Cumulative Reward</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes for CS181: Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-math-of-training-and-interpreting-logistic-regression-models" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Chapter 9</span> The Math of Training and Interpreting Logistic Regression Models<a href="the-math-of-training-and-interpreting-logistic-regression-models.html#the-math-of-training-and-interpreting-logistic-regression-models" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><img src="https://i.imgur.com/xDR9VQd.png" /></p>
<div id="the-math-of-convex-optimization" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> The Math of Convex Optimization<a href="the-math-of-training-and-interpreting-logistic-regression-models.html#the-math-of-convex-optimization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A <strong><em>convex set</em></strong> <span class="math inline">\(S\subset \mathbb{R}^D\)</span> is a set that contains the line segment between any two points in <span class="math inline">\(S\)</span>. Formally, if <span class="math inline">\(x, y \in S\)</span> then <span class="math inline">\(S\)</span> contains all convex combinations of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>:
<span class="math display">\[
tx + (1-t) y \in S,\quad t\in [0, 1].
\]</span>
This definition is illustrated intuitively in the below:
<img src="https://i.imgur.com/pu8TTOw.jpg" /></p>
<p>A function <span class="math inline">\(f\)</span> is a <strong><em>convex function</em></strong> if domain of <span class="math inline">\(f\)</span> is a convex set, and the line segment between the points <span class="math inline">\((x, f(x))\)</span> and <span class="math inline">\((y, f(y))\)</span> lie above the graph of <span class="math inline">\(f\)</span>. Formally, for any <span class="math inline">\(x, y\in \mathrm{dom}(f)\)</span>, we have
<span class="math display">\[
\underbrace{f(tx + (1-t)y)}_{\text{height of graph of $f$}\\  \text{at a point between $x$ and $y$}} \quad \leq \underbrace{tf(x) + (1-t)f(y)}_{\text{height of point on line segment}\\ \text{between $(x, f(x))$ and $(y, f(y))$}},\quad t\in [0, 1]
\]</span></p>
<p>This definition is intuitively illustrated below:
<img src="https://i.imgur.com/3AXnm2n.jpg" /></p>
<p>How do we check that a function <span class="math inline">\(f\)</span> is convex? If <span class="math inline">\(f\)</span> is differentiable then <span class="math inline">\(f\)</span> is convex if the graph of <span class="math inline">\(f\)</span> lies above every tangent plane.</p>
<p><strong>Theorem:</strong> If <span class="math inline">\(f\)</span> is differentiable then <span class="math inline">\(f\)</span> is convex if and only if for every <span class="math inline">\(x \in \mathrm{dom}(f)\)</span>, we have</p>
<p><span class="math display">\[
\underbrace{f(y)}_{\text{height of graph of $f$ over $y$}} \geq \underbrace{f(x) + \nabla f(x)^\top (y - x)}_{\text{height of plane tangent to $f$ at $x$, evaluated over $y$}},\quad \forall y\in \mathrm{dom}(f)
\]</span></p>
<p>This theorem is illustrated below. The theorem can be made intuitive, but takes a bit of unwrapping. Don’t worry about the exact statement, it suffices to know that there is a condition we can check to see if a function is convex.</p>
<p><img src="https://i.imgur.com/OhoxOkl.jpg" /></p>
<p>Luckily, checking a twice-differentiable function is convex is (relatively) easier! If <span class="math inline">\(f\)</span> is twice-differentiable then <span class="math inline">\(f\)</span> is convex if the “second derivative is positive”.</p>
<p><strong>Theorem:</strong> If <span class="math inline">\(f\)</span> is twice-differentiable then <span class="math inline">\(f\)</span> is convex if and only if the Hessian <span class="math inline">\(\nabla^2 f(x)\)</span> is positive semi-definite for every <span class="math inline">\(x\in \mathrm{dom}(f)\)</span>.</p>
<p>Verifying that a complex function <span class="math inline">\(f\)</span> is complex can be difficult (even if <span class="math inline">\(f\)</span> is twice-differentiable). More commonly, we show that a complex function is convex because it is made from simple convex functions using a set of allowed operations.</p>
<p><strong>Properties of Convex Functions.</strong> How to build complex convex functions from simple convex functions:</p>
<ol style="list-style-type: decimal">
<li><p>if <span class="math inline">\(w_1, w_2 \geq 0\)</span> and <span class="math inline">\(f_1, f_2\)</span> are convex, then <span class="math inline">\(h = w_1 f_1 + w_2 f_2\)</span> is convex<br><br></p></li>
<li><p>if <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are convex, and <span class="math inline">\(g\)</span> is univariate and non-decreasing then <span class="math inline">\(h = g \circ f\)</span> is convex<br><br></p></li>
<li><p>Log-sum-exp functions are convex: <span class="math inline">\(f(x) = \log \sum_{k=1}^K e^{x}\)</span></p></li>
</ol>
<p><strong>Note:</strong> there are many other convexity preserving operations on functions.</p>
<p>A <strong><em>convex optimization problem</em></strong> is an optimization of the following form:</p>
<span class="math display">\[\begin{aligned}
\mathrm{min}\; &amp;f(x) &amp; (\text{convex objective function})\\
\text{subject to}\; &amp; h_i(x) \leq 0, i=1, \ldots, i &amp; (\text{convex inequality constraints}) \\
&amp; a_j^\top x - b_j = 0, j=1, \ldots, J &amp; (\text{affine equality constraints}) \\
\end{aligned}\]</span>
<p>The set of points that satisfy the constraints is called the <strong><em>feasible set</em></strong>.</p>
<p>You can prove that the a convex optimization problem optimizes a convex objective function over a convex feasible set. But why should we care about convex optimization problems?</p>
<p><strong>Theorem:</strong> Let <span class="math inline">\(f\)</span> be a convex function defined over a convex feasible set <span class="math inline">\(\Omega\)</span>. Then if <span class="math inline">\(f\)</span> has a local minimum at <span class="math inline">\(x\in \Omega\)</span> – <span class="math inline">\(f(y) \geq f(x)\)</span> for <span class="math inline">\(y\)</span> in a small neighbourhood of <span class="math inline">\(x\)</span> – then <span class="math inline">\(f\)</span> has a global minimum at <span class="math inline">\(x\)</span>.</p>
<p><strong>Corollary:</strong> Let <span class="math inline">\(f\)</span> be a differentiable convex function:
1. if <span class="math inline">\(f\)</span> is unconstrained, then <span class="math inline">\(f\)</span> has a <strong>local minimum</strong> and hence <strong>global minimum</strong> at <span class="math inline">\(x\)</span> if <span class="math inline">\(\nabla f(x) = 0\)</span>.
2. if <span class="math inline">\(f\)</span> is constrained by equalities, then <span class="math inline">\(f\)</span> has a global minimum at <span class="math inline">\(x\)</span> if <span class="math inline">\(\nabla J(x, \lambda) = 0\)</span>, where <span class="math inline">\(J(x, \lambda)\)</span> is the Lagrangian of the constrained optimization problem.</p>
<p><strong>Note:</strong> we can also characterize the global minimum of inequalities constrained convex optimization problems using the Lagrangian, but the formulation is more complicated.</p>
<div id="convexity-of-the-logistic-regression-negative-log-likelihood" class="section level3 hasAnchor" number="9.1.1">
<h3><span class="header-section-number">9.1.1</span> Convexity of the Logistic Regression Negative Log-Likelihood<a href="the-math-of-training-and-interpreting-logistic-regression-models.html#convexity-of-the-logistic-regression-negative-log-likelihood" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>But why do we care about convex optimization problems? Let’s connect the theory of convex optimization to MLE inference for logistic regression. Recall that the negative log-likelihood of the logistic regression model is</p>
<span class="math display">\[\begin{aligned}
-\ell(\mathbf{w}) &amp;= -\sum_{n=1}^N y^{(n)}\,\log\,\mathrm{sigm}(\mathbf{w}^\top \mathbf{x}^{(n)}) + (1 - y^{(n)}) \log (1 -\mathrm{sigm}(\mathbf{w}^\top \mathbf{x}^{(n)}))\\
&amp;= -\sum_{n=1}^N y^{(n)}\,\log\,\frac{1}{1 + e^{-\mathbf{w}^\top \mathbf{x}^{(n)}}} + (1 - y^{(n)}) \log \left(1 -\frac{1}{1 + e^{-\mathbf{w}^\top \mathbf{x}^{(n)}}}\right)\\
&amp;= -\sum_{n=1}^N y^{(n)}\left(\log(1) - \log(1 + e^{-\mathbf{w}^\top \mathbf{x}^{(n)}})\right) + (1 - y^{(n)}) \log \left(\frac{1 + e^{-\mathbf{w}^\top \mathbf{x}^{(n)}}}{1 + e^{-\mathbf{w}^\top \mathbf{x}^{(n)}}} -\frac{1}{1 + e^{-\mathbf{w}^\top \mathbf{x}^{(n)}}}\right)\\
&amp;= -\sum_{n=1}^N -y^{(n)} \log(1 + e^{-\mathbf{w}^\top \mathbf{x}^{(n)}}) + (1 - y^{(n)}) \log \left( \frac{e^{-\mathbf{w}^\top \mathbf{x}^{(n)}}}{1 + e^{-\mathbf{w}^\top \mathbf{x}^{(n)}}}\right)\\
&amp;= -\sum_{n=1}^N -y^{(n)} \log(1 + e^{-\mathbf{w}^\top \mathbf{x}^{(n)}}) + (1 - y^{(n)}) \left(\log \left( {e^{-\mathbf{w}^\top \mathbf{x}^{(n)}}}\right) - \log\left({1 + e^{-\mathbf{w}^\top \mathbf{x}^{(n)}}}\right)\right)\\
&amp;= -\sum_{n=1}^N -y^{(n)} \log(1 + e^{-\mathbf{w}^\top \mathbf{x}^{(n)}}) + (1 - y^{(n)}) \log \left( {e^{-\mathbf{w}^\top \mathbf{x}^{(n)}}}\right) - (1 - y^{(n)})\log\left({1 + e^{-\mathbf{w}^\top \mathbf{x}^{(n)}}}\right)\\
&amp;= -\sum_{n=1}^N -y^{(n)} \log(1 + e^{-\mathbf{w}^\top \mathbf{x}^{(n)}}) + (1 - y^{(n)}) (-\mathbf{w}^\top \mathbf{x}^{(n)}) - \log\left({1 + e^{-\mathbf{w}^\top \mathbf{x}^{(n)}}}\right) + y^{(n)}\log\left({1 + e^{-\mathbf{w}^\top \mathbf{x}^{(n)}}}\right)\\
&amp;= -\sum_{n=1}^N (1 - y^{(n)}) (-\mathbf{w}^\top \mathbf{x}^{(n)}) - \log\left({1 + e^{-\mathbf{w}^\top \mathbf{x}^{(n)}}}\right) \\
&amp;= -\sum_{n=1}^N (1 - y^{(n)}) (-\mathbf{w}^\top \mathbf{x}^{(n)}) + \log\left(\frac{1}{1 + e^{-\mathbf{w}^\top \mathbf{x}^{(n)}}}\right) \\
&amp;= -\sum_{n=1}^N -\mathbf{w}^\top \mathbf{x}^{(n)} + y^{(n)}\mathbf{w}^\top \mathbf{x}^{(n)} + \log\left(\frac{1}{1 + e^{-\mathbf{w}^\top \mathbf{x}^{(n)}}}\right) \\
&amp;= -\sum_{n=1}^N \log e^{-\mathbf{w}^\top \mathbf{x}^{(n)}} + y^{(n)}\mathbf{w}^\top \mathbf{x}^{(n)} + \log\left(\frac{1}{1 + e^{-\mathbf{w}^\top \mathbf{x}^{(n)}}}\right) \\
&amp;= -\sum_{n=1}^N  y^{(n)}\mathbf{w}^\top \mathbf{x}^{(n)} + \log\left(\frac{e^{-\mathbf{w}^\top \mathbf{x}^{(n)}}}{1 + e^{-\mathbf{w}^\top \mathbf{x}^{(n)}}}\right) \\
&amp;= -\sum_{n=1}^N  y^{(n)}\mathbf{w}^\top \mathbf{x}^{(n)} + \log\left(\frac{1}{e^{\mathbf{w}^\top \mathbf{x}^{(n)}} + 1}\right) \\
&amp;=-\sum_{n=1}^N  y^{(n)}\mathbf{w}^\top \mathbf{x}^{(n)} + \log\left({1}\right) - \log \left({e^{\mathbf{w}^\top \mathbf{x}^{(n)}} + 1}\right) \\
&amp;=-\sum_{n=1}^N  y^{(n)}\mathbf{w}^\top \mathbf{x}^{(n)}  - \log \left({e^{\mathbf{w}^\top \mathbf{x}^{(n)}} + 1}\right) \\
&amp;=\sum_{n=1}^N \log \left({e^{\mathbf{w}^\top \mathbf{x}^{(n)}} + 1}\right) - y^{(n)}\mathbf{w}^\top \mathbf{x}^{(n)} \\
&amp;=\sum_{n=1}^N \log \left({e^{\mathbf{w}^\top \mathbf{x}^{(n)}} +e^{0}}\right) - y^{(n)}\mathbf{w}^\top \mathbf{x}^{(n)} \\
\end{aligned}\]</span>
<p><strong>Proposition:</strong> The negative log-likelihood of logistic regression <span class="math inline">\(-\ell(\mathbf{w})\)</span> is convex.</p>
<p><strong>What does this mean for gradient descent?</strong> If gradient descent finds that <span class="math inline">\(\mathbf{w}^*\)</span> is a stationary point of <span class="math inline">\(-\nabla_{\mathbf{w}}\ell(\mathbf{w})\)</span> then <span class="math inline">\(-\ell(\mathbf{w})\)</span> has a global minimum at <span class="math inline">\(\mathbf{w}^*\)</span>. Hence, <span class="math inline">\(\ell(\mathbf{w})\)</span> is maximized at <span class="math inline">\(\mathbf{w}^*\)</span>.</p>
<p><strong><em>Proof of the Proposition:</em></strong> Note that
1. <span class="math inline">\(- \mathbf{w}^\top \mathbf{x}^{(n)}\)</span> and <span class="math inline">\(y^{(n)}(\mathbf{w}^\top \mathbf{x}^{(n)})\)</span> are convex, since they are linear
2. <span class="math inline">\(\log(e^0 + e^{\mathbf{w}^\top \mathbf{x}^{(n)}})\)</span> is convex since it is the composition of a log-sum-exp function (which is convex) and a convex function <span class="math inline">\(\mathbf{w}^\top \mathbf{x}^{(n)}\)</span>
3. <span class="math inline">\(\sum_{n=1}^N \log(e^0 + e^{\mathbf{w}^\top \mathbf{x}^{(n)}})\)</span> is convex since it is a nonnegative linear combination of convex functions
4. <span class="math inline">\(-\ell(\mathbf{w})\)</span> is convex since it is the sum of two convex functions</p>
</div>
</div>
<div id="important-mathy-details-of-gradient-descent" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Important Mathy Details of Gradient Descent<a href="the-math-of-training-and-interpreting-logistic-regression-models.html#important-mathy-details-of-gradient-descent" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Gradient descent as an algorithm is intuitive to understand: follow the arrow pointing to the fastest way down (i.e. the negative gradient). Intuitively, it seems like this heuristic would get us to at least the bottom of a valley. But can we formally prove that gradient descent has desirable properties?</p>
<div id="does-it-converge" class="section level3 hasAnchor" number="9.2.1">
<h3><span class="header-section-number">9.2.1</span> Does It Converge?<a href="the-math-of-training-and-interpreting-logistic-regression-models.html#does-it-converge" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We’ve seen that if we choose the learning rate to be too large (say for example <code>1e10</code> then gradient descent can fail to converge even if the function <span class="math inline">\(f\)</span> is convex. But how large is “too large”. There are two cases to consider</p>
<ol style="list-style-type: decimal">
<li>You have some prior knowledge about how smooth the function <span class="math inline">\(f\)</span> is – i.e. how quickly <span class="math inline">\(f\)</span> can increase or decrease. Then using this you can choose a learning rate that will provably guaratee convergence<br><br></li>
<li>In most cases, the objective function (like the log-likelihood) may be too complex to reason about. In which case,</li>
<li>we do a scientific “guess-and-check” to determine the learning rate:
- we find a learning rate that is large enough to cause gradient descent to diverge
- we find a leanring rate that is small enough to cause gradient descent to converge too slowly
- we choose a range of values between the large rate and the small rate and try them all to determine the optimal rate<br><br></li>
<li>alternatively, we can choose the step-size <span class="math inline">\(\eta\)</span> adaptively (e.g. when the gradient is large we can set <span class="math inline">\(\eta\)</span> to be moderate to small and when the gradient is small we can set <span class="math inline">\(\eta\)</span> to be larger). There are a number of adaptive step-size regimes that you may want to look up and implement for your specific problem.</li>
</ol>
<p>The prior knowledge required to choose <span class="math inline">\(\eta\)</span> for provable convergence is called Lipschitz continuity. If we knew that <span class="math inline">\(f\)</span> is convex, differentiable and that there is a constant <span class="math inline">\(L&gt;0\)</span> such that <span class="math inline">\(\|\nabla f(x) - \nabla f(y)\|_2 \leq L\|x -y\|_2\)</span>, then if we choose a fixed step size to be <span class="math inline">\(\eta \leq \frac{1}{L}\)</span> then gradient descent <strong>provably</strong> converges to the global minimum of <span class="math inline">\(f\)</span> as the number of iterations <span class="math inline">\(N\)</span> goes to infinity. The constant <span class="math inline">\(L\)</span> is called the <strong><em>Lipschitz constant</em></strong>.</p>
</div>
<div id="how-quickly-can-we-get-there" class="section level3 hasAnchor" number="9.2.2">
<h3><span class="header-section-number">9.2.2</span> How Quickly Can We Get There?<a href="the-math-of-training-and-interpreting-logistic-regression-models.html#how-quickly-can-we-get-there" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Just because we know gradient descent will converge it doesn’t mean that it will give us a good enough approximation of the global minimum within our time limit. This is why studying the <strong><em>rate of convergence</em></strong> of gradient descent is extremely important. Again there are two cases to consider</p>
<ol style="list-style-type: decimal">
<li><p>You have prior knowledge that <span class="math inline">\(f\)</span> is convex, differentiable and its Lipschitz constant is <span class="math inline">\(L\)</span> and suppose that <span class="math inline">\(f\)</span> has a global minimum at <span class="math inline">\(x^*\)</span>, then for gradient descent to get within <span class="math inline">\(\epsilon\)</span> of <span class="math inline">\(f(x^*)\)</span>, we need <span class="math inline">\(O(1/\epsilon)\)</span> number of iterations.</p></li>
<li><p>In most cases, the objective function will fail to be convex and its Lipschitz constant may be too difficult to compute. In this case, we simply stop the gradient descent when the gradient is sufficiently small.</p></li>
</ol>
</div>
<div id="does-it-scale" class="section level3 hasAnchor" number="9.2.3">
<h3><span class="header-section-number">9.2.3</span> Does It Scale?<a href="the-math-of-training-and-interpreting-logistic-regression-models.html#does-it-scale" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Gradient descent is such a simple algorithm that can be applied to <strong>any optimization problem</strong> for which you can compute the gradient of the objective function.</p>
<p><strong>Question:</strong> Does this mean that maximum likelihood inference for statistical models is now an easy task (i.e. just use gradient descent)?</p>
<p>For every likelihood optimization problem, evaluating the gradient at a set of parameters <span class="math inline">\(\mathbf{w}\)</span> requires evaluating the likelihood of the entire dataset using <span class="math inline">\(\mathbf{w}\)</span>:</p>
<p><span class="math display">\[
\nabla_{\mathbf{w}} \ell(\mathbf{w}) = -\sum_{n=1}^N \left(y^{(n)} - \frac{1}{1 + e^{-\mathbf{w}^\top\mathbf{x}^{(n)}}} \right) \mathbf{x}^{(n)} =\mathbf{0}
\]</span></p>
<p>Imagine if the size of your dataset <span class="math inline">\(N\)</span> is in the millions. Naively evaluating the gardient <strong>just once</strong> may take up to seconds or minutes, thus running gradient descent until convergence may be unachievable in practice!</p>
<p><strong>Idea:</strong> Maybe we don’t need to use the entire data set to evaluate the gradient during each step of gradient descent. Maybe we can approximate the gradient at <span class="math inline">\(\mathbf{w}\)</span> well enough with just a subset of the data.</p>
</div>
</div>
<div id="interpreting-a-logistic-regression-model-log-odds" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Interpreting a Logistic Regression Model: Log-Odds<a href="the-math-of-training-and-interpreting-logistic-regression-models.html#interpreting-a-logistic-regression-model-log-odds" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A more formal way of interpreting the parameters of the logistic regression model is through the <strong><em>log-odds</em></strong>. That is, we solve for <span class="math inline">\(\mathbf{w}^\top\mathbf{x}\)</span> in terms of <span class="math inline">\(\text{Prob}(y = 1 | \mathbf{x})\)</span>.</p>
<span class="math display">\[\begin{aligned}
\text{Prob}(y = 1 | \mathbf{x}) &amp;= \text{sigm}(\mathbf{w}^\top\mathbf{x})\\
\text{sigm}^{-1}(\text{Prob}(y = 1 | \mathbf{x})) &amp;= \mathbf{w}^\top\mathbf{x}\\
\log \left( \frac{\text{Prob}(y = 1 | \mathbf{x})}{1 - \text{Prob}(y = 1 | \mathbf{x})}\right)&amp;= \mathbf{w}^\top\mathbf{x}\\
\log \left( \frac{\text{Prob}(y = 1 | \mathbf{x})}{\text{Prob}(y = 0 | \mathbf{x})}\right)&amp;= \mathbf{w}^\top\mathbf{x}
\end{aligned}\]</span>
<p>where we used the fact that <span class="math inline">\(\text{sigm}^{-1}(z) = \log\left(\frac{z}{1 - z}\right)\)</span>.</p>
<p>The term <span class="math inline">\(\log \left( \frac{\text{Prob}(y = 1 | \mathbf{x})}{\text{Prob}(y = 0 | \mathbf{x})}\right)\)</span> is essentially a ratio of the probability of <span class="math inline">\(y=1\)</span> and the probability of <span class="math inline">\(y=0\)</span>, we can interpret this quantity as the (log) odds of you winning if you’d bet that <span class="math inline">\(y=1\)</span>. Thus, we can imagine the parameter <span class="math inline">\(w_d\)</span> for the covariate <span class="math inline">\(x_d\)</span> as telling us if the odd of winning a bet on <span class="math inline">\(y=1\)</span> is good.</p>
<ol style="list-style-type: decimal">
<li><p>if <span class="math inline">\(w_d &lt; 0\)</span>, then by increasing <span class="math inline">\(x_d\)</span> (while holding all other covariates constant) we make <span class="math inline">\(\mathbf{w}^\top\mathbf{x}\)</span> more negative, and hence the ratio <span class="math inline">\(\frac{\text{Prob}(y = 1 | \mathbf{x})}{\text{Prob}(y = 0 | \mathbf{x})}\)</span> closer to 0. That is, when <span class="math inline">\(w_d &lt; 0\)</span>, increasing <span class="math inline">\(x_d\)</span> decreases our odds.<br><br></p></li>
<li><p>if <span class="math inline">\(w_d &gt; 0\)</span>, then by increasing <span class="math inline">\(x_d\)</span> (while holding all other covariates constant) we make <span class="math inline">\(\mathbf{w}^\top\mathbf{x}\)</span> more positive, and hence the ratio <span class="math inline">\(\frac{\text{Prob}(y = 1 | \mathbf{x})}{\text{Prob}(y = 0 | \mathbf{x})}\)</span> larger. That is, when <span class="math inline">\(w_d &gt; 0\)</span>, increasing <span class="math inline">\(x_d\)</span> increases our odds.</p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="case-study-responsibly-using-logistic-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="what-are-neural-networks.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/08-Math-of-Logistic-Regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
