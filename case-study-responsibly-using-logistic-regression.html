<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Case Study: Responsibly Using Logistic Regression | Notes for CS181: Machine Learning</title>
  <meta name="description" content="This is a set of note for CS181: Machine Learning (Spring 2023)" />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Case Study: Responsibly Using Logistic Regression | Notes for CS181: Machine Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a set of note for CS181: Machine Learning (Spring 2023)" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Case Study: Responsibly Using Logistic Regression | Notes for CS181: Machine Learning" />
  
  <meta name="twitter:description" content="This is a set of note for CS181: Machine Learning (Spring 2023)" />
  

<meta name="author" content="Weiwei Pan" />


<meta name="date" content="2023-05-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="how-do-we-responsibly-use-conditional-models.html"/>
<link rel="next" href="the-math-of-training-and-interpreting-logistic-regression-models.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> What is CS181?</a>
<ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#why-is-ai-a-big-deal"><i class="fa fa-check"></i><b>2.1</b> Why Is AI a Big Deal?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="intro.html"><a href="intro.html#but-is-accuracy-enough"><i class="fa fa-check"></i><b>2.1.1</b> But Is Accuracy Enough?</a></li>
<li class="chapter" data-level="2.1.2" data-path="intro.html"><a href="intro.html#what-happens-when-machine-learning-models-are-catastrophically-wrong"><i class="fa fa-check"></i><b>2.1.2</b> What Happens When Machine Learning Models are Catastrophically Wrong?</a></li>
<li class="chapter" data-level="2.1.3" data-path="intro.html"><a href="intro.html#are-machine-models-right-for-the-right-reasons"><i class="fa fa-check"></i><b>2.1.3</b> Are Machine Models Right for the Right Reasons?</a></li>
<li class="chapter" data-level="2.1.4" data-path="intro.html"><a href="intro.html#what-is-the-role-of-the-human-decision-maker"><i class="fa fa-check"></i><b>2.1.4</b> What is the Role of the Human Decision Maker?</a></li>
<li class="chapter" data-level="2.1.5" data-path="intro.html"><a href="intro.html#what-are-the-broader-impacts-of-tech"><i class="fa fa-check"></i><b>2.1.5</b> What are the Broader Impacts of Tech?</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#machine-learning-is-much-more-than-accuracy"><i class="fa fa-check"></i><b>2.2</b> Machine Learning is Much More Than Accuracy</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#what-is-cs181"><i class="fa fa-check"></i><b>2.3</b> What is CS181?</a></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#what-we-are-offering-you"><i class="fa fa-check"></i><b>2.4</b> What We are Offering You</a></li>
<li class="chapter" data-level="2.5" data-path="intro.html"><a href="intro.html#what-we-are-asking-from-you"><i class="fa fa-check"></i><b>2.5</b> What We are Asking From You</a></li>
<li class="chapter" data-level="2.6" data-path="intro.html"><a href="intro.html#grading-evaluation"><i class="fa fa-check"></i><b>2.6</b> Grading &amp; Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="what-is-regression.html"><a href="what-is-regression.html"><i class="fa fa-check"></i><b>3</b> What is Regression?</a>
<ul>
<li class="chapter" data-level="3.1" data-path="what-is-regression.html"><a href="what-is-regression.html#what-is-machine-learning"><i class="fa fa-check"></i><b>3.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="3.2" data-path="what-is-regression.html"><a href="what-is-regression.html#what-is-regression-1"><i class="fa fa-check"></i><b>3.2</b> What is Regression?</a></li>
<li class="chapter" data-level="3.3" data-path="what-is-regression.html"><a href="what-is-regression.html#almost-everything-is-linear-regression"><i class="fa fa-check"></i><b>3.3</b> (Almost) Everything is Linear Regression</a></li>
<li class="chapter" data-level="3.4" data-path="what-is-regression.html"><a href="what-is-regression.html#what-is-model-evaluation"><i class="fa fa-check"></i><b>3.4</b> What is Model Evaluation?</a></li>
<li class="chapter" data-level="3.5" data-path="what-is-regression.html"><a href="what-is-regression.html#what-is-model-critique"><i class="fa fa-check"></i><b>3.5</b> What is Model Critique?</a></li>
<li class="chapter" data-level="3.6" data-path="what-is-regression.html"><a href="what-is-regression.html#limitations-and-connections"><i class="fa fa-check"></i><b>3.6</b> Limitations and Connections</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="what-are-probablistic-and-non-probablistic-regression.html"><a href="what-are-probablistic-and-non-probablistic-regression.html"><i class="fa fa-check"></i><b>4</b> What are Probablistic and Non-Probablistic Regression?</a>
<ul>
<li class="chapter" data-level="4.1" data-path="what-are-probablistic-and-non-probablistic-regression.html"><a href="what-are-probablistic-and-non-probablistic-regression.html#what-is-probabilistic-regression"><i class="fa fa-check"></i><b>4.1</b> What is Probabilistic Regression?</a></li>
<li class="chapter" data-level="4.2" data-path="what-are-probablistic-and-non-probablistic-regression.html"><a href="what-are-probablistic-and-non-probablistic-regression.html#almost-everything-is-linear-regression-1"><i class="fa fa-check"></i><b>4.2</b> (Almost) Everything is Linear Regression</a></li>
<li class="chapter" data-level="4.3" data-path="what-are-probablistic-and-non-probablistic-regression.html"><a href="what-are-probablistic-and-non-probablistic-regression.html#the-cube-a-model-comparison-paradigm"><i class="fa fa-check"></i><b>4.3</b> The Cube: A Model Comparison Paradigm</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html"><i class="fa fa-check"></i><b>5</b> What Matters in ML Besides Prediction?</a>
<ul>
<li class="chapter" data-level="5.1" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#what-is-machine-learning-revisited"><i class="fa fa-check"></i><b>5.1</b> What is Machine Learning? Revisited</a></li>
<li class="chapter" data-level="5.2" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#what-are-we-uncertain-about"><i class="fa fa-check"></i><b>5.2</b> What Are We Uncertain About?</a></li>
<li class="chapter" data-level="5.3" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#where-is-uncertainty-coming-from"><i class="fa fa-check"></i><b>5.3</b> Where is Uncertainty Coming From?</a></li>
<li class="chapter" data-level="5.4" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#how-do-we-compute-uncertainty"><i class="fa fa-check"></i><b>5.4</b> How Do We Compute Uncertainty?</a></li>
<li class="chapter" data-level="5.5" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#mathematizing-uncertainty-starting-with-bias-and-variance"><i class="fa fa-check"></i><b>5.5</b> Mathematizing Uncertainty: Starting with Bias and Variance</a></li>
<li class="chapter" data-level="5.6" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#the-bias-variance-trade-off-in-machine-learning"><i class="fa fa-check"></i><b>5.6</b> The Bias-Variance Trade-off in Machine Learning</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="what-matters-in-ml-besides-prediction.html"><a href="what-matters-in-ml-besides-prediction.html#examples-of-the-bias-variance-trade-off"><i class="fa fa-check"></i><b>5.6.1</b> Examples of the Bias-Variance Trade-off</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html"><i class="fa fa-check"></i><b>6</b> What is Logistic Regression?</a>
<ul>
<li class="chapter" data-level="6.1" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html#logistic-regression-and-soft-classification"><i class="fa fa-check"></i><b>6.1</b> Logistic Regression and Soft-Classification</a></li>
<li class="chapter" data-level="6.2" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html#logistic-regression-and-bernoulli-likelihood"><i class="fa fa-check"></i><b>6.2</b> Logistic Regression and Bernoulli Likelihood</a></li>
<li class="chapter" data-level="6.3" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html#how-to-perform-maximum-likelihood-inference-for-logistic-regression"><i class="fa fa-check"></i><b>6.3</b> How to Perform Maximum Likelihood Inference for Logistic Regression</a></li>
<li class="chapter" data-level="6.4" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html#how-not-to-evaluate-classifiers"><i class="fa fa-check"></i><b>6.4</b> How (Not) to Evaluate Classifiers</a></li>
<li class="chapter" data-level="6.5" data-path="what-is-logistic-regression.html"><a href="what-is-logistic-regression.html#how-to-interpret-logistic-regression"><i class="fa fa-check"></i><b>6.5</b> How to Interpret Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="how-do-we-responsibly-use-conditional-models.html"><a href="how-do-we-responsibly-use-conditional-models.html"><i class="fa fa-check"></i><b>7</b> How Do We Responsibly Use Conditional Models?</a>
<ul>
<li class="chapter" data-level="7.1" data-path="how-do-we-responsibly-use-conditional-models.html"><a href="how-do-we-responsibly-use-conditional-models.html#everything-weve-done-so-far-in-probabilistic-ml"><i class="fa fa-check"></i><b>7.1</b> Everything We’ve Done So Far in Probabilistic ML</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="case-study-responsibly-using-logistic-regression.html"><a href="case-study-responsibly-using-logistic-regression.html"><i class="fa fa-check"></i><b>8</b> Case Study: Responsibly Using Logistic Regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="case-study-responsibly-using-logistic-regression.html"><a href="case-study-responsibly-using-logistic-regression.html#case-study-machine-learning-model-for-loan-approval"><i class="fa fa-check"></i><b>8.1</b> Case Study: Machine Learning Model for Loan Approval</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="case-study-responsibly-using-logistic-regression.html"><a href="case-study-responsibly-using-logistic-regression.html#the-big-vague-question"><i class="fa fa-check"></i><b>8.1.1</b> The Big Vague Question</a></li>
<li class="chapter" data-level="8.1.2" data-path="case-study-responsibly-using-logistic-regression.html"><a href="case-study-responsibly-using-logistic-regression.html#the-concrete-and-rigorous-process-of-post-inference-analysis-of-machine-learning-models"><i class="fa fa-check"></i><b>8.1.2</b> The Concrete and Rigorous Process of Post-Inference Analysis of Machine Learning Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html"><i class="fa fa-check"></i><b>9</b> The Math of Training and Interpreting Logistic Regression Models</a>
<ul>
<li class="chapter" data-level="9.1" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#the-math-of-convex-optimization"><i class="fa fa-check"></i><b>9.1</b> The Math of Convex Optimization</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#convexity-of-the-logistic-regression-negative-log-likelihood"><i class="fa fa-check"></i><b>9.1.1</b> Convexity of the Logistic Regression Negative Log-Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#important-mathy-details-of-gradient-descent"><i class="fa fa-check"></i><b>9.2</b> Important Mathy Details of Gradient Descent</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#does-it-converge"><i class="fa fa-check"></i><b>9.2.1</b> Does It Converge?</a></li>
<li class="chapter" data-level="9.2.2" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#how-quickly-can-we-get-there"><i class="fa fa-check"></i><b>9.2.2</b> How Quickly Can We Get There?</a></li>
<li class="chapter" data-level="9.2.3" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#does-it-scale"><i class="fa fa-check"></i><b>9.2.3</b> Does It Scale?</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="the-math-of-training-and-interpreting-logistic-regression-models.html"><a href="the-math-of-training-and-interpreting-logistic-regression-models.html#interpreting-a-logistic-regression-model-log-odds"><i class="fa fa-check"></i><b>9.3</b> Interpreting a Logistic Regression Model: Log-Odds</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html"><i class="fa fa-check"></i><b>10</b> What are Neural Networks?</a>
<ul>
<li class="chapter" data-level="10.1" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#neural-network-as-universal-function-approximators"><i class="fa fa-check"></i><b>10.1</b> Neural Network as Universal Function Approximators</a></li>
<li class="chapter" data-level="10.2" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#neural-networks-as-regression-on-learned-feature-map"><i class="fa fa-check"></i><b>10.2</b> Neural Networks as Regression on Learned Feature Map</a></li>
<li class="chapter" data-level="10.3" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#everything-is-a-neural-network"><i class="fa fa-check"></i><b>10.3</b> Everything is a Neural Network</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#architecture-zoo"><i class="fa fa-check"></i><b>10.3.1</b> Architecture Zoo</a></li>
<li class="chapter" data-level="10.3.2" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#chatgpt"><i class="fa fa-check"></i><b>10.3.2</b> ChatGPT</a></li>
<li class="chapter" data-level="10.3.3" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#stable-diffusion"><i class="fa fa-check"></i><b>10.3.3</b> Stable Diffusion</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#neural-network-optimization"><i class="fa fa-check"></i><b>10.4</b> Neural Network Optimization</a></li>
<li class="chapter" data-level="10.5" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#bias-variance-trade-off-for-neural-networks"><i class="fa fa-check"></i><b>10.5</b> Bias-Variance Trade-off for Neural Networks</a></li>
<li class="chapter" data-level="10.6" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#interpretation-of-neural-networks"><i class="fa fa-check"></i><b>10.6</b> Interpretation of Neural Networks</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-1-can-neural-network-models-make-use-of-human-concepts"><i class="fa fa-check"></i><b>10.6.1</b> Example 1: Can Neural Network Models Make Use of Human Concepts?</a></li>
<li class="chapter" data-level="10.6.2" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-2-can-neural-network-models-learn-to-explore-hypothetical-scenarios"><i class="fa fa-check"></i><b>10.6.2</b> Example 2: Can Neural Network Models Learn to Explore Hypothetical Scenarios?</a></li>
<li class="chapter" data-level="10.6.3" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-3-a-powerful-generalization-of-feature-importance-for-neural-network-models"><i class="fa fa-check"></i><b>10.6.3</b> Example 3: A Powerful Generalization of Feature Importance for Neural Network Models</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#the-difficulty-with-interpretable-machine-learning"><i class="fa fa-check"></i><b>10.7</b> The Difficulty with Interpretable Machine Learning</a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-4-not-all-explanations-are-created-equal"><i class="fa fa-check"></i><b>10.7.1</b> Example 4: Not All Explanations are Created Equal</a></li>
<li class="chapter" data-level="10.7.2" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-5-explanations-can-lie"><i class="fa fa-check"></i><b>10.7.2</b> Example 5: Explanations Can Lie</a></li>
<li class="chapter" data-level="10.7.3" data-path="what-are-neural-networks.html"><a href="what-are-neural-networks.html#example-6-the-perils-of-explanations-in-socio-technical-systems"><i class="fa fa-check"></i><b>10.7.3</b> Example 6: The Perils of Explanations in Socio-Technical Systems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html"><i class="fa fa-check"></i><b>11</b> The Math and Interpretation of Neural Network Models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#neural-networks-regression"><i class="fa fa-check"></i><b>11.1</b> Neural Networks Regression</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#why-its-hard-to-differentiate-a-neural-network"><i class="fa fa-check"></i><b>11.1.1</b> Why It’s Hard to Differentiate a Neural Network</a></li>
<li class="chapter" data-level="11.1.2" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#differentiating-neural-networks-backpropagation"><i class="fa fa-check"></i><b>11.1.2</b> Differentiating Neural Networks: Backpropagation</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#interpreting-neural-networks"><i class="fa fa-check"></i><b>11.2</b> Interpreting Neural Networks</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#example-1-can-neural-network-models-make-use-of-human-concepts-1"><i class="fa fa-check"></i><b>11.2.1</b> Example 1: Can Neural Network Models Make Use of Human Concepts?</a></li>
<li class="chapter" data-level="11.2.2" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#example-2-can-neural-network-models-learn-to-explore-hypothetical-scenarios-1"><i class="fa fa-check"></i><b>11.2.2</b> Example 2: Can Neural Network Models Learn to Explore Hypothetical Scenarios?</a></li>
<li class="chapter" data-level="11.2.3" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#example-3-a-powerful-generalization-of-feature-importance-for-neural-network-models-1"><i class="fa fa-check"></i><b>11.2.3</b> Example 3: A Powerful Generalization of Feature Importance for Neural Network Models</a></li>
<li class="chapter" data-level="11.2.4" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#example-4-the-perils-of-explanations"><i class="fa fa-check"></i><b>11.2.4</b> Example 4: The Perils of Explanations</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="the-math-and-interpretation-of-neural-network-models.html"><a href="the-math-and-interpretation-of-neural-network-models.html#neural-network-models-and-generalization"><i class="fa fa-check"></i><b>11.3</b> Neural Network Models and Generalization</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="the-math-behind-bayesian-regression.html"><a href="the-math-behind-bayesian-regression.html"><i class="fa fa-check"></i><b>12</b> The Math Behind Bayesian Regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="the-math-behind-bayesian-regression.html"><a href="the-math-behind-bayesian-regression.html#bayesian-linear-regression"><i class="fa fa-check"></i><b>12.1</b> Bayesian Linear Regression</a></li>
<li class="chapter" data-level="12.2" data-path="the-math-behind-bayesian-regression.html"><a href="the-math-behind-bayesian-regression.html#bayesian-linear-regression-over-arbitrary-bases"><i class="fa fa-check"></i><b>12.2</b> Bayesian Linear Regression over Arbitrary Bases</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="bayesian-modeling-framework.html"><a href="bayesian-modeling-framework.html"><i class="fa fa-check"></i><b>13</b> Bayesian Modeling Framework</a>
<ul>
<li class="chapter" data-level="13.1" data-path="bayesian-modeling-framework.html"><a href="bayesian-modeling-framework.html#components-of-machine-learning-reasoning"><i class="fa fa-check"></i><b>13.1</b> Components of Machine Learning Reasoning</a></li>
<li class="chapter" data-level="13.2" data-path="bayesian-modeling-framework.html"><a href="bayesian-modeling-framework.html#bayesian-modeling-paradigm"><i class="fa fa-check"></i><b>13.2</b> Bayesian Modeling Paradigm</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="bayesain-vs-frequentist-inference.html"><a href="bayesain-vs-frequentist-inference.html"><i class="fa fa-check"></i><b>14</b> Bayesain vs Frequentist Inference?</a>
<ul>
<li class="chapter" data-level="14.1" data-path="bayesain-vs-frequentist-inference.html"><a href="bayesain-vs-frequentist-inference.html#the-bayesian-modeling-process"><i class="fa fa-check"></i><b>14.1</b> The Bayesian Modeling Process</a></li>
<li class="chapter" data-level="14.2" data-path="bayesain-vs-frequentist-inference.html"><a href="bayesain-vs-frequentist-inference.html#bayesian-vs-frequentist-inference"><i class="fa fa-check"></i><b>14.2</b> Bayesian vs Frequentist Inference</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html"><i class="fa fa-check"></i><b>15</b> The Math of Posterior Inference</a>
<ul>
<li class="chapter" data-level="15.1" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html#the-bayesian-modeling-process-1"><i class="fa fa-check"></i><b>15.1</b> The Bayesian Modeling Process</a></li>
<li class="chapter" data-level="15.2" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html#point-estimates-from-the-posterior"><i class="fa fa-check"></i><b>15.2</b> Point Estimates from the Posterior</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html#comparison-of-posterior-point-estimates-and-mle"><i class="fa fa-check"></i><b>15.2.1</b> Comparison of Posterior Point Estimates and MLE</a></li>
<li class="chapter" data-level="15.2.2" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html#law-of-large-numbers-for-bayesian-inference"><i class="fa fa-check"></i><b>15.2.2</b> Law of Large Numbers for Bayesian Inference</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="the-math-of-posterior-inference.html"><a href="the-math-of-posterior-inference.html#bayesian-logistic-regression"><i class="fa fa-check"></i><b>15.3</b> Bayesian Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="whats-hard-about-sampling.html"><a href="whats-hard-about-sampling.html"><i class="fa fa-check"></i><b>16</b> What’s Hard About Sampling?</a>
<ul>
<li class="chapter" data-level="16.1" data-path="whats-hard-about-sampling.html"><a href="whats-hard-about-sampling.html#bayesian-vs-frequentist-inference-1"><i class="fa fa-check"></i><b>16.1</b> Bayesian vs Frequentist Inference</a></li>
<li class="chapter" data-level="16.2" data-path="whats-hard-about-sampling.html"><a href="whats-hard-about-sampling.html#what-is-sampling-and-why-do-we-care"><i class="fa fa-check"></i><b>16.2</b> What is Sampling and Why do We Care?</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html"><i class="fa fa-check"></i><b>17</b> The Math of Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="17.1" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#pca-as-dimensionality-reduction-to-maximize-variance"><i class="fa fa-check"></i><b>17.1</b> PCA as Dimensionality Reduction to Maximize Variance</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#finding-a-single-pca-component"><i class="fa fa-check"></i><b>17.1.1</b> Finding a Single PCA Component</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#pca-as-dimensionality-reduction-to-minimize-reconstruction-loss"><i class="fa fa-check"></i><b>17.2</b> PCA as Dimensionality Reduction to Minimize Reconstruction Loss</a></li>
<li class="chapter" data-level="17.3" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#a-latent-variable-model-for-pca"><i class="fa fa-check"></i><b>17.3</b> A Latent Variable Model for PCA</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#one-principle-component"><i class="fa fa-check"></i><b>17.3.1</b> One Principle Component</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#autoencoders-and-nonlinear-pca"><i class="fa fa-check"></i><b>17.4</b> Autoencoders and Nonlinear PCA</a></li>
<li class="chapter" data-level="17.5" data-path="the-math-of-principal-component-analysis.html"><a href="the-math-of-principal-component-analysis.html#a-probabilistic-latent-variable-model-for-pca"><i class="fa fa-check"></i><b>17.5</b> A Probabilistic Latent Variable Model for PCA</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="the-math-of-expectation-maximization.html"><a href="the-math-of-expectation-maximization.html"><i class="fa fa-check"></i><b>18</b> The Math of Expectation Maximization</a></li>
<li class="chapter" data-level="19" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html"><i class="fa fa-check"></i><b>19</b> Motivation for Latent Variable Models</a>
<ul>
<li class="chapter" data-level="19.1" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#latent-variable-models"><i class="fa fa-check"></i><b>19.1</b> Latent Variable Models</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#example-gaussian-mixture-models-gmms"><i class="fa fa-check"></i><b>19.1.1</b> Example: Gaussian Mixture Models (GMMs)</a></li>
<li class="chapter" data-level="19.1.2" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#item-response-models"><i class="fa fa-check"></i><b>19.1.2</b> Item-Response Models</a></li>
<li class="chapter" data-level="19.1.3" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#example-factor-analysis-models"><i class="fa fa-check"></i><b>19.1.3</b> Example: Factor Analysis Models</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#maximum-likelihood-estimation-for-latent-variable-models-expectation-maximization"><i class="fa fa-check"></i><b>19.2</b> Maximum Likelihood Estimation for Latent Variable Models: Expectation Maximization</a></li>
<li class="chapter" data-level="19.3" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#the-expectation-maximization-algorithm"><i class="fa fa-check"></i><b>19.3</b> The Expectation Maximization Algorithm</a></li>
<li class="chapter" data-level="19.4" data-path="motivation-for-latent-variable-models.html"><a href="motivation-for-latent-variable-models.html#monotonicity-and-convergence-of-em"><i class="fa fa-check"></i><b>19.4</b> Monotonicity and Convergence of EM</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html"><i class="fa fa-check"></i><b>20</b> Review of Latent Variables, Compression and Clustering</a>
<ul>
<li class="chapter" data-level="20.0.1" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#example-gaussian-mixture-models-gmms-1"><i class="fa fa-check"></i><b>20.0.1</b> Example: Gaussian Mixture Models (GMMs)</a></li>
<li class="chapter" data-level="20.0.2" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#example-item-response-models"><i class="fa fa-check"></i><b>20.0.2</b> Example: Item-Response Models</a></li>
<li class="chapter" data-level="20.0.3" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#example-factor-analysis-models-1"><i class="fa fa-check"></i><b>20.0.3</b> Example: Factor Analysis Models</a></li>
<li class="chapter" data-level="20.1" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#pca-versus-probabilistic-pca-ppca"><i class="fa fa-check"></i><b>20.1</b> PCA Versus Probabilistic PCA (pPCA)</a>
<ul>
<li class="chapter" data-level="20.1.1" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#what-to-know-about-expectation-maximization"><i class="fa fa-check"></i><b>20.1.1</b> What to Know About Expectation Maximization</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="review-of-latent-variables-compression-and-clustering.html"><a href="review-of-latent-variables-compression-and-clustering.html#non-probabilistic-clustering-versus-probabilistic-clustering"><i class="fa fa-check"></i><b>20.2</b> Non-Probabilistic Clustering Versus Probabilistic Clustering</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="topic-models.html"><a href="topic-models.html"><i class="fa fa-check"></i><b>21</b> Topic Models</a>
<ul>
<li class="chapter" data-level="21.1" data-path="topic-models.html"><a href="topic-models.html#our-first-latent-variable-model"><i class="fa fa-check"></i><b>21.1</b> Our First Latent Variable Model</a></li>
<li class="chapter" data-level="21.2" data-path="topic-models.html"><a href="topic-models.html#reasoning-about-text-corpa-using-topic-modeling"><i class="fa fa-check"></i><b>21.2</b> Reasoning About Text Corpa Using Topic Modeling</a></li>
<li class="chapter" data-level="21.3" data-path="topic-models.html"><a href="topic-models.html#our-second-latent-variable-model-plsa"><i class="fa fa-check"></i><b>21.3</b> Our Second Latent Variable Model: pLSA</a></li>
<li class="chapter" data-level="21.4" data-path="topic-models.html"><a href="topic-models.html#our-third-latent-variable-model-lda"><i class="fa fa-check"></i><b>21.4</b> Our Third Latent Variable Model: LDA</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html"><i class="fa fa-check"></i><b>22</b> Math and Intuition of Hidden Markov Models</a>
<ul>
<li class="chapter" data-level="22.1" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html#markov-models"><i class="fa fa-check"></i><b>22.1</b> Markov Models</a>
<ul>
<li class="chapter" data-level="22.1.1" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html#transition-matrices-and-kernels"><i class="fa fa-check"></i><b>22.1.1</b> Transition Matrices and Kernels</a></li>
<li class="chapter" data-level="22.1.2" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html#applications-of-markov-models"><i class="fa fa-check"></i><b>22.1.2</b> Applications of Markov Models</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html#hidden-markov-models"><i class="fa fa-check"></i><b>22.2</b> Hidden Markov Models</a></li>
<li class="chapter" data-level="22.3" data-path="math-and-intuition-of-hidden-markov-models.html"><a href="math-and-intuition-of-hidden-markov-models.html#learning-and-inference-for-hmms"><i class="fa fa-check"></i><b>22.3</b> Learning and Inference for HMMs</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html"><i class="fa fa-check"></i><b>23</b> The Intuition of Markov Decision Processes</a>
<ul>
<li class="chapter" data-level="23.1" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#review-modeling-sequential-data"><i class="fa fa-check"></i><b>23.1</b> Review: Modeling Sequential Data</a>
<ul>
<li class="chapter" data-level="23.1.1" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#why-model-sequential-data-dynamics"><i class="fa fa-check"></i><b>23.1.1</b> Why Model Sequential Data (Dynamics)?</a></li>
</ul></li>
<li class="chapter" data-level="23.2" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#modeling-sequential-data-and-sequential-actions"><i class="fa fa-check"></i><b>23.2</b> Modeling Sequential Data and Sequential Actions</a>
<ul>
<li class="chapter" data-level="23.2.1" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#describing-a-dynamic-world"><i class="fa fa-check"></i><b>23.2.1</b> Describing a Dynamic World</a></li>
<li class="chapter" data-level="23.2.2" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#acting-in-a-dynamic-world"><i class="fa fa-check"></i><b>23.2.2</b> Acting in a Dynamic World</a></li>
<li class="chapter" data-level="23.2.3" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#describing-worlds-as-mdps"><i class="fa fa-check"></i><b>23.2.3</b> Describing Worlds as MDP’s</a></li>
</ul></li>
<li class="chapter" data-level="23.3" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#modeling-sequential-decisions-planning"><i class="fa fa-check"></i><b>23.3</b> Modeling Sequential Decisions: Planning</a>
<ul>
<li class="chapter" data-level="23.3.1" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#modeling-action-choice"><i class="fa fa-check"></i><b>23.3.1</b> Modeling Action Choice</a></li>
<li class="chapter" data-level="23.3.2" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#modeling-cumulative-reward"><i class="fa fa-check"></i><b>23.3.2</b> Modeling Cumulative Reward</a></li>
<li class="chapter" data-level="23.3.3" data-path="the-intuition-of-markov-decision-processes.html"><a href="the-intuition-of-markov-decision-processes.html#planning-optimizing-action-choice-for-cumulative-reward"><i class="fa fa-check"></i><b>23.3.3</b> Planning: Optimizing Action Choice for Cumulative Reward</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes for CS181: Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="case-study-responsibly-using-logistic-regression" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Chapter 8</span> Case Study: Responsibly Using Logistic Regression<a href="case-study-responsibly-using-logistic-regression.html#case-study-responsibly-using-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><img src="https://i.imgur.com/xDR9VQd.png" /></p>
<p><strong>Motto:</strong> Being a machine learning researcher/practitioner is like being a doctor not like constructing a house according to blueprint. That is, you are using theory to <strong><em>diagnose problems</em></strong> and <strong><em>suggest treatment</em></strong>, you are executing actions based on a blueprint.</p>
<p><strong>Motto for Resolving Choice Paralysis:</strong> Non-action is the action of upholding the status quo!</p>
<div id="case-study-machine-learning-model-for-loan-approval" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Case Study: Machine Learning Model for Loan Approval<a href="case-study-responsibly-using-logistic-regression.html#case-study-machine-learning-model-for-loan-approval" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose that you have been asked by the International Bank of Z to use see if it’s possible to use ML to automate their loan approval process in order to reduce the cost of retaining a large staff of expert loan officers.</p>
<p>They give you a training dataset consisting of 10,000 loan applications considered between 2002 and 2010 at the International Bank of Z.</p>
<p>In this training set you have the following information:
&gt;<code>x_1</code> representing gender: 0 for male, 1 for female
&gt;<code>x_2</code> represents income
&gt;<code>x_3</code> represents personal debt
&gt;<code>x_4</code> represents the loan amount
&gt;<code>x_5</code> represents credit score
&gt; <code>y</code> represents whether or not the loan was approved</p>
<p>You do a 70/15/15 split of the historical data you have into training, validation and test.</p>
<p>You fit a <strong>logistic regression model</strong> to predict whether a loan application should be approved.</p>
<p>Suppose that you found the following parameters by maximizing the joint log-likelihood of your training data using your own implementation of gradient descent:
<span class="math display">\[
   p(y=1 | \mathbf{x}) = \sigma(-1 + 3 x_1 + 1.5 x_2 - 1.75 x_3 + 0.6 x_4 - 0.01 x_5).
   \]</span>
For a new loan application, you approve if <span class="math inline">\(p(y=1 | \mathbf{x}) \geq 0.5\)</span> and deny if <span class="math inline">\(p(y=1 | \mathbf{x}) &lt; 0.5\)</span>.</p>
<p>You computed the following metrics:</p>
<table>
<thead>
<tr class="header">
<th>Metric</th>
<th>Train</th>
<th>Validation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Accuracy</td>
<td>49%</td>
<td>51%</td>
</tr>
<tr class="even">
<td>AUC</td>
<td>0.51</td>
<td>0.52</td>
</tr>
<tr class="odd">
<td>Log-likelihood</td>
<td>-307.760577547</td>
<td>-407.547760577</td>
</tr>
</tbody>
</table>
<div id="the-big-vague-question" class="section level3 hasAnchor" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> The Big Vague Question<a href="case-study-responsibly-using-logistic-regression.html#the-big-vague-question" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Is this a good model? Do I present it to my clients tomorrow as a solution to their problem? If so, how will I convince them that this ML model is going to solve their business problem?</p>
</div>
<div id="the-concrete-and-rigorous-process-of-post-inference-analysis-of-machine-learning-models" class="section level3 hasAnchor" number="8.1.2">
<h3><span class="header-section-number">8.1.2</span> The Concrete and Rigorous Process of Post-Inference Analysis of Machine Learning Models<a href="case-study-responsibly-using-logistic-regression.html#the-concrete-and-rigorous-process-of-post-inference-analysis-of-machine-learning-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><strong>(Inference)</strong> Did I do inference correctly?
<ol style="list-style-type: lower-alpha">
<li><p>Did I write down the correct objective function and did I implement it correctly?
&gt; You chose to maximize the log-likelihood since that’s pretty standard but you know that MLE has potential drawbacks. You check your <code>numpy</code> implementation of the objective function and it looks correct.
&gt;
&gt; You move on and will come back to check more carefully if you don’t find other issues.</p></li>
<li><p>Did I optimize my objective function correctly?
&gt; You checked that your gradient descent was implemented correctly and that your loss function was decreasing during gradient descent.
&gt;
&gt; You move on and will come back to check more carefully if you don’t find other issues.</p></li>
</ol></li>
<li><strong>(Bias-Variance Trade-off)</strong> Did I choose the wrong model class?
<ol style="list-style-type: lower-alpha">
<li>Am I underfitting (i.e. my model’s prediction on new data will have high error because it has high bias)?
&gt; You check for underfitting by reducing the potential bias in your model. You try a polynomial feature map <span class="math inline">\(\phi\)</span> of degree 3 and get:
&gt;
<span class="math display">\[\begin{aligned}
p(y=1 | \mathbf{x}) = \sigma(&amp;-50 + 4 x_1 + 17 x_2 + 0.0001 x_3 + 0.2 x_4 + 9 x_5\\
&amp;- 0.0012 x^2_1 + 0.15 x^2_2 + 0.2226 x^2_3 - 0.26 x^2_4 + x^2_5\\
&amp;+ 1.2 x^3_1 - 1.0001 x_2 - 0.34 x^3_3 + 0.5 x^3_4 +  x^3_5)\\
\end{aligned}\]</span>
<blockquote>
<p>Now your metrics look like:
| Metric | Train | Validation |
| ——– | ——– | ——– |
| Accuracy | 98% | 85% |
| AUC | 0.8 | 0.7 |
| Log-likelihood | -1.2349e-9 | -7.547760577 |</p>
</blockquote></li>
<li>Am I overfitting (i.e. my model’s prediction on new data will have high error because it has high variance)?
&gt; It looks like adding your polynomial feature map caused overfitting! Your generalization error will be high due to high model variance. For now, you try to reduce the variance of your model by increasing the potential bias of your model – by (1) using only degree 2 polynomials and by (2) adding <span class="math inline">\(\ell_1\)</span> regularization:
&gt;
<span class="math display">\[\begin{aligned}
p(y=1 | \mathbf{x}) = \sigma(&amp;-0.5 + 2.1 x_1 + 0.0001 x_3 + 0.2 x_4 + 0.15 x^2_2 + 0.063x^2_5)
\end{aligned}\]</span>
<blockquote>
<p>Now your metrics look like:
| Metric | Train | Validation |
| ——– | ——– | ——– |
| Accuracy | 89% | 86% |
| AUC | 0.79 | 0.81 |
| Log-likelihood | -4.9812e-3 | -5.21774e-3 |</p>
<p>Since your train validation metrics look similar and similarly high-ish, you believe you’ve manage your bias-variance trade-off well.</p>
<p>You move on and will come back to check more carefully if you need to.</p>
</blockquote></li>
</ol></li>
<li><strong>(Choose Meaningful Evaluation Metrics)</strong> Of the metrics that I know how to compute, which metrics should I use to evaluate my model?
<ol style="list-style-type: lower-alpha">
<li>Do my metrics agree? Is it ok if they disagree?
&gt; In this case, accuracy is higher than AUC – is this a deal breaker?!
&gt; Not necessarily. Recall from that AUC is defined as the area under the ROC curve. Roughly, the AUC is measuring how well your model does over <em>all possible</em> classification thresholds. In contrast, the accuracy is measuring how well your model does for the threshold 0.5.
&gt; As long as you are making good decisions using the threshold 0.5, you may not need to worry about the AUC.
&gt;
&gt; You move on and will come back to check more carefully if you need to.
&gt;</li>
<li>Which of my chosen metrics should I prioritize?
&gt; If your client is just interested in making accurate decisions using your model, maybe prioritizing accuracy at a threshold of 0.5 is fine for this down-stream task.
&gt;
&gt; You move on and will come back to check more carefully if you need to.</li>
</ol></li>
<li><strong>(Task-Sensitive Evaluation)</strong> But <em>really</em>, how should I choose to evaluate my model?</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li><p>What are my real-life goals? Do any of my metrics correspond to any real-life performance goals?
&gt; In this case, your client most likely wants to avoid approving bad loans. This means that they care want the model to predict <span class="math inline">\(y=0\)</span> when the loan would have been rejected by a human expert. Does a high accuracy mean that we’ve achieved this goal?
&gt;
&gt; You check the balance of the two classes in your loan data and see that about 75% of the loans were approved – there is <em>class imbalance</em>.
&gt;
&gt; You then check the training confusion matrix and see:
&gt;
&gt;| | Predicted 0| Predicted 1 |
&gt;| ——– | ——– | ——– |
&gt;| y=0 | 1488 | 262 |
&gt;| y=1 | 578 | 4672 |
&gt;
&gt; Luckily, despite the class imbalance, your model does equally well on both classes!</p></li>
<li><p>Have I interpreted my model? Does the interpretation make sense in the context of the problem? Have I made use of all available application domain knowledge?</p></li>
</ol>
<blockquote>
So far, your model seem to be doing well on your numeric metrics, does it mean that it’s right for the right reasons? Let’s interpre the model and see how it makes decisions:
<span class="math display">\[\begin{aligned}
p(y=1 | \mathbf{x}) = \sigma(&amp;-0.5 + 2.1 x_1 + 0.0001 x_3 + 0.2 x_4 + 0.15 x^2_2 + 0.063x^2_5)
\end{aligned}\]</span>
<p>You note several things right away: (1) it seems like “gender”, <span class="math inline">\(x_1\)</span>, has the biggest impact on probability of approval (2) the loan amount and debt positively impact the probability of approval! Do these model properties make sense?</p>
<p>You worry that the coefficients cannot be reliably interpreted because the covariates are measured at different scales. So you check and are relieved to see that the data has been standardized and all take value between -1 and 1. This gives you some confidence that your interpretation of coefficients are not obviously wrong</p>
<p>It seems counter-intuitive in your model that the more loans you take out and the more debt you have, the more likely your loan will be approved. Maybe there’s a bug in your optimization or coding?! Not necessarily! If you consult a domain expert, they will tell you that often times loan amount is correlated with income, as is debt amount (this is why people look at income to debt ratio). That is, co-linearity amongst your covariates might explain the counter-intuitive coefficients. You compute the Pearson corrlation between <span class="math inline">\(x_3\)</span> and <span class="math inline">\(x_2\)</span>, then <span class="math inline">\(x_4\)</span> and <span class="math inline">\(x_2\)</span>; you find that both loan amount and debt are slightly correlated with income in the data.</p>
<p>You move on and will come back to check more carefully if you need to.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>What are your uncertainties and where are they?
You bootstrap your training data 100 times and estimate confidence intervals on the coefficients of your logisic regression and get the following means and 95% confidence intervals:
<span class="math display">\[\begin{aligned}
&amp;w_0 \text{(bias term)}: 0.5 \pm 1.2\\
&amp;w_1 \text{(for $x_1$)}: 0.05 \pm 1.9,\\
&amp;w_2\text{(for $x^2_2$)}: 0.1 \pm 2.12, \\
&amp;w_3\text{(for $x_3$)}: 0.108 \pm 1.3, \\
&amp;w_4\text{(for $x_4$)}: 0.08 \pm 0.9, \\
&amp;w_5\text{(for $x^2_5$)}: 0.25 \pm 1.34
\end{aligned}\]</span>
As the confidence intervals of the coefficients overlap a lot, it’s not reasonable for us to be certain about our model interpretation – e.g. that “gender” has the greatest impact on loan approval probability.</li>
</ol>
</blockquote>
<ol start="5" style="list-style-type: decimal">
<li><strong>(Model Critique)</strong> Revisit every design decision – did I make the wrong decision?
<ol style="list-style-type: lower-alpha">
<li>Did I choose the right data set? Are there forseeable issues with my data set?
&gt; You worry that outliers, mistakes, missingness and other odd features of the data might be biasing your model. You should check the quality of your data by looking at data summaries (mean, median, variance, range and percentage missing for each covariate) and visualizations (scatter plots of your data, choosing a couple of covariates at a time).
&gt;
&gt; You also sanity-check that the data summaries match your expectation of what are reasonable values. You notice that the data dictionary says <span class="math inline">\(x_1\)</span> represents gender (i.e. related to lived-experiences) but the values encode “male” and “female” (i.e. related to biological invariants). You wonder if the bank mislabled this covariate (i.e. <span class="math inline">\(x_1\)</span> should represent sex rather than gender).
&gt;
&gt; You also noticed that the “gender” covariate is coded as a binary, this would cause non-binary applicants to be misclassified (misclassification potentially introduces noise).
&gt;</li>
<li>Did I choose the right model type and right model class?
&gt; You struggled a bit interpreting logistic regression with polynomial features. Would KNN or decision tree be easier to interpret (<em>Hint:</em> the model decision process might be easier to explain but validating your interpretation would be harder, as there are fewer exposed assumptions)?
&gt;</li>
<li>Did I choose the right loss function and the right optimization procedure?
&gt; You wonder if you should have maximized likelihood if what you really wanted to check was accuracy. What happens if you try to optimize for accuracy directly (<em>Hint:</em> you’ll have problems differentiating)?</li>
</ol></li>
<li><strong>(Model Deployment)</strong> Should I use this model? If so how?
<ol style="list-style-type: lower-alpha">
<li>Did your model pass all your checks above?
&gt; Running through the above, you’ve already flagged a potential issue: while accurate, your model (as it stands) cannot be reliably interpreted.
&gt;
&gt; Is this a problem?
&gt;
&gt; You realize that your client International Bank of Z is based in the EU, and thus under <a href="https://www.consilium.europa.eu/en/policies/data-protection/data-protection-regulation/">The General Data Protection Regulation</a>, the bank may be legally required to provide an explanation for each loan decision as well as a recommendation for recourse in case the loan is rejected. In this case, it is critical that we can provide a reliable interpretation of the model’s decision making process.
&gt;
&gt; You also flagged that the “gender” covariate might actually be containing sex information rather than gender, the binary encoding of this covariate also causes potential misclassification of non-binary applicants.
&gt;
&gt; Is this a problem?
&gt;
&gt; Incorrect encoding and mislabeling introduces noise into the data, potentially making the tasks of modeling and model interpretation much harder!
&gt;</li>
<li>Is your dataset legally obtained for the purposes of building predictive models?
&gt; You realize that you need to check with the bank whether or not the loan applicants had given legal consent for their data to be used by a 3rd-party body for analysis!
&gt;
&gt; Just because the bank has the right to analyze historical loan application data it does not necessarily mean that the bank is authorized to released the data to 3rd-parties!
&gt;</li>
<li>How will your model be used in decision making once deployed? Is it legal to use your model as intended?
&gt; As described in your project spec, the bank wants to rely on your model to make loan decisions, replacing human loan officers for most loan applications.
&gt;
&gt; As it stands, it may not be legal for your model to make loan decisions or to even inform loan decisions in the US! Laws like the Equal Credit Opportunity Act prohibits basing loan decisions, for example, on sex. Given your interpretation, sex has the biggest impact on the classification probability of your logistic regression!
&gt;
&gt; To complicate matters, the terms “gender” and “sex” can have very specific legal definitions – that may not align with defintions in medical or sociological contexts! For example, the language of the Equal Credit Opportunity Act is written in terms of “sex”, however, this term is not defined in the ECOA. The Consumer Financial Protection Bureau interprets (in their operating philosophy) this term to include both sexual orientation and gender identity.
&gt;</li>
<li>Is your model serving the needs of your users? Is your model harming the public good (e.g. who is your affected community and how are they being impacted)? Is your model causing disparate impact?
&gt; In this case, the users of our model are bank employees, but the most significant affected community is that of loan applicants. It can very well be the case that the bank might be perfectly happy with your model but loan applicants will not be – for example, if your model systemmatically denies loan applications from a specific subpopulation.
&gt; Given that you only have sex information but no other demographic covariates, you can check if your model is equally accurate for male applicants as it is for female applicants:
<span class="math display">\[\begin{aligned}
&amp;\texttt{male}: 89\% \text{ test accuracy}\;
&amp;\texttt{female}: 85\% \text{ test accuracy}
\end{aligned}\]</span>
<blockquote>
<p>While the test accuracy for male applicants is slightly higher, the two groups are comparable.</p>
<p>Does this mean that your model will not cause disparate impact? No!</p>
<p>Just because that your model is accurate in predicting a human loan officer’s decision, it does not make your model fair! In fact, you check the observed loan approval rates in your data set and find that over 80% of loans from female applicants were rejected while over 60% of male applicants were approved. This is very much disparate impact! If your model captured this decision process perfectly, you would be automating and perpetuating this disparate impact!</p>
</blockquote></li>
</ol></li>
</ol>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="how-do-we-responsibly-use-conditional-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="the-math-of-training-and-interpreting-logistic-regression-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/07-Case-Study-1.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
